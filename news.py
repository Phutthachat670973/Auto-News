# news.py
# ============================================================================================================
# NEWS BOT: Dual output in one run
# 1) Project Impact (‡πÄ‡∏î‡∏¥‡∏°): ‡∏Ñ‡∏±‡∏î‡∏Ç‡πà‡∏≤‡∏ß+‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô "‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£" ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á LINE (Text/Flex)
# 2) Energy Digest (‡πÉ‡∏´‡∏°‡πà): ‡∏™‡πà‡∏á "‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô" + "‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô" ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
#
# ‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á (ENV):
# - OUTPUT_MODE=both (default)       -> ‡∏™‡πà‡∏á 2 ‡∏ä‡∏∏‡∏î‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô: [Project Impact] + [Energy Digest]
# - OUTPUT_MODE=project_only         -> ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
# - OUTPUT_MODE=digest_only          -> ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
#
# ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß (ENV):
# - PROJECT_SEND_LIMIT=10            -> ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ù‡∏±‡πà‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
# - DIGEST_MAX_PER_SECTION=8         -> ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î‡πÉ‡∏ô digest
#
# ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏±‡πà‡∏ô (ENV):
# - ADD_SECTION_HEADERS=true/false   -> ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏±‡πà‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
#
# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ ‚Äú‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‚Äù ‡∏ö‡∏ô GitHub Actions / Local ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ENV ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
# ============================================================================================================

import os
import re
import json
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import feedparser
from dateutil import parser as dateutil_parser
import pytz
import requests

# -----------------------------
# Optional dotenv (local dev)
# -----------------------------
try:
    from dotenv import load_dotenv  # type: ignore
    load_dotenv()
except Exception:
    pass

# ============================================================================================================
# ENV
# ============================================================================================================

GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()

if not GROQ_API_KEY:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY")

if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_CHANNEL_ACCESS_TOKEN")

GROQ_MODEL_NAME = os.getenv("GROQ_MODEL_NAME", "llama-3.1-8b-instant").strip()

MAX_RETRIES = int(os.getenv("MAX_RETRIES", "6"))
SLEEP_BETWEEN_CALLS = (
    float(os.getenv("SLEEP_MIN", "1.0")),
    float(os.getenv("SLEEP_MAX", "2.0")),
)
LLM_BATCH_SIZE = int(os.getenv("LLM_BATCH_SIZE", "10"))
DRY_RUN = os.getenv("DRY_RUN", "false").strip().lower() in ("1", "true", "yes", "y")

# Project-mode controls (‡πÄ‡∏î‡∏¥‡∏°)
PROJECT_SEND_LIMIT = int(os.getenv("PROJECT_SEND_LIMIT", "10"))
MIN_SOURCE_SCORE = float(os.getenv("MIN_SOURCE_SCORE", "0"))
SHOW_SOURCE_RATING = os.getenv("SHOW_SOURCE_RATING", "true").strip().lower() in ("1", "true", "yes", "y")
ENABLE_IMPACT_REWRITE = os.getenv("ENABLE_IMPACT_REWRITE", "true").strip().lower() in ("1", "true", "yes", "y")
USE_KEYWORD_GATE = os.getenv("USE_KEYWORD_GATE", "false").strip().lower() in ("1", "true", "yes", "y")

# Dual output mode
OUTPUT_MODE = os.getenv("OUTPUT_MODE", "both").strip().lower()  # both | project_only | digest_only
ADD_SECTION_HEADERS = os.getenv("ADD_SECTION_HEADERS", "true").strip().lower() in ("1", "true", "yes", "y")

# Digest-mode controls (‡πÉ‡∏´‡∏°‡πà)
DIGEST_MAX_PER_SECTION = int(os.getenv("DIGEST_MAX_PER_SECTION", "8"))

DEFAULT_HERO_URL = os.getenv("DEFAULT_HERO_URL", "").strip()
USER_AGENT = os.getenv("USER_AGENT", "Mozilla/5.0 (NewsBot)").strip()

# Timezone
bangkok_tz = pytz.timezone("Asia/Bangkok")

# ============================================================================================================
# RSS FEEDS (‡∏õ‡∏£‡∏±‡∏ö/‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ)
# ============================================================================================================

RSS_FEEDS: List[Dict[str, str]] = [
    # International
    {"name": "OilPrice", "url": "https://oilprice.com/rss/main", "country": "Global"},
    {"name": "Reuters Energy (fallback)", "url": "https://www.reuters.com/rssFeed/energyNews", "country": "Global"},
    {"name": "Bloomberg Energy (fallback)", "url": "https://www.bloomberg.com/feed/podcast/etf-report.xml", "country": "Global"},
    # Thailand / local (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
    {"name": "Prachachat", "url": "https://www.prachachat.net/feed", "country": "Thailand"},
    {"name": "Bangkokbiznews", "url": "https://www.bangkokbiznews.com/rss", "country": "Thailand"},
    {"name": "PostToday", "url": "https://www.posttoday.com/rss", "country": "Thailand"},
    # Add more as needed...
]

# ============================================================================================================
# STYLE LEARNING EXAMPLES (Few-shot)
# ‡πÉ‡∏´‡πâ LLM ‡∏¢‡∏∂‡∏î‡πÇ‡∏ó‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
# ============================================================================================================

STYLE_EXAMPLES = """
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡πÇ‡∏ó‡∏ô/‡∏™‡∏≥‡∏ô‡∏ß‡∏ô/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß):

[‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß]
üî∏‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
1. ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡πÅ‡∏ó‡πà‡∏ô‡∏Ç‡∏∏‡∏î‡πÄ‡∏à‡∏≤‡∏∞‡∏≠‡πà‡∏≤‡∏ß‡πÑ‡∏ó‡∏¢ ‡∏™‡∏Å‡∏±‡∏î‡πÇ‡∏î‡∏£‡∏ô‡∏õ‡πà‡∏ß‡∏ô ‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï
2. ‚Äò‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‚Äô ‡∏£‡∏≠ ‡∏Ñ‡∏£‡∏°.‡∏ä‡∏∏‡∏î‡πÉ‡∏´‡∏°‡πà ‡∏´‡πà‡∏ß‡∏á‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢‡πÄ‡∏î‡∏¥‡∏ô‡∏ö‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ö‡∏≤‡∏á ‡πÜ

[‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡πà‡∏≤‡∏ß]
üî∏‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
1.‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏±‡πà‡∏á‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏£‡∏≠‡∏ö‡πÅ‡∏ó‡πà‡∏ô‡∏Ç‡∏∏‡∏î‡πÄ‡∏à‡∏≤‡∏∞‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏°‡πÉ‡∏ô‡∏≠‡πà‡∏≤‡∏ß‡πÑ‡∏ó‡∏¢ ‡∏´‡∏•‡∏±‡∏á‡∏û‡∏ö‡πÇ‡∏î‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ù‡πà‡∏≤‡∏¢‡∏£‡∏∏‡∏Å‡∏•‡πâ‡∏≥‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡πÇ‡∏î‡∏¢‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≠‡∏á‡∏ó‡∏±‡∏û‡πÄ‡∏£‡∏∑‡∏≠‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á 24 ‡∏ä‡∏°. ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ 5 ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î ‡πÅ‡∏ï‡πà‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ
(‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
2.‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏á‡∏ß‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ ‚Äú‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‚Äù ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ‡∏ä‡∏∏‡∏î‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ö‡∏≤‡∏á ‡πÜ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏∞‡∏≠‡∏≤‡∏î
(‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)

‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤‡∏™‡πÑ‡∏ï‡∏•‡πå:
- headline_th: ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πâ‡∏ô 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡πÅ‡∏ô‡∏ß‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
- summary_th: 2‚Äì4 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÇ‡∏ó‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏ô‡πâ‡∏ô ‚Äú‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô/‡πÉ‡∏Ñ‡∏£/‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á/‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‚Äù
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å title/summary
"""

# ============================================================================================================
# Helpers: URL normalize / dedupe
# ============================================================================================================

TRACK_DIR = os.getenv("TRACK_DIR", "sent_links").strip()

def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

def normalize_url(url: str) -> str:
    try:
        u = url.strip()
        if not u:
            return u
        p = urlparse(u)
        # remove tracking params
        q = [(k, v) for k, v in parse_qsl(p.query, keep_blank_values=True)
             if k.lower() not in ("utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content", "fbclid", "gclid")]
        new_query = urlencode(q)
        p2 = p._replace(query=new_query, fragment="")
        return urlunparse(p2)
    except Exception:
        return url.strip()

def load_sent_links() -> set:
    ensure_dir(TRACK_DIR)
    fp = os.path.join(TRACK_DIR, "sent_links.txt")
    if not os.path.exists(fp):
        return set()
    try:
        with open(fp, "r", encoding="utf-8") as f:
            return set([line.strip() for line in f if line.strip()])
    except Exception:
        return set()

def save_sent_links(links: List[str]) -> None:
    ensure_dir(TRACK_DIR)
    fp = os.path.join(TRACK_DIR, "sent_links.txt")
    old = load_sent_links()
    new = old.union(set([normalize_url(x) for x in links if x]))
    with open(fp, "w", encoding="utf-8") as f:
        for x in sorted(new):
            f.write(x + "\n")

# ============================================================================================================
# HTTP utilities
# ============================================================================================================

def http_get(url: str, timeout: int = 15) -> requests.Response:
    headers = {"User-Agent": USER_AGENT}
    return requests.get(url, headers=headers, timeout=timeout)

def resolve_final_url(url: str) -> str:
    try:
        r = http_get(url, timeout=15)
        return normalize_url(r.url or url)
    except Exception:
        return normalize_url(url)

def extract_og_image(url: str) -> Optional[str]:
    try:
        r = http_get(url, timeout=15)
        if r.status_code >= 400 or not r.text:
            return None
        html = r.text
        m = re.search(r'property=["\']og:image["\']\s+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
        if m:
            return m.group(1).strip()
        m = re.search(r'name=["\']twitter:image["\']\s+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
        if m:
            return m.group(1).strip()
        return None
    except Exception:
        return None

# ============================================================================================================
# GROQ API
# ============================================================================================================

GROQ_URL = "https://api.groq.com/openai/v1/chat/completions"

def _sleep_jitter():
    a, b = SLEEP_BETWEEN_CALLS
    time.sleep(random.uniform(a, b))

def call_groq_with_retries(prompt: str, temperature: float = 0.25, max_tokens: int = 1200) -> str:
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": GROQ_MODEL_NAME,
        "messages": [
            {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"},
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            _sleep_jitter()
            r = requests.post(GROQ_URL, headers=headers, json=payload, timeout=60)
            if r.status_code == 429:
                time.sleep(2.0 * attempt)
                continue
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.5 * attempt)
    raise RuntimeError(f"Groq call failed: {last_err}")

def _extract_json_object(text: str) -> Any:
    # try direct json
    t = text.strip()
    try:
        return json.loads(t)
    except Exception:
        pass
    # attempt to find first { ... } block
    m = re.search(r"\{.*\}", t, re.DOTALL)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None

# ============================================================================================================
# Credibility scoring (simple heuristic)
# ============================================================================================================

HIGH_TRUST_DOMAINS = {
    "reuters.com", "bloomberg.com", "wsj.com", "ft.com", "nytimes.com",
    "theguardian.com", "bbc.co.uk", "bbc.com", "oilprice.com",
    "prachachat.net", "bangkokbiznews.com", "posttoday.com",
    "energynewscenter.com", "mgronline.com", "matichon.co.th",
}

MED_TRUST_DOMAINS = {
    "msn.com", "yahoo.com", "investing.com", "seekingalpha.com", "marketwatch.com",
}

def domain_of(url: str) -> str:
    try:
        h = urlparse(url).netloc.lower()
        if h.startswith("www."):
            h = h[4:]
        return h
    except Exception:
        return ""

def source_score(url: str) -> float:
    d = domain_of(url)
    if not d:
        return 0.3
    if d in HIGH_TRUST_DOMAINS:
        return 0.85
    if d in MED_TRUST_DOMAINS:
        return 0.6
    # fallback: treat unknown as low-mid
    return 0.45

# ============================================================================================================
# Parse RSS feeds
# ============================================================================================================

def parse_datetime(dt_str: str) -> Optional[datetime]:
    try:
        dt = dateutil_parser.parse(dt_str)
        if not dt.tzinfo:
            dt = bangkok_tz.localize(dt)
        return dt.astimezone(bangkok_tz)
    except Exception:
        return None

def fetch_feed(feed: Dict[str, str]) -> List[Dict[str, Any]]:
    url = feed["url"]
    country = feed.get("country", "").strip() or "Global"
    name = feed.get("name", "feed").strip()

    d = feedparser.parse(url)
    items = []
    for e in d.entries:
        link = e.get("link", "") or ""
        title = (e.get("title", "") or "").strip()
        summary = (e.get("summary", "") or e.get("description", "") or "").strip()

        published = None
        for k in ("published", "updated", "pubDate"):
            if e.get(k):
                published = parse_datetime(e.get(k))
                if published:
                    break
        items.append({
            "feed_name": name,
            "feed_country": country,
            "title": title,
            "summary": summary,
            "link": normalize_url(link),
            "published": published,
        })
    return items

def load_news() -> List[Dict[str, Any]]:
    all_items: List[Dict[str, Any]] = []
    for f in RSS_FEEDS:
        try:
            all_items.extend(fetch_feed(f))
        except Exception as e:
            print("Feed error:", f.get("name"), e)
    # basic sort newest first
    all_items.sort(key=lambda x: x.get("published") or datetime.min.replace(tzinfo=bangkok_tz), reverse=True)
    return all_items

def dedupe_news(items: List[Dict[str, Any]], sent: set) -> List[Dict[str, Any]]:
    out = []
    seen = set()
    for n in items:
        link = normalize_url(n.get("link", ""))
        if not link:
            continue
        if link in sent:
            continue
        if link in seen:
            continue
        seen.add(link)
        out.append(n)
    return out

# ============================================================================================================
# Project-mode LLM: tag & filter + impact rewrite (‡πÄ‡∏î‡∏¥‡∏°)
# ============================================================================================================

PROJECT_CATEGORIES = [
    "Energy Policy / Regulation",
    "Oil & Gas / Upstream",
    "Gas / LNG",
    "Power / Electricity",
    "Finance / FX / Macro",
    "Geopolitics / Sanctions",
    "Technology / Transition",
    "Other",
]

def groq_batch_tag_and_filter(news_list: List[Dict[str, Any]], chunk_size: int = 10) -> List[Dict[str, Any]]:
    """
    ‡∏Ñ‡∏∑‡∏ô list ‡∏Ç‡∏ô‡∏≤‡∏ô‡∏Å‡∏±‡∏ö news_list:
    {
      "pass": true/false,
      "country": "...",
      "project": "...",
      "impact": "..."   # bullet ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
      "category": "..."
    }
    """
    results: List[Dict[str, Any]] = []
    for i in range(0, len(news_list), chunk_size):
        chunk = news_list[i:i + chunk_size]
        payload = []
        for idx, n in enumerate(chunk):
            payload.append({
                "id": idx,
                "feed_country": (n.get("feed_country") or "").strip(),
                "title": n.get("title", ""),
                "summary": n.get("summary", ""),
            })

        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠ "‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®/‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"
‡πÉ‡∏´‡πâ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠: ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô ‡∏Ñ‡πà‡∏≤‡πÄ‡∏á‡∏¥‡∏ô ‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å‡∏™‡πå ‡∏´‡πà‡∏ß‡∏á‡πÇ‡∏ã‡πà‡∏≠‡∏∏‡∏õ‡∏ó‡∏≤‡∏ô ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô/‡∏Å‡πä‡∏≤‡∏ã/LNG/‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü ‡∏Ø‡∏•‡∏Ø
‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏¢‡πÉ‡∏´‡πâ pass=false

‡πÄ‡∏°‡∏∑‡πà‡∏≠ pass=true ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏:
- country: ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ feed_country ‡∏´‡∏£‡∏∑‡∏≠ "Global")
- project: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà "-")
- category: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ {json.dumps(PROJECT_CATEGORIES, ensure_ascii=False)}
- impact: bullet ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞ "‡∏¢‡∏≤‡∏ß‡∏û‡∏≠" (3-5 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ) ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£/‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥

‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°:
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å title/summary
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ PTTEP ‡πÉ‡∏ô impact

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
{{
  "items":[
    {{
      "id":0,
      "pass":true,
      "country":"Thailand",
      "project":"-",
      "category":"Power / Electricity",
      "impact":"..."
    }}
  ]
}}

‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ:
{json.dumps(payload, ensure_ascii=False)}
"""
        text = call_groq_with_retries(prompt, temperature=0.3, max_tokens=1400)
        data = _extract_json_object(text)

        if not (isinstance(data, dict) and isinstance(data.get("items"), list)):
            for _ in chunk:
                results.append({"pass": False})
            continue

        by_id = {}
        for it in data["items"]:
            if isinstance(it, dict) and "id" in it:
                by_id[it.get("id")] = it

        for idx, _n in enumerate(chunk):
            it = by_id.get(idx, {"pass": False})
            if not isinstance(it, dict):
                it = {"pass": False}
            results.append(it)

    return results

def enforce_thai(text: str) -> str:
    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô + ‡∏ñ‡πâ‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏•‡∏∏‡∏î ‡∏à‡∏∞ rewrite ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢
    if not text:
        return text
    # ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡πÉ‡∏´‡πâ rewrite
    eng = re.findall(r"[A-Za-z]{3,}", text)
    if len(eng) >= 4:
        prompt = f"""
‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô ‡∏≠‡πà‡∏≤‡∏ô‡∏•‡∏∑‡πà‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏¥‡∏°
‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:
{text}
"""
        try:
            out = call_groq_with_retries(prompt, temperature=0.2, max_tokens=900)
            return out.strip()
        except Exception:
            return text
    return text

# ============================================================================================================
# Digest-mode LLM: energy digest classify + summarize (‡πÉ‡∏´‡∏°‡πà)
# ============================================================================================================

DIGEST_CATEGORIES = [
    "domestic_policy",
    "domestic_lng",
    "domestic_tech_other",
    "intl_situation",
    "intl_lng",
    "intl_tech_other",
]

BUCKET_LABELS = {
    "domestic_policy": "üî∏‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô",
    "domestic_lng": "üî∏‡∏Ç‡πà‡∏≤‡∏ß‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ LNG",
    "domestic_tech_other": "üî∏‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ",
    "intl_situation": "üî∏‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô",
    "intl_lng": "üî∏‡∏Ç‡πà‡∏≤‡∏ß‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ LNG",
    "intl_tech_other": "üî∏‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ",
}

def groq_batch_energy_digest(news_list: List[Dict[str, Any]], chunk_size: int = 10) -> List[Dict[str, Any]]:
    """
    ‡∏Ñ‡∏∑‡∏ô list ‡∏Ç‡∏ô‡∏≤‡∏ô‡∏Å‡∏±‡∏ö news_list:
    {
      "is_energy": true/false,
      "bucket": one of DIGEST_CATEGORIES,
      "headline_th": "...",
      "summary_th": "..."   # 2-4 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÇ‡∏ó‡∏ô‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    }
    """
    results: List[Dict[str, Any]] = []
    for i in range(0, len(news_list), chunk_size):
        chunk = news_list[i:i + chunk_size]
        payload = []
        for idx, n in enumerate(chunk):
            payload.append({
                "id": idx,
                "feed_country": (n.get("feed_country") or "").strip(),
                "title": n.get("title", ""),
                "summary": n.get("summary", ""),
            })

        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ì‡∏≤‡∏ò‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

{STYLE_EXAMPLES}

‡∏á‡∏≤‡∏ô:
- ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πà‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô/‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü/‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô/‡∏Å‡πä‡∏≤‡∏ã/LNG/‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å‡∏™‡πå/‡∏Ñ‡πà‡∏≤‡πÄ‡∏á‡∏¥‡∏ô ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå is_energy=false

‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î bucket (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏ô‡∏µ‡πâ):
{json.dumps(DIGEST_CATEGORIES, ensure_ascii=False)}
‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤ bucket:
- ‡∏ñ‡πâ‡∏≤ feed_country ‡πÄ‡∏õ‡πá‡∏ô Thailand -> domestic_*
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Thailand -> intl_*
- policy = ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏£‡∏±‡∏ê/‡∏Å‡∏Å‡∏û./‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü/‡∏†‡∏≤‡∏©‡∏µ/‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ê/‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏¢‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ä‡∏±‡∏î
- lng = LNG/‡∏Å‡πä‡∏≤‡∏ã/‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢/‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Å‡πä‡∏≤‡∏ã
- situation = ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡∏•‡∏≤‡∏î/‡∏Ñ‡∏ß‡πà‡∏≥‡∏ö‡∏≤‡∏ï‡∏£/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏∂‡∏á‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î/‡∏≠‡∏∏‡∏õ‡∏ó‡∏≤‡∏ô-‡∏≠‡∏∏‡∏õ‡∏™‡∏á‡∏Ñ‡πå/‡∏Ç‡∏ô‡∏™‡πà‡∏á‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô
- tech_other = ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô/AI/‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå/‡πÅ‡∏ö‡∏ï/‡∏î‡∏≤‡∏ï‡πâ‡∏≤‡πÄ‡∏ã‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå:
- headline_th: ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏ó‡∏¢ 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏ö‡∏ö‡∏Ç‡πà‡∏≤‡∏ß)
- summary_th: 2‚Äì4 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô (‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô) ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥/‡∏ß‡∏•‡∏µ‡∏à‡∏≤‡∏Å title/summary ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏à‡∏∏‡∏î
‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°:
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å title/summary
- ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏¢‡∏≤‡∏ß ‡πÜ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡πà‡∏ô LNG, AI)

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
{{
  "items":[
    {{
      "id":0,
      "is_energy":true,
      "bucket":"domestic_policy",
      "headline_th":"...",
      "summary_th":"..."
    }}
  ]
}}

‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ:
{json.dumps(payload, ensure_ascii=False)}
"""
        text = call_groq_with_retries(prompt, temperature=0.25, max_tokens=1500)
        data = _extract_json_object(text)

        if not (isinstance(data, dict) and isinstance(data.get("items"), list)):
            for _ in chunk:
                results.append({"is_energy": False})
            continue

        by_id = {}
        for it in data["items"]:
            if isinstance(it, dict) and "id" in it:
                by_id[it.get("id")] = it

        for idx, _n in enumerate(chunk):
            it = by_id.get(idx, {"is_energy": False})
            if not isinstance(it, dict):
                it = {"is_energy": False}
            # enforce thai on outputs
            if it.get("is_energy"):
                it["headline_th"] = enforce_thai((it.get("headline_th") or "").strip())
                it["summary_th"] = enforce_thai((it.get("summary_th") or "").strip())
            results.append(it)

    return results

# ============================================================================================================
# Digest text formatting
# ============================================================================================================

THAI_MONTH_ABBR = ["‡∏°.‡∏Ñ.","‡∏Å.‡∏û.","‡∏°‡∏µ.‡∏Ñ.","‡πÄ‡∏°.‡∏¢.","‡∏û.‡∏Ñ.","‡∏°‡∏¥.‡∏¢.","‡∏Å.‡∏Ñ.","‡∏™.‡∏Ñ.","‡∏Å.‡∏¢.","‡∏ï.‡∏Ñ.","‡∏û.‡∏¢.","‡∏ò.‡∏Ñ."]

def thai_date_str(dt: datetime) -> str:
    dt = dt.astimezone(bangkok_tz)
    day = dt.day
    mon = THAI_MONTH_ABBR[dt.month - 1]
    year_be = dt.year + 543
    return f"{day} {mon} {year_be}"

def news_items_by_bucket(items: List[Dict[str, Any]], bucket: str) -> List[Dict[str, Any]]:
    xs = [x for x in items if (x.get("bucket") == bucket)]
    xs.sort(key=lambda z: z.get("published") or datetime.min.replace(tzinfo=bangkok_tz), reverse=True)
    return xs[:DIGEST_MAX_PER_SECTION]

def _render_section(items: List[Dict[str, Any]], with_summary: bool) -> str:
    if not items:
        return "-"

    lines = []
    for i, n in enumerate(items, 1):
        head = (n.get("headline_th") or n.get("title") or "").strip()
        summ = (n.get("summary_th") or "").strip()
        link = (n.get("final_url") or n.get("link") or "").strip()

        if with_summary:
            # ‚úÖ ‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: "1.‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á..." (‡πÑ‡∏°‡πà‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ)
            text = summ if summ else head
            lines.append(f"{i}.{text}")
            if link:
                lines.append(link)
        else:
            # ‚úÖ ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß: "1. ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô..." (‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ)
            lines.append(f"{i}. {head if head else (n.get('title') or '')}")

    return "\n".join(lines)

def build_energy_digest_text(news_items: List[Dict[str, Any]], report_dt: datetime, with_summary: bool) -> str:
    date_txt = thai_date_str(report_dt)
    title = "‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô" if with_summary else "‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"
    out = [f"{title} ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {date_txt}"]

    out.append("üîπ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\u202f ")
    for b in ["domestic_policy", "domestic_lng", "domestic_tech_other"]:
        out.append(BUCKET_LABELS[b])
        out.append(_render_section(news_items_by_bucket(news_items, b), with_summary))

    out.append("")
    out.append("üîπ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\u202f ")
    for b in ["intl_situation", "intl_lng", "intl_tech_other"]:
        out.append(BUCKET_LABELS[b])
        out.append(_render_section(news_items_by_bucket(news_items, b), with_summary))

    return "\n".join(out).strip()

def chunk_text_for_line(text: str, max_chars: int = 4500) -> List[str]:
    text = (text or "").strip()
    if len(text) <= max_chars:
        return [text]
    parts, buf = [], ""
    for line in text.split("\n"):
        if len(buf) + len(line) + 1 > max_chars:
            if buf.strip():
                parts.append(buf.strip())
            buf = line
        else:
            buf = (buf + "\n" + line) if buf else line
    if buf.strip():
        parts.append(buf.strip())
    return parts

def create_text_messages(text: str) -> List[Dict[str, Any]]:
    return [{"type": "text", "text": t} for t in chunk_text_for_line(text)]

# ============================================================================================================
# LINE Messaging API
# ============================================================================================================

LINE_PUSH_URL = "https://api.line.me/v2/bot/message/push"
LINE_BROADCAST_URL = "https://api.line.me/v2/bot/message/broadcast"

LINE_TARGET = os.getenv("LINE_TARGET", "broadcast").strip().lower()  # broadcast | user
LINE_USER_ID = os.getenv("LINE_USER_ID", "").strip()

def send_to_line(messages: List[Dict[str, Any]]) -> None:
    if DRY_RUN:
        print("[DRY_RUN] send_to_line messages:", json.dumps(messages, ensure_ascii=False)[:800], "...")
        return

    headers = {
        "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}",
        "Content-Type": "application/json",
    }

    if LINE_TARGET == "user":
        if not LINE_USER_ID:
            raise RuntimeError("LINE_TARGET=user ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_USER_ID")
        payload = {"to": LINE_USER_ID, "messages": messages}
        url = LINE_PUSH_URL
    else:
        payload = {"messages": messages}
        url = LINE_BROADCAST_URL

    r = requests.post(url, headers=headers, json=payload, timeout=60)
    if r.status_code >= 400:
        raise RuntimeError(f"LINE API error {r.status_code}: {r.text}")

# ============================================================================================================
# Optional: Flex message builder (‡πÄ‡∏î‡∏¥‡∏°) - ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ compatibility
# ============================================================================================================

def create_flex(news: Dict[str, Any]) -> Dict[str, Any]:
    # Minimal flex based on impact content
    hero = news.get("hero") or DEFAULT_HERO_URL
    title = (news.get("title") or "")[:80]
    impact = (news.get("impact") or "").strip()
    country = (news.get("country") or "-").strip()
    project = (news.get("project") or "-").strip()
    category = (news.get("category") or "-").strip()
    link = (news.get("final_url") or news.get("link") or "").strip()

    # Source rating
    score = news.get("source_score", 0.0)
    src_txt = f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: {score:.2f}" if SHOW_SOURCE_RATING else ""

    body_contents = [
        {"type": "text", "text": title, "weight": "bold", "wrap": True, "size": "md"},
        {"type": "text", "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {country}", "wrap": True, "size": "sm", "color": "#555555"},
        {"type": "text", "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {project}", "wrap": True, "size": "sm", "color": "#555555"},
        {"type": "text", "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {category}", "wrap": True, "size": "sm", "color": "#555555"},
    ]

    if src_txt:
        body_contents.append({"type": "text", "text": src_txt, "wrap": True, "size": "xs", "color": "#888888"})

    body_contents.append({"type": "separator", "margin": "md"})
    body_contents.append({"type": "text", "text": impact, "wrap": True, "size": "sm"})

    flex = {
        "type": "flex",
        "altText": title or "‡∏Ç‡πà‡∏≤‡∏ß",
        "contents": {
            "type": "bubble",
            "hero": {
                "type": "image",
                "url": hero,
                "size": "full",
                "aspectRatio": "20:13",
                "aspectMode": "cover",
            } if hero else None,
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [c for c in body_contents if c],
            },
            "footer": {
                "type": "box",
                "layout": "vertical",
                "spacing": "sm",
                "contents": [
                    {
                        "type": "button",
                        "style": "link",
                        "height": "sm",
                        "action": {"type": "uri", "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß", "uri": link or news.get("link") or ""},
                    }
                ],
                "flex": 0,
            },
        },
    }
    return flex

# ============================================================================================================
# Keyword gate (optional) (‡πÄ‡∏î‡∏¥‡∏°)
# ============================================================================================================

KEYWORDS = [
    "oil", "crude", "gas", "lng", "opec", "power", "electricity", "sanction",
    "pipeline", "refinery", "diesel", "gasoline", "brent", "wti", "dubai",
    "‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü", "‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô", "‡∏Å‡πä‡∏≤‡∏ã", "LNG", "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡∏Ñ‡∏ß‡πà‡∏≥‡∏ö‡∏≤‡∏ï‡∏£"
]

def keyword_hit(n: Dict[str, Any]) -> bool:
    t = (n.get("title") or "") + " " + (n.get("summary") or "")
    tl = t.lower()
    for kw in KEYWORDS:
        if kw.lower() in tl:
            return True
    return False

# ============================================================================================================
# Main pipeline
# ============================================================================================================

def prepare_items(raw: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    # Resolve final URLs & hero images & source score
    out = []
    for n in raw:
        link = n.get("link", "")
        if not link:
            continue
        final_url = resolve_final_url(link)
        hero = extract_og_image(final_url) or DEFAULT_HERO_URL
        sc = source_score(final_url)

        n2 = dict(n)
        n2["final_url"] = final_url
        n2["hero"] = hero
        n2["source_score"] = sc
        out.append(n2)
    return out

def run_project_mode(selected: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[str]]:
    """
    Returns:
      messages (LINE messages list)
      sent_links (links to track)
    """
    # Optional keyword gate (‡πÄ‡∏î‡∏¥‡∏°)
    if USE_KEYWORD_GATE:
        selected = [x for x in selected if keyword_hit(x)]

    # Score filter
    selected = [x for x in selected if (x.get("source_score", 0.0) >= MIN_SOURCE_SCORE)]

    # LLM tag & filter
    tags = groq_batch_tag_and_filter(selected, chunk_size=LLM_BATCH_SIZE)

    passed = []
    for n, t in zip(selected, tags):
        if not isinstance(t, dict) or not t.get("pass"):
            continue
        n2 = dict(n)
        n2["country"] = (t.get("country") or n.get("feed_country") or "Global").strip()
        n2["project"] = (t.get("project") or "-").strip()
        n2["category"] = (t.get("category") or "Other").strip()
        n2["impact"] = enforce_thai((t.get("impact") or "").strip())
        passed.append(n2)

    # Limit output
    passed.sort(key=lambda x: x.get("published") or datetime.min.replace(tzinfo=bangkok_tz), reverse=True)
    passed = passed[:PROJECT_SEND_LIMIT]

    if not passed:
        return (create_text_messages("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç"), [])

    # Build LINE messages: ‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Flex ‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Text ‡∏Å‡πá‡πÑ‡∏î‡πâ)
    msgs: List[Dict[str, Any]] = []
    for n in passed:
        msgs.append(create_flex(n))

    links = [x.get("link") for x in passed if x.get("link")]
    return (msgs, links)

def run_digest_mode(selected: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[str]]:
    """
    Returns:
      messages (LINE messages list)  -> text digest (2 ‡∏ä‡∏∏‡∏î: ‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç + ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß)
      sent_links (links to track)
    """
    digest_tags = groq_batch_energy_digest(selected, chunk_size=LLM_BATCH_SIZE)

    digest_items = []
    for n, tag in zip(selected, digest_tags):
        if not isinstance(tag, dict) or not tag.get("is_energy"):
            continue
        bucket = (tag.get("bucket") or "").strip()
        if bucket not in DIGEST_CATEGORIES:
            continue

        n2 = dict(n)
        n2["bucket"] = bucket
        n2["headline_th"] = (tag.get("headline_th") or "").strip()
        n2["summary_th"] = (tag.get("summary_th") or "").strip()
        digest_items.append(n2)

    if not digest_items:
        return (create_text_messages("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏°‡∏ß‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà"), [])

    report_dt = max(
        [x.get("published") for x in digest_items if x.get("published")],
        default=datetime.now(bangkok_tz),
    )

    text_full = build_energy_digest_text(digest_items, report_dt, with_summary=True)
    text_titles = build_energy_digest_text(digest_items, report_dt, with_summary=False)

    msgs: List[Dict[str, Any]] = []
    msgs += create_text_messages(text_full)
    msgs += create_text_messages("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    msgs += create_text_messages(text_titles)

    links = [x.get("link") for x in digest_items if x.get("link")]
    return (msgs, links)

def main():
    print("‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß...")
    raw = load_news()
    print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡∏¥‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:", len(raw))

    sent = load_sent_links()
    raw = dedupe_news(raw, sent)
    print("‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥/‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á:", len(raw))

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM (‡∏Ñ‡∏∏‡∏ì‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 80 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô
    selected = raw[:80]
    selected = prepare_items(selected)

    all_msgs: List[Dict[str, Any]] = []
    all_links: List[str] = []

    if OUTPUT_MODE not in ("both", "project_only", "digest_only"):
        print("OUTPUT_MODE ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á -> ‡πÉ‡∏ä‡πâ both")
        mode = "both"
    else:
        mode = OUTPUT_MODE

    if mode in ("both", "project_only"):
        if ADD_SECTION_HEADERS:
            all_msgs += create_text_messages("üìå ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ (Project Impact)")
        msgs, links = run_project_mode(selected)
        all_msgs += msgs
        all_links += links

    if mode == "both":
        if ADD_SECTION_HEADERS:
            all_msgs += create_text_messages("")

    if mode in ("both", "digest_only"):
        if ADD_SECTION_HEADERS:
            all_msgs += create_text_messages("üì∞ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô (Energy Digest)")
        msgs, links = run_digest_mode(selected)
        all_msgs += msgs
        all_links += links

    # ‡∏™‡πà‡∏á LINE
    send_to_line(all_msgs)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏±‡∏ô‡∏™‡πà‡∏á‡∏ã‡πâ‡∏≥
    save_sent_links([normalize_url(x) for x in all_links if x])

    print("‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:", len(all_msgs), "messages")

if __name__ == "__main__":
    main()
