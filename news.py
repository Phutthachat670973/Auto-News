# ============================================================================================================
# PTTEP Domestic News Bot (Google News SEARCH RSS - NO MAX LIMITS)
# - Pull ALL news (no MAX_ITEMS_PER_FEED / MAX_PER_COUNTRY / MAX_LLM_ITEMS / SEND_MAX)
# - Resolve Google News link -> publisher
# - Fetch source context (og:description/meta/lead paragraphs)
# - Gemini summarizes impact based on source context (no hallucination)
# - Send ALL items to LINE (split 10 bubbles per flex carousel automatically)
# ============================================================================================================

import os
import re
import json
import time
import random
import html as _html
from datetime import datetime, timedelta
from urllib.parse import (
    urlparse, urlunparse, parse_qsl, urlencode,
    quote_plus, unquote
)

import feedparser
from dateutil import parser as dateutil_parser
import pytz
import requests
import google.generativeai as genai

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ============================================================================================================
# ENV
# ============================================================================================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()

if not GEMINI_API_KEY:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY")
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_CHANNEL_ACCESS_TOKEN")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel(os.getenv("GEMINI_MODEL_NAME", "gemini-2.5-flash"))

GEMINI_DAILY_BUDGET = int(os.getenv("GEMINI_DAILY_BUDGET", "250"))
MAX_RETRIES = 6
SLEEP_BETWEEN_CALLS = (0.6, 1.2)
DRY_RUN = os.getenv("DRY_RUN", "false").lower() == "true"

# Rolling window (‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ RSS ‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô)
HOURS_BACK = int(os.getenv("HOURS_BACK", "12"))

# ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß (‡∏ï‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô token)
SOURCE_CONTEXT_MAX_CHARS = int(os.getenv("SOURCE_CONTEXT_MAX_CHARS", "1400"))

# ‡∏´‡∏ô‡πà‡∏ß‡∏á‡∏ï‡∏≠‡∏ô‡∏¢‡∏¥‡∏á LINE ‡∏Å‡∏±‡∏ô‡∏ñ‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô
SLEEP_BETWEEN_LINE_SEND = float(os.getenv("SLEEP_BETWEEN_LINE_SEND", "0.25"))

bangkok_tz = pytz.timezone("Asia/Bangkok")

S = requests.Session()
S.headers.update({"User-Agent": "Mozilla/5.0"})
TIMEOUT = 15

SENT_LINKS_DIR = "sent_links"
os.makedirs(SENT_LINKS_DIR, exist_ok=True)

DEFAULT_ICON_URL = "https://scdn.line-apps.com/n/channel_devcenter/img/fx/01_1_cafe.png"

# ============================================================================================================
# CONTEXT: Countries & Projects
# ============================================================================================================
PROJECT_COUNTRIES = [
    "Thailand", "Myanmar", "Vietnam", "Malaysia", "Indonesia",
    "UAE", "Oman", "Algeria", "Mozambique", "Australia", "Brazil", "Mexico"
]

PROJECTS_BY_COUNTRY = {
    "Thailand": ["G1/61", "G2/61", "Arthit", "Sinphuhorm", "MTJDA Block A-18"],
    "Myanmar": ["Zawtika", "Yadana", "Yetagun"],
    "Vietnam": ["Block B & 48/95", "Block 52/97", "16-1 (Te Giac Trang)"],
    "Malaysia": ["MTJDA Block A-18", "SK309", "SK311", "SK410B"],
    "Indonesia": ["South Sageri", "South Mandar", "Malunda"],
    "UAE": ["Ghasha Concession", "Abu Dhabi Offshore"],
    "Oman": ["Oman Block 12"],
    "Algeria": ["Bir Seba", "Hirad", "Touat"],
    "Mozambique": ["Mozambique Area 1 (Rovuma LNG)"],
    "Australia": ["Montara", "Timor Sea / Browse Basin"],
    "Brazil": ["BM-ES-23", "BM-ES-24"],
    "Mexico": ["Mexico Block 12 (2.4)"],
}

PTTEP_PROJECTS_CONTEXT = r"""
[PTTEP_PROJECTS_CONTEXT]
Thailand - G1/61, G2/61, Arthit, Sinphuhorm, MTJDA Block A-18
Myanmar ‚Äì Zawtika, Yadana, Yetagun
Vietnam ‚Äì Block B & 48/95, Block 52/97, 16-1 (Te Giac Trang)
Malaysia ‚Äì MTJDA Block A-18, SK309, SK311, SK410B
Indonesia ‚Äì South Sageri, South Mandar, Malunda
UAE ‚Äì Ghasha Concession, Abu Dhabi Offshore
Oman ‚Äì Oman Block 12
Algeria ‚Äì Bir Seba, Hirad, Touat
Mozambique ‚Äì Mozambique Area 1 (Rovuma LNG)
Australia ‚Äì Montara, Timor Sea / Browse Basin
Brazil ‚Äì BM-ES-23, BM-ES-24
Mexico ‚Äì Mexico Block 12 (2.4)
"""

# ============================================================================================================
# Google News SEARCH RSS (old style) + broad topic guardrail
# ============================================================================================================
COUNTRY_QUERY = {
    "Thailand": "Thailand OR ‡πÑ‡∏ó‡∏¢ OR ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ OR Bangkok",
    "Myanmar": "Myanmar OR Burma OR ‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤ OR ‡∏û‡∏°‡πà‡∏≤",
    "Vietnam": "Vietnam OR ‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏°",
    "Malaysia": "Malaysia OR ‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢",
    "Indonesia": "Indonesia OR ‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢",
    "UAE": "UAE OR \"United Arab Emirates\" OR Abu Dhabi OR Dubai OR ‡∏™‡∏´‡∏£‡∏±‡∏ê‡∏≠‡∏≤‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏°‡∏¥‡πÄ‡∏£‡∏ï‡∏™‡πå",
    "Oman": "Oman OR ‡πÇ‡∏≠‡∏°‡∏≤‡∏ô",
    "Algeria": "Algeria OR ‡πÅ‡∏≠‡∏•‡∏à‡∏µ‡πÄ‡∏£‡∏µ‡∏¢",
    "Mozambique": "Mozambique OR ‡πÇ‡∏°‡∏ã‡∏±‡∏°‡∏ö‡∏¥‡∏Å OR Rovuma",
    "Australia": "Australia OR ‡∏≠‡∏≠‡∏™‡πÄ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢",
    "Brazil": "Brazil OR ‡∏ö‡∏£‡∏≤‡∏ã‡∏¥‡∏•",
    "Mexico": "Mexico OR ‡πÄ‡∏°‡πá‡∏Å‡∏ã‡∏¥‡πÇ‡∏Å",
}

TOPIC_GUARDRAIL = (
    "(economy OR economic OR inflation OR gdp OR currency OR rate OR bond OR trade OR tariff OR "
    "politics OR election OR government OR policy OR tax OR regulation OR ministry OR "
    "energy OR oil OR gas OR lng OR pipeline OR power OR electricity OR upstream OR "
    "sanction OR protest OR strike OR conflict OR security)"
)

def google_news_search_rss(q: str, hl="en", gl="US", ceid="US:en"):
    q2 = f"({q}) {TOPIC_GUARDRAIL} when:1d"
    return f"https://news.google.com/rss/search?q={quote_plus(q2)}&hl={hl}&gl={gl}&ceid={ceid}"

NEWS_FEEDS = [("GoogleNews", c, google_news_search_rss(COUNTRY_QUERY[c])) for c in PROJECT_COUNTRIES]

# ============================================================================================================
# HELPERS
# ============================================================================================================
def clean_text(s: str) -> str:
    if not s:
        return ""
    s = str(s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = _html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _normalize_link(url: str) -> str:
    try:
        p = urlparse(url)
        netloc = p.netloc.lower()
        scheme = (p.scheme or "https").lower()
        drop = {"fbclid", "gclid", "ref", "mc_cid", "mc_eid"}
        new_q = [
            (k, v)
            for k, v in parse_qsl(p.query, keep_blank_values=True)
            if not (k.startswith("utm_") or k in drop)
        ]
        return urlunparse(p._replace(scheme=scheme, netloc=netloc, query=urlencode(new_q)))
    except Exception:
        return (url or "").strip()

def get_sent_links_file(date=None):
    if date is None:
        date = datetime.now(bangkok_tz).strftime("%Y-%m-%d")
    return os.path.join(SENT_LINKS_DIR, f"{date}.txt")

def load_sent_links():
    sent = set()
    today_str = datetime.now(bangkok_tz).strftime("%Y-%m-%d")
    p = get_sent_links_file(today_str)
    if os.path.exists(p):
        with open(p, "r", encoding="utf-8") as f:
            for line in f:
                u = _normalize_link(line.strip())
                if u:
                    sent.add(u)
    return sent

def save_sent_links(links):
    p = get_sent_links_file()
    with open(p, "a", encoding="utf-8") as f:
        for l in links:
            f.write(_normalize_link(l) + "\n")

def _impact_to_bullets(text: str):
    if not text:
        return ["‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß (‡∏™‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ï‡πà‡∏≠)"]
    text = clean_text(text)
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    if not lines:
        lines = [text.strip()]
    bullets = []
    for line in lines:
        s = line.strip()
        s = re.sub(r"^[\u2022\*\-\u00b7¬∑‚Ä¢\s]+", "", s)
        s = re.sub(r"^\d+[\.\)]\s*", "", s)
        if s:
            bullets.append(s)
    return bullets or ["‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß (‡∏™‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ï‡πà‡∏≠)"]

def _extract_json_object(raw: str):
    if not raw:
        return None
    s = raw.strip()
    if s.startswith("```"):
        s = re.sub(r"^```(json)?", "", s, flags=re.I).strip()
        s = re.sub(r"```$", "", s).strip()
    try:
        return json.loads(s)
    except Exception:
        pass
    first, last = s.find("{"), s.rfind("}")
    if first != -1 and last != -1 and last > first:
        try:
            return json.loads(s[first:last + 1])
        except Exception:
            return None
    return None

# ============================================================================================================
# Resolve Google News -> publisher link
# ============================================================================================================
def resolve_google_news_url(url: str) -> str:
    if not url:
        return ""
    if "news.google.com" not in url:
        return url
    try:
        r = S.get(url, timeout=TIMEOUT, allow_redirects=True)
        html = r.text or ""
        m = re.search(r'https?://www\.google\.[^/]+/url\?[^"\']*url=([^&"\']+)', html)
        if m:
            return unquote(m.group(1))
        m = re.search(r'href="(https?://[^"]+)"', html, flags=re.I)
        if m and "news.google.com" not in m.group(1):
            return m.group(1)
        if r.url and "news.google.com" not in r.url:
            return r.url
    except Exception:
        pass
    return url

# ============================================================================================================
# Fetch source context (og:description / meta description / first paragraphs)
# ============================================================================================================
def fetch_source_context(url: str) -> str:
    if not url:
        return ""
    try:
        r = S.get(url, timeout=TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return ""
        html = r.text or ""

        def _meta(patterns):
            for pat in patterns:
                m = re.search(pat, html, re.I)
                if m:
                    return clean_text(m.group(1))
            return ""

        og_title = _meta([r'<meta[^>]+property=[\'"]og:title[\'"][^>]+content=[\'"]([^\'"]+)[\'"]'])
        og_desc  = _meta([r'<meta[^>]+property=[\'"]og:description[\'"][^>]+content=[\'"]([^\'"]+)[\'"]'])
        meta_desc = _meta([r'<meta[^>]+name=[\'"]description[\'"][^>]+content=[\'"]([^\'"]+)[\'"]'])

        paras = re.findall(r"<p[^>]*>(.*?)</p>", html, flags=re.I | re.S)
        para_texts = []
        for p in paras[:10]:
            t = clean_text(p)
            if len(t) >= 40:
                para_texts.append(t)
        lead = " ".join(para_texts[:3])

        parts = []
        if og_title: parts.append(f"Title: {og_title}")
        if og_desc: parts.append(f"OG_Desc: {og_desc}")
        if meta_desc and meta_desc != og_desc: parts.append(f"Meta_Desc: {meta_desc}")
        if lead: parts.append(f"Lead: {lead}")

        ctx = "\n".join(parts).strip()
        if len(ctx) > SOURCE_CONTEXT_MAX_CHARS:
            ctx = ctx[:SOURCE_CONTEXT_MAX_CHARS].rstrip() + "‚Ä¶"
        return ctx
    except Exception:
        return ""

# ============================================================================================================
# Fetch og:image
# ============================================================================================================
def fetch_article_image(url: str) -> str:
    if not url:
        return ""
    try:
        r = S.get(url, timeout=TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return ""
        html = r.text or ""

        m = re.search(r'<meta[^>]+property=[\'"]og:image[\'"][^>]+content=[\'"]([^\'"]+)[\'"]', html, re.I)
        if m:
            return m.group(1)
        m = re.search(r'<meta[^>]+name=[\'"]twitter:image[\'"][^>]+content=[\'"]([^\'"]+)[\'"]', html, re.I)
        if m:
            return m.group(1)
        m = re.search(r'<img[^>]+src=[\'"]([^\'"]+)[\'"]', html, re.I)
        if m:
            src = m.group(1)
            if src.startswith("//"):
                parsed = urlparse(url)
                return f"{parsed.scheme}:{src}"
            if src.startswith("/"):
                parsed = urlparse(url)
                return f"{parsed.scheme}://{parsed.netloc}{src}"
            return src
        return ""
    except Exception:
        return ""

# ============================================================================================================
# Gemini wrapper
# ============================================================================================================
GEMINI_CALLS = 0

def call_gemini(prompt: str, want_json: bool = False):
    global GEMINI_CALLS
    if GEMINI_CALLS >= GEMINI_DAILY_BUDGET:
        raise RuntimeError("‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ Gemini ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô")

    last_error = None
    for i in range(1, MAX_RETRIES + 1):
        try:
            gen_cfg = {"temperature": 0.2, "max_output_tokens": 950}
            if want_json:
                gen_cfg["response_mime_type"] = "application/json"
            try:
                r = model.generate_content(prompt, generation_config=gen_cfg)
            except TypeError:
                r = model.generate_content(prompt)
            GEMINI_CALLS += 1
            return r
        except Exception as e:
            last_error = e
            if any(x in str(e) for x in ["429", "unavailable", "deadline", "503", "500"]) and i < MAX_RETRIES:
                time.sleep(5 * i)
                continue
            raise e
    raise last_error

def gemini_score_and_impact(news):
    feed_country = (news.get("feed_country") or "").strip()
    allowed_projects = PROJECTS_BY_COUNTRY.get(feed_country, [])

    schema = {
        "type": "object",
        "properties": {
            "is_relevant": {"type": "boolean"},
            "relevance_score": {"type": "integer", "minimum": 0, "maximum": 100},
            "impact_strength": {"type": "string", "enum": ["high", "medium", "low"]},
            "summary": {"type": "string"},
            "impact_reason": {"type": "string"},
            "country": {"type": "string"},
            "projects": {"type": "array", "items": {"type": "string"}},
        },
        "required": ["is_relevant", "relevance_score", "impact_strength"],
    }

    prompt = f"""
{PTTEP_PROJECTS_CONTEXT}

‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì: Analyst + News Screener ‡∏Ç‡∏≠‡∏á PTTEP

‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤ (STRICT):
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® {feed_country}" ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏µ‡πâ ‚Üí is_relevant=false, relevance_score ‡∏ï‡πà‡∏≥
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô relevance_score (0-100) ‡∏ï‡∏≤‡∏° "‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏µ‡πâ"
- impact_strength:
  - high: ‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏/‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏á‡∏ö/‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö/‡∏†‡∏≤‡∏©‡∏µ/‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏ó‡∏µ‡πà‡∏ä‡∏µ‡πâ‡∏ä‡∏±‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≠ cost/schedule/supply/security
  - medium: ‡∏°‡∏µ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à/‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà specific ‡∏°‡∏≤‡∏Å
  - low: ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ/soft news ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏¢‡∏≤‡∏Å

‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: impact_reason ‡∏ï‡πâ‡∏≠‡∏á ‚Äú‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‚Äù ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏•‡∏≠‡∏¢ ‡πÜ ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÇ‡∏¢‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å source_context
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô bullet 1‚Äì3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤ "‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á" (‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô/‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á/‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô)
- ‡∏ñ‡πâ‡∏≤‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ß‡πà‡∏≤ "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß" + ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏≠‡∏∞‡πÑ‡∏£

projects:
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ALL
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô: {allowed_projects}
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1‚Äì2 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏µ‡πâ

‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏Ç‡πà‡∏≤‡∏ß:
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {news['title']}
‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å RSS: {news['summary']}
source_context: {news.get('source_context','')}

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ï‡∏≤‡∏° schema ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
{json.dumps(schema, ensure_ascii=False)}
"""

    r = call_gemini(prompt, want_json=True)
    raw = (getattr(r, "text", "") or "").strip()
    data = _extract_json_object(raw)
    if not isinstance(data, dict):
        return {"is_relevant": False, "relevance_score": 0, "impact_strength": "low"}

    if isinstance(data.get("summary"), str):
        data["summary"] = clean_text(data["summary"])
    if isinstance(data.get("impact_reason"), str):
        data["impact_reason"] = clean_text(data["impact_reason"])

    # normalize projects
    projs = data.get("projects") or []
    if not isinstance(projs, list):
        projs = [str(projs)]
    projs = [p for p in projs if isinstance(p, str) and p.strip().lower() != "all"]
    projs = [p for p in projs if p in allowed_projects]
    if not projs:
        projs = allowed_projects[:2] if allowed_projects else []
    data["projects"] = projs

    # enforce country
    data["country"] = feed_country

    # clamp score
    try:
        sc = int(data.get("relevance_score", 0))
    except Exception:
        sc = 0
    data["relevance_score"] = max(0, min(100, sc))

    if data.get("impact_strength") not in ("high", "medium", "low"):
        data["impact_strength"] = "low"

    return data

# ============================================================================================================
# Fetch ALL news in window (no per-feed max)
# ============================================================================================================
def fetch_news_window():
    now_local = datetime.now(bangkok_tz)
    start = now_local - timedelta(hours=HOURS_BACK)
    end = now_local

    out = []
    for site, feed_country, url in NEWS_FEEDS:
        try:
            feed = feedparser.parse(url)
            for e in feed.entries:
                pub = getattr(e, "published", None) or getattr(e, "updated", None)
                if not pub:
                    continue

                dt = dateutil_parser.parse(pub)
                if dt.tzinfo is None:
                    dt = pytz.UTC.localize(dt)
                dt = dt.astimezone(bangkok_tz)

                if start <= dt <= end:
                    title = clean_text(getattr(e, "title", "") or "")
                    summary = clean_text(getattr(e, "summary", "") or "")
                    link_google = getattr(e, "link", "") or ""
                    link_real = resolve_google_news_url(link_google)

                    out.append({
                        "site": site,
                        "feed_country": feed_country,
                        "title": title,
                        "summary": summary,
                        "link_google": link_google,
                        "link": link_real,
                        "published": dt,
                        "date": dt.strftime("%d/%m/%Y %H:%M"),
                    })
        except Exception:
            pass

    # dedupe by real link
    uniq, seen = [], set()
    for n in out:
        k = _normalize_link(n.get("link", ""))
        if k and k not in seen:
            seen.add(k)
            uniq.append(n)

    uniq.sort(key=lambda x: x["published"], reverse=True)
    return uniq

# ============================================================================================================
# FLEX (split 10/batch)
# ============================================================================================================
def create_flex(news_items):
    now_txt = datetime.now(bangkok_tz).strftime("%d/%m/%Y")
    bubbles = []

    for n in news_items:
        bullets = _impact_to_bullets(n.get("impact_reason", ""))

        link = n.get("link") or "https://news.google.com/"
        img = n.get("image") or DEFAULT_ICON_URL
        if not (isinstance(img, str) and img.startswith(("http://", "https://"))):
            img = DEFAULT_ICON_URL

        country_txt = (n.get("country") or n.get("feed_country") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏").strip()
        projects = n.get("projects") or []
        proj_txt = ", ".join(projects[:3]) if isinstance(projects, list) and projects else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"

        summary_txt = clean_text(n.get("summary_llm") or n.get("summary") or "")
        if len(summary_txt) > 260:
            summary_txt = summary_txt[:260].rstrip() + "‚Ä¶"

        score = n.get("relevance_score", 0)
        strength = n.get("impact_strength", "low")
        note = n.get("note", "")

        body_contents = [
            {"type": "text", "text": n["title"], "weight": "bold", "size": "lg", "wrap": True},
            {"type": "text", "text": f"üóì {n['date']}", "size": "xs", "color": "#888888", "margin": "sm"},
            {"type": "text", "text": f"üåç {country_txt} | {n['site']}", "size": "xs", "color": "#448AFF", "margin": "xs"},
            {"type": "text", "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {proj_txt}", "size": "xs", "color": "#555555", "margin": "sm", "wrap": True},
            {"type": "text", "text": f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {score}/100 | ‡∏£‡∏∞‡∏î‡∏±‡∏ö: {strength}", "size": "xs", "color": "#555555", "margin": "sm", "wrap": True},
        ]

        if note:
            body_contents.append({"type": "text", "text": note, "size": "xs", "color": "#999999", "margin": "xs", "wrap": True})

        if summary_txt:
            body_contents.append({
                "type": "text",
                "text": f"‡∏™‡∏£‡∏∏‡∏õ: {summary_txt}",
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#111111",
            })

        impact_box = {
            "type": "box",
            "layout": "vertical",
            "margin": "lg",
            "contents": [{"type": "text", "text": "‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ (‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß)", "size": "lg", "weight": "bold"}]
            + [{"type": "text", "text": f"‚Ä¢ {b}", "wrap": True, "size": "md", "weight": "bold", "margin": "xs"} for b in bullets],
        }
        body_contents.append(impact_box)

        bubbles.append({
            "type": "bubble",
            "size": "mega",
            "hero": {"type": "image", "url": img, "size": "full", "aspectRatio": "16:9", "aspectMode": "cover"},
            "body": {"type": "box", "layout": "vertical", "contents": body_contents},
            "footer": {"type": "box", "layout": "vertical", "contents": [
                {"type": "button", "style": "primary", "color": "#1DB446",
                 "action": {"type": "uri", "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "uri": link}}
            ]},
        })

    messages = []
    for i in range(0, len(bubbles), 10):
        chunk = bubbles[i:i + 10]
        part = (i // 10) + 1
        messages.append({
            "type": "flex",
            "altText": f"‡∏Ç‡πà‡∏≤‡∏ß PTTEP (Domestic) {now_txt} [{part}]",
            "contents": {"type": "carousel", "contents": chunk},
        })
    return messages

# ============================================================================================================
# LINE broadcast
# ============================================================================================================
def send_to_line(messages):
    url = "https://api.line.me/v2/bot/message/broadcast"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}",
    }

    for i, msg in enumerate(messages, 1):
        payload = {"messages": [msg]}
        print("=== LINE PAYLOAD ===")
        print(json.dumps(payload, ensure_ascii=False, indent=2))

        if DRY_RUN:
            print("[DRY_RUN] ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ DRY_RUN = true")
            continue

        r = S.post(url, headers=headers, json=payload, timeout=15)
        print(f"Send {i}/{len(messages)}: {r.status_code}")
        print("Response body:", r.text)

        if r.status_code >= 300:
            print("‡∏´‡∏¢‡∏∏‡∏î‡∏™‡πà‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞ LINE ‡∏ï‡∏≠‡∏ö error")
            break

        time.sleep(SLEEP_BETWEEN_LINE_SEND)

# ============================================================================================================
# MAIN
# ============================================================================================================
def main():
    print("‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß...")
    all_news = fetch_news_window()
    print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡∏¥‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:", len(all_news))

    if not all_news:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤")
        return

    sent = load_sent_links()

    # ‡πÄ‡∏≠‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà Max) ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏ï‡∏±‡∏î‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    candidates = []
    for n in all_news:
        link_norm = _normalize_link(n.get("link", ""))
        if link_norm and link_norm in sent:
            continue
        candidates.append(n)

    print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà (‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡∏ã‡πâ‡∏≥‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ):", len(candidates))
    if not candidates:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà")
        return

    tagged = []
    quota_exhausted = False

    for idx, n in enumerate(candidates, 1):
        print(f"[{idx}/{len(candidates)}] ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° source_context: ({n.get('feed_country')}) {n['title'][:80]}...")

        # ‡∏î‡∏∂‡∏á context ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á (‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö ‚Äú‡∏≠‡∏¥‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‚Äù)
        n["source_context"] = fetch_source_context(n.get("link", ""))

        if not quota_exhausted:
            try:
                print(f"    -> Gemini score+impact (calls={GEMINI_CALLS}/{GEMINI_DAILY_BUDGET})")
                tag = gemini_score_and_impact(n)

                n["country"] = (tag.get("country") or n.get("feed_country") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏").strip()
                n["projects"] = tag.get("projects") or PROJECTS_BY_COUNTRY.get(n["country"], [])[:2]
                n["summary_llm"] = clean_text(tag.get("summary") or n.get("summary") or n["title"])
                n["impact_reason"] = clean_text(tag.get("impact_reason") or "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß (‡∏™‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ï‡πà‡∏≠)")
                n["relevance_score"] = int(tag.get("relevance_score", 0) or 0)
                n["impact_strength"] = tag.get("impact_strength", "low")
                n["is_relevant"] = bool(tag.get("is_relevant", False))
            except RuntimeError as e:
                # ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏° -> ‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ß‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ
                if "‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤" in str(e) or "quota" in str(e).lower():
                    quota_exhausted = True
                    n["note"] = "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ Gemini ‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πà‡∏≤‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ LLM"
                else:
                    n["note"] = f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: LLM error ({str(e)[:120]})"
                # fallback ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ß
                n["country"] = (n.get("feed_country") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏").strip()
                n["projects"] = PROJECTS_BY_COUNTRY.get(n["country"], [])[:2]
                n["summary_llm"] = clean_text(n.get("summary") or n["title"])
                n["impact_reason"] = "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤/‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î) ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß"
                n["relevance_score"] = 0
                n["impact_strength"] = "low"
                n["is_relevant"] = False
            except Exception as e:
                n["note"] = f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: LLM error ({str(e)[:120]})"
                n["country"] = (n.get("feed_country") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏").strip()
                n["projects"] = PROJECTS_BY_COUNTRY.get(n["country"], [])[:2]
                n["summary_llm"] = clean_text(n.get("summary") or n["title"])
                n["impact_reason"] = "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏î‡πâ‡∏ß‡∏¢ LLM (‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î) ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß"
                n["relevance_score"] = 0
                n["impact_strength"] = "low"
                n["is_relevant"] = False
        else:
            # ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡πâ‡∏ß -> ‡∏™‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÅ‡∏ï‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö
            n["note"] = "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏°) ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß"
            n["country"] = (n.get("feed_country") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏").strip()
            n["projects"] = PROJECTS_BY_COUNTRY.get(n["country"], [])[:2]
            n["summary_llm"] = clean_text(n.get("summary") or n["title"])
            n["impact_reason"] = "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏°) ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß"
            n["relevance_score"] = 0
            n["impact_strength"] = "low"
            n["is_relevant"] = False

        tagged.append(n)
        time.sleep(random.uniform(*SLEEP_BETWEEN_CALLS))

    print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á:", len(tagged))

    # ‡∏£‡∏π‡∏õ‡∏õ‡∏Å: ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å publisher ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ default
    for i, n in enumerate(tagged, 1):
        print(f"[img {i}/{len(tagged)}] ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ: {n.get('title','')[:60]}...")
        img = fetch_article_image(n.get("link", ""))
        if not (isinstance(img, str) and img.startswith(("http://", "https://"))):
            img = DEFAULT_ICON_URL
        n["image"] = img
        time.sleep(0.15)

    msgs = create_flex(tagged)
    print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô flex messages ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á (10 ‡∏Ç‡πà‡∏≤‡∏ß/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°):", len(msgs))

    send_to_line(msgs)
    save_sent_links([n["link"] for n in tagged])

    print("‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

if __name__ == "__main__":
    main()
