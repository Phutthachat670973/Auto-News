# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote

import requests
import feedparser
import pytz
from dateutil import parser as dateutil_parser

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# =============================================================================
# ENV / CONFIG
# =============================================================================
TZ = pytz.timezone(os.getenv("TZ", "Asia/Bangkok"))

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE_CHANNEL_ACCESS_TOKEN")

# Groq Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant").strip()
GROQ_ENDPOINT = os.getenv("GROQ_ENDPOINT", "https://api.groq.com/openai/v1/chat/completions").strip()
USE_LLM_SUMMARY = os.getenv("USE_LLM_SUMMARY", "1").strip().lower() in ["1", "true", "yes", "y"]

WINDOW_HOURS = int(os.getenv("WINDOW_HOURS", "48"))
MAX_PER_FEED = int(os.getenv("MAX_PER_FEED", "25"))
DRY_RUN = os.getenv("DRY_RUN", "0").strip().lower() in ["1", "true", "yes", "y"]
MAX_MESSAGES_PER_RUN = int(os.getenv("MAX_MESSAGES_PER_RUN", "10"))
BUBBLES_PER_CAROUSEL = int(os.getenv("BUBBLES_PER_CAROUSEL", "10"))

# Sent links tracking
SENT_DIR = os.getenv("SENT_DIR", "sent_links")
os.makedirs(SENT_DIR, exist_ok=True)

# =============================================================================
# TEXT CLEANER
# =============================================================================
class TextCleaner:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    
    BAD_PHRASES = [
        "‡∏™‡∏≤‡∏¢‡∏ö‡∏≥‡∏£‡∏ì‡∏µ‡∏°", "‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏≥‡∏£‡∏ì‡∏µ‡∏°", "‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏™‡∏±‡∏ô‡∏ã‡∏¥‡∏õ‡πÑ‡∏ï‡∏¢‡πÄ‡∏™‡∏µ‡∏¢‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≤‡∏¢‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡∏ü‡πâ‡∏≤",
        "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°", "‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°",
        "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡πà‡∏≤‡∏ß", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß", "‡πÅ‡∏ä‡∏£‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ",
        "Advertisement", "Promoted", "Sponsored", "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤",
        "Click here to read more", "Read full story", "Continue reading",
        "‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°", "‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà", "‡∏î‡∏π‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
    ]
    
    @staticmethod
    def clean_text(text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not text:
            return ""
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        for phrase in TextCleaner.BAD_PHRASES:
            text = text.replace(phrase, "")
        
        # ‡∏•‡∏ö HTML tags ‡πÅ‡∏•‡∏∞ entities
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'&[a-z]+;', '', text)
        
        # ‡∏•‡∏ö URL
        text = re.sub(r'https?://\S+', '', text)
        
        # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        text = re.sub(r'[.,!?;:]{3,}', '...', text)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        return text.strip()
    
    @staticmethod
    def extract_meaningful_summary(text: str, max_length: int = 200) -> str:
        """‡∏î‡∏∂‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        text = TextCleaner.clean_text(text)
        if not text:
            return ""
        
        # ‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        sentences = re.split(r'[.!?]+\s*', text)
        
        # ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£)
        meaningful_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence.split()) >= 5:  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏≥
                meaningful_sentences.append(sentence)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
        if not meaningful_sentences:
            return text[:max_length]
        
        # ‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
        summary = ' '.join(meaningful_sentences[:2])  # ‡πÉ‡∏ä‡πâ 2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÅ‡∏£‡∏Å
        return summary[:max_length]

# =============================================================================
# URL VALIDATOR
# =============================================================================
class URLValidator:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡∏¥‡∏á‡∏Å‡πå URL"""
    
    @staticmethod
    def is_valid_url(url: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ"""
        if not url or not isinstance(url, str):
            return False
        
        url = url.strip()
        if not url:
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß (LINE ‡∏à‡∏≥‡∏Å‡∏±‡∏î 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
        if len(url) > 1000:
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö URL
        try:
            result = urlparse(url)
            if not all([result.scheme, result.netloc]):
                return False
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö scheme
            if result.scheme not in ['http', 'https']:
                return False
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö domain
            if len(result.netloc) < 3:
                return False
                
            return True
        except:
            return False
    
    @staticmethod
    def extract_actual_url(google_news_url: str) -> str:
        """‡∏î‡∏∂‡∏á URL ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Google News URL"""
        if not google_news_url:
            return ""
        
        try:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Google News URL ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á URL ‡∏à‡∏£‡∏¥‡∏á
            if "news.google.com" in google_news_url:
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å query parameter
                parsed = urlparse(google_news_url)
                query_params = parse_qs(parsed.query)
                
                if 'url' in query_params:
                    actual_url = unquote(query_params['url'][0])
                    if URLValidator.is_valid_url(actual_url):
                        return actual_url
                
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏≤‡∏° redirect 1 ‡∏£‡∏∞‡∏î‡∏±‡∏ö
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Accept': 'text/html,application/xhtml+xml',
                    }
                    response = requests.get(
                        google_news_url, 
                        headers=headers, 
                        timeout=5, 
                        allow_redirects=False
                    )
                    
                    if response.status_code in [301, 302, 303, 307, 308]:
                        location = response.headers.get('Location')
                        if location and URLValidator.is_valid_url(location):
                            return location
                except:
                    pass
        except:
            pass
        
        return google_news_url

# =============================================================================
# PROJECT DATABASE
# =============================================================================
PROJECTS_BY_COUNTRY = {
    "Thailand": [
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 2/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå", "Arthit",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏™ 1", "S1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4", "Contract 4",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ‡∏≠‡∏µ‡∏û‡∏µ 1", "PTTEP 1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 6/27",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 22/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏µ 5", "E5",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏†‡∏π‡∏Æ‡πà‡∏≠‡∏°", "Sinphuhorm",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 8/32", "B8/32", "9A", "9‡πÄ‡∏≠",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/48", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 12/48",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/65", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 3/65",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 53/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 54/43"
    ],
    "Myanmar": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏ï‡∏¥‡∏Å‡πâ‡∏≤", "Zawtika", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏≤‡∏î‡∏≤‡∏ô‡∏≤", "Yadana", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤ ‡πÄ‡∏≠‡πá‡∏° 3", "Myanmar M3"],
    "Malaysia": ["Malaysia SK309", "SK309", "Malaysia SK311", "SK311", "Malaysia Block H", "Block H"],
    "Vietnam": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° 16-1", "Vietnam 16-1", "16-1", "Block B", "48/95"],
    "Indonesia": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏ó‡∏π‡∏ô‡πà‡∏≤ ‡∏ã‡∏µ ‡πÄ‡∏≠", "Natuna Sea A"],
    "Kazakhstan": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∏‡∏á‡∏Å‡∏≤", "Dunga"],
    "Oman": ["Oman Block 61", "Block 61", "Oman Block 6", "PDO"],
    "UAE": ["Abu Dhabi Offshore 1", "Abu Dhabi Offshore 2", "Abu Dhabi Offshore 3"],
}

# =============================================================================
# COUNTRY DETECTION - ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô PROJECTS_BY_COUNTRY
# =============================================================================
class CountryDetector:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß - ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"""
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô PROJECTS_BY_COUNTRY
    COUNTRY_PATTERNS = {}
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á patterns ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô PROJECTS_BY_COUNTRY
    @classmethod
    def initialize_patterns(cls):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° patterns ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"""
        if cls.COUNTRY_PATTERNS:
            return
            
        # Patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        country_patterns_base = {
            "Thailand": [
                r'\b‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\b', r'\b‡πÑ‡∏ó‡∏¢\b', r'\bthailand\b', r'\bbangkok\b',
                r'\b‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô\b', r'\b‡∏Å‡∏ü‡∏ú\b', r'\b‡∏Å‡∏Å‡∏û\b', r'\b‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ\b',
                r'\b‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û\b', r'\bchiang mai\b', r'\b‡∏™‡∏õ‡∏õ\b'
            ],
            "Myanmar": [
                r'\b‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤\b', r'\bmyanmar\b', r'\byangon\b', r'\b‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á\b',
                r'\bnaypyidaw\b', r'\bmoge\b'
            ],
            "Malaysia": [
                r'\b‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢\b', r'\bmalaysia\b', r'\bkuala lumpur\b',
                r'\bpetronas\b', r'\bsabah\b', r'\bsarawak\b'
            ],
            "Vietnam": [
                r'\b‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏°\b', r'\bvietnam\b', r'\bhanoi\b', r'\bho chi minh\b',
                r'\bpetrovietnam\b', r'\bda nang\b'
            ],
            "Indonesia": [
                r'\b‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢\b', r'\bindonesia\b', r'\bjakarta\b',
                r'\bpertamina\b', r'\bbali\b', r'\bsumatra\b'
            ],
            "Kazakhstan": [
                r'\b‡∏Ñ‡∏≤‡∏ã‡∏±‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô\b', r'\bkazakhstan\b', r'\bastana\b',
                r'\bkazmunaigas\b'
            ],
            "Oman": [
                r'\b‡πÇ‡∏≠‡∏°‡∏≤‡∏ô\b', r'\boman\b', r'\bmuscat\b', r'\bpdo\b',
                r'\boq\b'
            ],
            "UAE": [
                r'\b‡∏™‡∏´‡∏£‡∏±‡∏ê‡∏≠‡∏≤‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏°‡∏¥‡πÄ‡∏£‡∏ï‡∏™‡πå\b', r'\buae\b', r'\babu dhabi\b',
                r'\bdubai\b', r'\badnoc\b'
            ],
        }
        
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô PROJECTS_BY_COUNTRY
        for country in PROJECTS_BY_COUNTRY.keys():
            if country in country_patterns_base:
                cls.COUNTRY_PATTERNS[country] = country_patterns_base[country]
    
    @classmethod
    def detect_country(cls, text: str) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° - ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"""
        if not text:
            return ""
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å initialize patterns
        if not cls.COUNTRY_PATTERNS:
            cls.initialize_patterns()
        
        text_lower = text.lower()
        
        for country, patterns in cls.COUNTRY_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, text_lower, re.IGNORECASE):
                    return country
        
        return ""  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

# =============================================================================
# KEYWORD FILTERS
# =============================================================================
class KeywordFilter:
    OFFICIAL_KEYWORDS = [
        '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏£‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏ü‡∏ú', '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤',
        '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏Å‡∏û', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        '‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏Ñ‡∏£‡∏°.', '‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤',
        'minister', 'ministry', 'regulation', 'policy', 'tariff', 'approval',
        '‡πÅ‡∏ñ‡∏•‡∏á‡∏Å‡∏≤‡∏£‡∏ì‡πå', '‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î', '‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö'
    ]
    
    ENERGY_KEYWORDS = [
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü', '‡∏Å‡πä‡∏≤‡∏ã', 'LNG', '‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÄ‡∏û‡∏•‡∏¥‡∏á',
        '‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô', '‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏°', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ä‡∏µ‡∏ß‡∏°‡∏ß‡∏•',
        'energy', 'electricity', 'power', 'gas', 'oil', 'fuel',
        'power plant', 'renewable', 'solar', 'wind', 'biomass',
        '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏™‡πà‡∏≠‡∏á‡∏™‡∏ß‡πà‡∏≤‡∏á', '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ä‡∏∏‡∏°‡∏ä‡∏ô', '‡∏™‡∏≤‡∏¢‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤'
    ]
    
    @classmethod
    def contains_official_keywords(cls, text: str) -> bool:
        """Check if text contains official keywords"""
        if not text:
            return False
        
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in cls.OFFICIAL_KEYWORDS)
    
    @classmethod
    def is_energy_related(cls, text: str) -> bool:
        """Check if text is energy related"""
        if not text:
            return False
        
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in cls.ENERGY_KEYWORDS)
    
    @classmethod
    def is_target_country_news(cls, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        international_indicators = [
            'global', 'world', 'international', 'united nations', 'un ',
            'european union', 'eu ', 'climate summit', 'cop ',
            'g20', 'g7', 'world bank', 'imf', 'opec+', 'international'
        ]
        
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        for indicator in international_indicators:
            if indicator in text_lower:
                return False
        
        return True

# =============================================================================
# SIMPLIFIED GOOGLE NEWS RSS FEEDS - ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
# =============================================================================
def gnews_rss(q: str, hl="en", gl="US", ceid="US:en") -> str:
    """Generate Google News RSS URL"""
    return f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl={hl}&gl={gl}&ceid={ceid}"

# ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ feed ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
FEEDS = [
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢ ====================
    ("Thai_Energy_General", "thai_energy", gnews_rss(
        '(‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR ‡∏Å‡πä‡∏≤‡∏ã OR LNG OR ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô OR ‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô) Thailand',
        hl="th", gl="TH", ceid="TH:th"
    )),
    
    ("Thai_Energy_Policy", "thai_official", gnews_rss(
        '(‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR ‡∏Å‡∏£‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR ‡∏Å‡∏ü‡∏ú OR ‡∏Å‡∏Å‡∏û OR ‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü OR Direct PPA) Thailand',
        hl="th", gl="TH", ceid="TH:th"
    )),
    
    ("Thai_Business_Energy", "thai_business", gnews_rss(
        '(‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR ‡∏Å‡πä‡∏≤‡∏ã) (‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ OR ‡πÑ‡∏ó‡∏¢)',
        hl="th", gl="TH", ceid="TH:th"
    )),
    
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° ====================
    ("Vietnam_Energy", "vietnam_energy", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas OR LNG) Vietnam',
        hl="en", gl="VN", ceid="VN:en"
    )),
    
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢ ====================
    ("Malaysia_Energy", "malaysia_energy", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas OR Petronas) Malaysia',
        hl="en", gl="MY", ceid="MY:en"
    )),
    
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢ ====================
    ("Indonesia_Energy", "indonesia_energy", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas OR Pertamina) Indonesia',
        hl="en", gl="ID", ceid="ID:en"
    )),
    
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤ ====================
    ("Myanmar_Energy", "myanmar_energy", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas) Myanmar',
        hl="en", gl="MM", ceid="MM:en"
    )),
    
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏Å‡∏•‡∏≤‡∏á ====================
    ("Oman_Energy", "oman_energy", gnews_rss(
        '(energy OR oil OR gas) Oman',
        hl="en", gl="OM", ceid="OM:en"
    )),
    
    ("UAE_Energy", "uae_energy", gnews_rss(
        '(energy OR oil OR gas) (UAE OR United Arab Emirates OR Abu Dhabi)',
        hl="en", gl="AE", ceid="AE:en"
    )),
    
    # ==================== ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏≤‡∏ã‡∏±‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô ====================
    ("Kazakhstan_Energy", "kazakhstan_energy", gnews_rss(
        '(energy OR oil OR gas) Kazakhstan',
        hl="en", gl="KZ", ceid="KZ:en"
    )),
]

# =============================================================================
# SIMPLE RSS PARSER
# =============================================================================
class SimpleRSSParser:
    """Parser RSS ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ"""
    
    @staticmethod
    def fetch_feed(feed_name: str, feed_url: str):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• RSS"""
        try:
            print(f"[RSS] Fetching {feed_name}...")
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ headers
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
            }
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• RSS
            response = requests.get(feed_url, headers=headers, timeout=15)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö status code
            if response.status_code != 200:
                print(f"[RSS WARNING] {feed_name}: HTTP {response.status_code}")
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ headers
                response = requests.get(feed_url, timeout=15)
                if response.status_code != 200:
                    print(f"[RSS ERROR] {feed_name}: Failed with HTTP {response.status_code}")
                    return []
            
            # Parse RSS
            feed = feedparser.parse(response.content)
            
            entries = feed.entries or []
            print(f"[RSS] {feed_name}: Found {len(entries)} entries")
            return entries
            
        except requests.exceptions.Timeout:
            print(f"[RSS ERROR] {feed_name}: Timeout")
            return []
        except requests.exceptions.RequestException as e:
            print(f"[RSS ERROR] {feed_name}: Request error - {str(e)}")
            return []
        except Exception as e:
            print(f"[RSS ERROR] {feed_name}: Unexpected error - {str(e)}")
            return []
    
    @staticmethod
    def parse_entry(entry, feed_name: str, feed_type: str):
        """Parse entry"""
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        title = TextCleaner.clean_text(getattr(entry, "title", "") or "")
        link = (getattr(entry, "link", "") or "").strip()
        summary = TextCleaner.clean_text(getattr(entry, "summary", "") or "")
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ title ‡∏´‡∏£‡∏∑‡∏≠ title ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        if not title or len(title) < 10:
            return None
        
        # ‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà
        published = getattr(entry, "published", None) or getattr(entry, "updated", None)
        published_dt = None
        
        try:
            if published:
                published_dt = dateutil_parser.parse(published)
                if published_dt.tzinfo is None:
                    published_dt = TZ.localize(published_dt)
                published_dt = published_dt.astimezone(TZ)
        except:
            published_dt = None
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç URL
        actual_url = URLValidator.extract_actual_url(link)
        if URLValidator.is_valid_url(actual_url):
            final_url = actual_url
        else:
            final_url = link
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary
        enhanced_summary = TextCleaner.extract_meaningful_summary(summary)
        
        return {
            "title": title[:120],
            "url": final_url,
            "original_url": link,
            "summary": enhanced_summary[:200],
            "published_dt": published_dt,
            "feed": feed_name,
            "section": feed_type,
            "has_valid_url": URLValidator.is_valid_url(final_url),
        }

# =============================================================================
# SIMPLE LLM ANALYZER
# =============================================================================
class SimpleLLMAnalyzer:
    """LLM Analyzer ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ"""
    
    def __init__(self, api_key: str, model: str, endpoint: str):
        self.api_key = api_key
        self.model = model
        self.endpoint = endpoint
    
    def analyze_news(self, title: str, summary: str) -> dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡πâ‡∏ß‡∏¢ LLM"""
        if not self.api_key:
            return self._get_default_analysis()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        combined_text = f"{title} {summary}"
        if not TextCleaner.clean_text(combined_text):
            return self._get_default_analysis()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏á‡πà‡∏≤‡∏¢‡πÜ
        system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON:
        {
            "summary_th": "‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢",
            "is_official": true/false,
            "country": "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: Thailand, Myanmar, Malaysia, Vietnam, Indonesia, Kazakhstan, Oman, UAE ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)"
        }"""
        
        user_prompt = f"""‡∏Ç‡πà‡∏≤‡∏ß: {title}
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {summary}
        ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:"""
        
        try:
            response = requests.post(
                self.endpoint,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 300
                },
                timeout=20
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data["choices"][0]["message"]["content"].strip()
                
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏¢‡∏Å JSON
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        analysis = json.loads(json_match.group())
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ country ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                        country = str(analysis.get("country", "")).strip()
                        if country not in PROJECTS_BY_COUNTRY:
                            country = ""
                        
                        return {
                            "summary_th": TextCleaner.clean_text(str(analysis.get("summary_th", "")))[:150],
                            "is_official": bool(analysis.get("is_official", False)),
                            "country": country,
                        }
                    except:
                        pass
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô summary
                return {
                    "summary_th": TextCleaner.clean_text(content)[:150],
                    "is_official": False,
                    "country": "",
                }
            else:
                print(f"[LLM] HTTP Error {response.status_code}")
                
        except Exception as e:
            print(f"[LLM] Error: {str(e)}")
        
        return self._get_default_analysis()
    
    def _get_default_analysis(self):
        """Default analysis"""
        return {
            "summary_th": "",
            "is_official": False,
            "country": "",
        }

# =============================================================================
# NEWS FILTER
# =============================================================================
class NewsFilter:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞"""
    
    @staticmethod
    def filter_by_target_countries(news_items: list) -> list:
        """‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"""
        filtered_news = []
        
        for item in news_items:
            country = item.get('country', '')
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
            if country in PROJECTS_BY_COUNTRY:
                filtered_news.append(item)
            else:
                print(f"[FILTER] ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß: {item.get('title', '')[:50]}... ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {country}")
        
        return filtered_news

# =============================================================================
# MAIN NEWS PROCESSOR
# =============================================================================
class NewsProcessor:
    def __init__(self):
        self.sent_links = self.read_sent_links()
        self.llm_analyzer = None
        if USE_LLM_SUMMARY and GROQ_API_KEY:
            self.llm_analyzer = SimpleLLMAnalyzer(GROQ_API_KEY, GROQ_MODEL, GROQ_ENDPOINT)
        self.rss_parser = SimpleRSSParser()
    
    def read_sent_links(self):
        """‡∏≠‡πà‡∏≤‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
        sent = set()
        for fn in os.listdir(SENT_DIR):
            if not fn.endswith(".txt"):
                continue
            fp = os.path.join(SENT_DIR, fn)
            try:
                with open(fp, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            sent.add(line)
            except Exception:
                continue
        return sent
    
    def append_sent_link(self, url: str):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
        url = self.normalize_url(url)
        if not url:
            return
        fn = os.path.join(SENT_DIR, self.now_tz().strftime("%Y-%m-%d") + ".txt")
        with open(fn, "a", encoding="utf-8") as f:
            f.write(url + "\n")
    
    def normalize_url(self, url: str) -> str:
        """‡∏ó‡∏≥‡πÉ‡∏´‡πâ URL ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"""
        url = (url or "").strip()
        if not url:
            return url
        try:
            u = urlparse(url)
            return u._replace(fragment="").geturl()
        except Exception:
            return url
    
    def now_tz(self) -> datetime:
        return datetime.now(TZ)
    
    def in_time_window(self, published_dt: datetime, hours: int) -> bool:
        if not published_dt:
            return False
        return published_dt >= (self.now_tz() - timedelta(hours=hours))
    
    def fetch_and_filter_news(self):
        """‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß"""
        all_news = []
        
        for feed_name, feed_type, feed_url in FEEDS:
            print(f"\n[Fetching] {feed_name}...")
            
            try:
                entries = self.rss_parser.fetch_feed(feed_name, feed_url)
                processed_count = 0
                
                for entry in entries[:MAX_PER_FEED]:
                    news_item = self._process_entry(entry, feed_name, feed_type)
                    if news_item:
                        all_news.append(news_item)
                        processed_count += 1
                
                print(f"  Processed: {processed_count} items")
                        
            except Exception as e:
                print(f"  Error in {feed_name}: {str(e)}")
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ URL ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
        all_news = [item for item in all_news if item.get('has_valid_url', False)]
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
        all_news.sort(key=lambda x: (
            -x.get('is_official', 0),
            -(x.get('published_dt') or datetime.min).timestamp()
        ))
        
        return all_news[:MAX_MESSAGES_PER_RUN * BUBBLES_PER_CAROUSEL]
    
    def _process_entry(self, entry, feed_name: str, feed_type: str):
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß - ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"""
        item = self.rss_parser.parse_entry(entry, feed_name, feed_type)
        if not item:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        normalized_url = self.normalize_url(item["url"])
        if normalized_url in self.sent_links:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
        if item["published_dt"] and not self.in_time_window(item["published_dt"], WINDOW_HOURS):
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        full_text = f"{item['title']} {item['summary']}"
        if not KeywordFilter.is_energy_related(full_text):
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£)
        country = CountryDetector.detect_country(full_text)
        if not country:  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
        is_official = (
            KeywordFilter.contains_official_keywords(full_text) or
            feed_type in ['thai_official', 'energy_policy']
        )
        
        # ‡∏î‡∏∂‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        project_hints = PROJECTS_BY_COUNTRY.get(country, [])[:2]
        
        # ‡πÉ‡∏ä‡πâ LLM (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        llm_analysis = None
        if self.llm_analyzer:
            llm_analysis = self.llm_analyzer.analyze_news(item['title'], item['summary'])
            
            # ‡∏ñ‡πâ‡∏≤ LLM ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏±‡πâ‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£)
            if llm_analysis.get('country') and llm_analysis['country'] in PROJECTS_BY_COUNTRY:
                country = llm_analysis['country']
                project_hints = PROJECTS_BY_COUNTRY.get(country, [])[:2]
            
            if llm_analysis.get('is_official'):
                is_official = True
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        return {
            'title': item['title'],
            'url': item['url'],
            'summary': item['summary'],
            'published_dt': item['published_dt'],
            'country': country,
            'project_hints': project_hints,
            'is_official': is_official,
            'llm_analysis': llm_analysis,
            'feed': feed_name,
            'has_valid_url': item.get('has_valid_url', True)
        }

# =============================================================================
# LINE MESSAGE BUILDER
# =============================================================================
class LineMessageBuilder:
    @staticmethod
    def create_flex_bubble(news_item):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á LINE Flex Bubble"""
        title = LineMessageBuilder.cut_text(news_item.get('title', ''), 100)
        
        # Format timestamp
        pub_dt = news_item.get('published_dt')
        time_str = pub_dt.strftime("%d/%m/%Y %H:%M") if pub_dt else ""
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏î‡∏à‡πå
        if news_item.get('is_official'):
            color = "#4CAF50"
            badge = "üì¢ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"
        elif news_item.get('llm_analysis'):
            color = "#2196F3"
            badge = "ü§ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI"
        else:
            color = "#FF9800"
            badge = "üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        contents = [
            {
                "type": "text",
                "text": title,
                "weight": "bold",
                "size": "md",
                "wrap": True,
                "margin": "md"
            }
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
        metadata = []
        if time_str:
            metadata.append(time_str)
        
        if metadata:
            contents.append({
                "type": "text",
                "text": " | ".join(metadata),
                "size": "xs",
                "color": "#888888",
                "margin": "sm"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        contents.append({
            "type": "text",
            "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {news_item.get('country', 'N/A')}",
            "size": "sm",
            "margin": "xs"
        })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        if news_item.get('project_hints'):
            hints_text = ", ".join(news_item['project_hints'][:2])
            contents.append({
                "type": "text",
                "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {hints_text}",
                "size": "sm",
                "color": "#2E7D32",
                "wrap": True,
                "margin": "xs"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏£‡∏∏‡∏õ
        if news_item.get('llm_analysis') and news_item['llm_analysis'].get('summary_th'):
            contents.append({
                "type": "text",
                "text": news_item['llm_analysis']['summary_th'],
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#424242"
            })
        elif news_item.get('summary'):
            contents.append({
                "type": "text",
                "text": news_item['summary'],
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#666666"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ö‡∏î‡∏à‡πå
        contents.append({
            "type": "text",
            "text": badge,
            "size": "xs",
            "color": color,
            "margin": "sm"
        })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bubble
        bubble = {
            "type": "bubble",
            "size": "kilo",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": contents,
                "paddingAll": "12px"
            }
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏∏‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß
        url = news_item.get('url')
        if url and URLValidator.is_valid_url(url) and len(url) <= 1000:
            bubble["footer"] = {
                "type": "box",
                "layout": "vertical",
                "spacing": "sm",
                "contents": [
                    {
                        "type": "button",
                        "style": "primary",
                        "height": "sm",
                        "action": {
                            "type": "uri",
                            "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°",
                            "uri": url
                        }
                    }
                ]
            }
        
        return bubble
    
    @staticmethod
    def cut_text(s: str, n: int) -> str:
        s = (s or "").strip()
        return s if len(s) <= n else s[: n - 1].rstrip() + "‚Ä¶"
    
    @staticmethod
    def create_carousel_message(news_items):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á carousel message"""
        if not news_items:
            return None
        
        bubbles = []
        for item in news_items[:BUBBLES_PER_CAROUSEL]:
            bubble = LineMessageBuilder.create_flex_bubble(item)
            if bubble:
                bubbles.append(bubble)
        
        if not bubbles:
            return None
        
        return {
            "type": "flex",
            "altText": f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô {datetime.now(TZ).strftime('%d/%m/%Y')} ({len(bubbles)} ‡∏Ç‡πà‡∏≤‡∏ß)",
            "contents": {
                "type": "carousel",
                "contents": bubbles
            }
        }

# =============================================================================
# LINE SENDER
# =============================================================================
class LineSender:
    def __init__(self, access_token):
        self.access_token = access_token
        self.headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
    
    def send_message(self, message_obj):
        """Send message to LINE"""
        if DRY_RUN:
            print("\n" + "="*60)
            print("DRY RUN - Would send the following news:")
            print("="*60)
            
            contents = message_obj.get('contents', {}).get('contents', [])
            for i, bubble in enumerate(contents):
                title = bubble.get('body', {}).get('contents', [{}])[0].get('text', 'No title')
                country = ""
                for content in bubble.get('body', {}).get('contents', []):
                    if content.get('text', '').startswith('‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®:'):
                        country = content['text']
                        break
                has_button = 'footer' in bubble
                print(f"{i+1}. {title[:60]}... {country} {'[‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå]' if has_button else '[‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå]'}")
            
            print(f"\nTotal: {len(contents)} news items")
            return True
        
        url = "https://api.line.me/v2/bot/message/broadcast"
        
        try:
            response = requests.post(
                url,
                headers=self.headers,
                json={"messages": [message_obj]},
                timeout=30
            )
            
            if response.status_code == 200:
                print("[LINE] Message sent successfully!")
                return True
            else:
                print(f"[LINE] Error {response.status_code}: {response.text[:200]}")
                return False
                
        except Exception as e:
            print(f"[LINE] Exception: {str(e)}")
            return False

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    print("="*60)
    print("‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô - Google News RSS Version")
    print("="*60)
    
    # Configuration check
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("[ERROR] LINE_CHANNEL_ACCESS_TOKEN is required")
        return
    
    print(f"\n[CONFIG] Use LLM: {'Yes' if USE_LLM_SUMMARY and GROQ_API_KEY else 'No'}")
    print(f"[CONFIG] Time window: {WINDOW_HOURS} hours")
    print(f"[CONFIG] Dry run: {'Yes' if DRY_RUN else 'No'}")
    print(f"[CONFIG] Feeds: {len(FEEDS)} Google News RSS sources")
    
    # Initialize CountryDetector patterns
    CountryDetector.initialize_patterns()
    target_countries = list(PROJECTS_BY_COUNTRY.keys())
    print(f"[CONFIG] Target countries: {', '.join(target_countries)}")
    
    # Initialize components
    processor = NewsProcessor()
    line_sender = LineSender(LINE_CHANNEL_ACCESS_TOKEN)
    
    # Step 1: Fetch and filter news
    print("\n[1] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß...")
    news_items = processor.fetch_and_filter_news()
    
    if not news_items:
        print("\n[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
        return
    
    print(f"\n[2] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # Step 3: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
    print("\n[3] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£...")
    news_items = NewsFilter.filter_by_target_countries(news_items)
    
    if not news_items:
        print("\n[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£")
        return
    
    print(f"\n[4] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    valid_url_count = sum(1 for item in news_items if item.get('has_valid_url', False))
    official_count = sum(1 for item in news_items if item.get('is_official'))
    llm_count = sum(1 for item in news_items if item.get('llm_analysis'))
    
    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
    country_stats = {}
    for item in news_items:
        country = item.get('country', 'Unknown')
        country_stats[country] = country_stats.get(country, 0) + 1
    
    print(f"   - ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠: {valid_url_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£: {official_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI: {llm_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join([f'{k} ({v})' for k, v in country_stats.items()])}")
    
    # Step 5: Create LINE message
    print("\n[5] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LINE...")
    line_message = LineMessageBuilder.create_carousel_message(news_items)
    
    if not line_message:
        print("[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ")
        return
    
    # Step 6: Send message
    print("\n[6] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
    success = line_sender.send_message(line_message)
    
    # Step 7: Mark as sent if successful
    if success and not DRY_RUN:
        for item in news_items:
            if item.get('has_valid_url', False):
                processor.append_sent_link(item.get('url'))
        print("\n[SUCCESS] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    
    print("\n" + "="*60)
    print("‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    print("="*60)

if __name__ == "__main__":
    main()
