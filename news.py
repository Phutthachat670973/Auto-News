# -*- coding: utf-8 -*-
"""News.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fs6sH5xRjuOIHBoKlDXF1DpuSXWt9nUk
"""
# ------------------- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Library -------------------
import feedparser
from datetime import datetime, timedelta
import pytz
import requests
from transformers import pipeline
import re
from bs4 import BeautifulSoup
from collections import Counter
import os

# ------------------- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Timezone -------------------
bangkok_tz = pytz.timezone("Asia/Bangkok")
now_thai = datetime.now(bangkok_tz)
today_thai = now_thai.date()
yesterday_thai = today_thai - timedelta(days=1)

# ------------------- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß -------------------
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")

# ------------------- DeepL API -------------------
DEEPL_API_KEY = "995e3d74-5184-444b-9fd9-a82a116c55cf:fx"  
def deepl_translate(text, target_lang="TH", source_lang="EN"):
    url = "https://api-free.deepl.com/v2/translate"
    params = {
        "auth_key": DEEPL_API_KEY,
        "text": text,
        "source_lang": source_lang.upper(),
        "target_lang": target_lang.upper()
    }
    try:
        response = requests.post(url, data=params)
        result = response.json()
        return result['translations'][0]['text']
    except Exception as e:
        return f"‚ùå ‡πÅ‡∏õ‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}"

def translate_en_to_th(text):
    return deepl_translate(text, target_lang="TH", source_lang="EN")

# ------------------- Line Channel Token -------------------
LINE_CHANNEL_ACCESS_TOKEN = 'tI3xxzlIq2sD6pg1ukIabWAnuxxoCgc68Bv0vDcvHZNCUnUYGk15EafVqLi3A6pDlyBiUwECDzwxLHtwzIfpoieIO5BIWVRHtfVa7uIy9XYuWwZpybcV/UmwOvhxySqTb4wOXdKRX8Gpo9N91VIOzAdB04t89/1O/w1cDnyilFU='

# ------------------- RSS URLs -------------------
feed_urls = {
    "BBC Economy": "http://feeds.bbci.co.uk/news/business/economy/rss.xml",
    "MarketWatch": "https://feeds.marketwatch.com/marketwatch/topstories/",
    "CNBC": "https://www.cnbc.com/id/15839135/device/rss/rss.html",
    "EIA Today in Energy": "https://www.eia.gov/rss/todayinenergy.xml",
    "OilPrice.com": "https://oilprice.com/rss/main"
}

keywords = [
    "economy", "economic", "recession", "inflation", "deflation", "gdp", "interest rate",
    "fiscal policy", "monetary policy", "stimulus", "unemployment", "debt", "deficit", "growth",
    "macroeconomics", "financial crisis", "energy", "oil", "gas", "natural gas", "crude", "power",
    "electricity", "renewable", "solar", "wind", "nuclear", "hydropower", "geothermal", "fuel",
    "petroleum", "coal", "biofuel", "emissions", "carbon", "carbon footprint", "energy market",
    "energy price", "energy policy", "energy crisis", "energy transition", "green energy",
    "clean energy", "fossil fuels", "climate", "net zero"
]

keyword_to_category = {
    "oil": "Oil", "gas": "Gas", "natural gas": "Gas", "crude": "Oil", "petroleum": "Oil",
    "fossil": "Fossil", "coal": "Fossil", "biofuel": "Fossil", "emissions": "Environment",
    "carbon": "Environment", "carbon footprint": "Environment", "energy": "Energy",
    "renewable": "Renewable", "solar": "Renewable", "wind": "Renewable", "nuclear": "Renewable",
    "hydropower": "Renewable", "geothermal": "Renewable", "power": "Energy",
    "electricity": "Energy", "economy": "Economy", "economic": "Economy", "recession": "Economy",
    "inflation": "Economy", "deflation": "Economy", "gdp": "Economy", "interest rate": "Economy",
    "fiscal policy": "Economy", "monetary policy": "Economy", "stimulus": "Economy",
    "unemployment": "Economy", "debt": "Economy", "deficit": "Economy", "growth": "Economy",
    "macroeconomics": "Economy", "financial crisis": "Economy", "energy market": "Energy",
    "energy price": "Energy", "energy policy": "Energy", "energy crisis": "Energy",
    "energy transition": "Energy", "green energy": "Renewable", "clean energy": "Renewable",
    "climate": "Environment", "net zero": "Environment"
}

def parse_date(entry):
    try:
        return datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    except:
        return None

def is_relevant(entry):
    text = (entry.title + " " + getattr(entry, 'summary', "")).lower()
    return any(k in text for k in keywords)

def extract_image(entry):
    if hasattr(entry, 'media_content'):
        for media in entry.media_content:
            if 'url' in media:
                return media['url']
    if 'img' in entry.summary:
        imgs = re.findall(r'<img[^>]+src="([^">]+)"', entry.summary)
        if imgs:
            return imgs[0]
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(entry.link, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                return og_image["content"]
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
    return "https://scdn.line-apps.com/n/channel_devcenter/img/fx/01_1_cafe.png"

def summarize_and_translate(title, summary):
    text = f"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ:\n\n{title}\n{summary}"
    try:
        result = summarizer(text, max_length=100, min_length=20, do_sample=False)
        english_summary = result[0]['summary_text']
        return translate_en_to_th(english_summary)
    except Exception as e:
        return f"‚ùå ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}"

def classify_category(entry):
    text = (entry.title + " " + getattr(entry, 'summary', "")).lower()
    matched = [keyword_to_category[k] for k in keyword_to_category if k in text]
    return Counter(matched).most_common(1)[0][0] if matched else "Other"

def create_flex_message(news_items):
    bubbles = []
    for item in news_items:
        summary_th = summarize_and_translate(item['title'], item['summary'])
        bubble = {
            "type": "bubble",
            "size": "mega",
            "hero": {
                "type": "image",
                "url": item.get("image", ""),
                "size": "full",
                "aspectRatio": "20:13",
                "aspectMode": "cover"
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {"type": "text", "text": item['title'], "weight": "bold", "size": "md", "wrap": True},
                    {"type": "text", "text": f"üóì {item['published'].strftime('%d/%m/%Y')}", "size": "xs", "color": "#888888", "margin": "sm"},
                    {"type": "text", "text": f"üìå {item['category']}", "size": "xs", "color": "#AAAAAA", "margin": "xs"},
                    {"type": "text", "text": summary_th, "size": "sm", "wrap": True, "margin": "md"},
                ]
            },
            "footer": {
                "type": "box",
                "layout": "vertical",
                "spacing": "sm",
                "contents": [
                    {
                        "type": "button",
                        "style": "link",
                        "height": "sm",
                        "action": {"type": "uri", "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "uri": item['link']}
                    }
                ]
            }
        }
        if bubble["hero"]["url"].startswith("http"):
            bubbles.append(bubble)
    return [{
        "type": "flex",
        "altText": f"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô {today_thai.strftime('%d/%m/%Y')}",
        "contents": {"type": "carousel", "contents": bubbles[i:i+10]}
    } for i in range(0, len(bubbles), 10)]

def send_flex_to_line(flex_messages):
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {LINE_CHANNEL_ACCESS_TOKEN}'
    }
    for i, msg in enumerate(flex_messages):
        print(f"üì¶ ‡∏™‡πà‡∏á‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà {i+1}/{len(flex_messages)} ‡∏Ç‡πà‡∏≤‡∏ß {len(msg['contents']['contents'])} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
        res = requests.post(url, headers=headers, json={"messages": [msg]})
        print(f"LINE Response: {res.status_code}, {res.text}")

# ------------------- ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ -------------------
sent_file = "sent_links.txt"
if os.path.exists(sent_file):
    with open(sent_file, "r", encoding="utf-8") as f:
        sent_links = set(f.read().splitlines())
else:
    sent_links = set()

# ------------------- ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß -------------------
all_news = []
for source, url in feed_urls.items():
    feed = feedparser.parse(url)
    for entry in feed.entries:
        pub_date = parse_date(entry)
        if not pub_date:
            continue
        local_date = pub_date.astimezone(bangkok_tz).date()
        if local_date in [today_thai, yesterday_thai] and entry.link not in sent_links:
            if is_relevant(entry):
                all_news.append({
                    "source": source,
                    "title": entry.title,
                    "summary": getattr(entry, 'summary', ''),
                    "link": entry.link,
                    "image": extract_image(entry),
                    "published": pub_date.astimezone(bangkok_tz),
                    "category": classify_category(entry)
                })
                sent_links.add(entry.link)

# ------------------- ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ô‡πå -------------------
all_news = sorted(all_news, key=lambda x: x["published"], reverse=True)
if all_news:
    flex_msgs = create_flex_message(all_news)
    send_flex_to_line(flex_msgs)
    with open(sent_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sent_links))
else:
    print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")


