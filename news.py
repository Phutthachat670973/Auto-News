# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote
from difflib import SequenceMatcher

import requests
import feedparser
import pytz
from dateutil import parser as dateutil_parser

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# =============================================================================
# ENV / CONFIG
# =============================================================================
TZ = pytz.timezone(os.getenv("TZ", "Asia/Bangkok"))

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE_CHANNEL_ACCESS_TOKEN")

# Groq Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant").strip()
GROQ_ENDPOINT = os.getenv("GROQ_ENDPOINT", "https://api.groq.com/openai/v1/chat/completions").strip()
USE_LLM_SUMMARY = os.getenv("USE_LLM_SUMMARY", "1").strip().lower() in ["1", "true", "yes", "y"]

WINDOW_HOURS = int(os.getenv("WINDOW_HOURS", "48"))
MAX_PER_FEED = int(os.getenv("MAX_PER_FEED", "30"))
DRY_RUN = os.getenv("DRY_RUN", "0").strip().lower() in ["1", "true", "yes", "y"]
BUBBLES_PER_CAROUSEL = int(os.getenv("BUBBLES_PER_CAROUSEL", "10"))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
ALLOWED_NEWS_SOURCES = os.getenv("ALLOWED_NEWS_SOURCES", "").strip()
if ALLOWED_NEWS_SOURCES:
    ALLOWED_NEWS_SOURCES_LIST = [s.strip().lower() for s in ALLOWED_NEWS_SOURCES.split(",") if s.strip()]
    print(f"[CONFIG] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß: {ALLOWED_NEWS_SOURCES_LIST}")
else:
    ALLOWED_NEWS_SOURCES_LIST = []
    print("[CONFIG] ‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß")

# Sent links tracking
SENT_DIR = os.getenv("SENT_DIR", "sent_links")
os.makedirs(SENT_DIR, exist_ok=True)

# =============================================================================
# PROJECT DATABASE
# =============================================================================
PROJECTS_BY_COUNTRY = {
    "Thailand": [
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 2/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå", "Arthit",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏™ 1", "S1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4", "Contract 4",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ‡∏≠‡∏µ‡∏û‡∏µ 1", "PTTEP 1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 6/27",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 22/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏µ 5", "E5",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏†‡∏π‡∏Æ‡πà‡∏≠‡∏°", "Sinphuhorm",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 8/32", "B8/32", "9A", "9‡πÄ‡∏≠",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/48", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 12/48",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/65", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 3/65",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 53/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 54/43"
    ],
    "Myanmar": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏ï‡∏¥‡∏Å‡πâ‡∏≤", "Zawtika", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏≤‡∏î‡∏≤‡∏ô‡∏≤", "Yadana", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤ ‡πÄ‡∏≠‡πá‡∏° 3", "Myanmar M3"],
    "Malaysia": ["Malaysia SK309", "SK309", "Malaysia SK311", "SK311", "Malaysia Block H", "Block H"],
    "Vietnam": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° 16-1", "Vietnam 16-1", "16-1", "Block B", "48/95"],
    "Indonesia": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏ó‡∏π‡∏ô‡πà‡∏≤ ‡∏ã‡∏µ ‡πÄ‡∏≠", "Natuna Sea A"],
    "Kazakhstan": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∏‡∏á‡∏Å‡∏≤", "Dunga"],
    "Oman": ["Oman Block 61", "Block 61", "Oman Block 6", "PDO"],
    "UAE": ["Abu Dhabi Offshore 1", "Abu Dhabi Offshore 2", "Abu Dhabi Offshore 3"],
}

# =============================================================================
# ENHANCED KEYWORD FILTER (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
# =============================================================================
class EnhancedKeywordFilter:
    # ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô (‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à)
    ENERGY_KEYWORDS = [
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤',
        '‡∏Å‡πä‡∏≤‡∏ã', 'LNG', '‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÄ‡∏û‡∏•‡∏¥‡∏á', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô',
        '‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏™‡∏á‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå', '‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏°',
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ä‡∏µ‡∏ß‡∏°‡∏ß‡∏•', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡πâ‡∏≥', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô',
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡∏¥‡∏ß‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå', '‡∏ñ‡πà‡∏≤‡∏ô‡∏´‡∏¥‡∏ô', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ü‡∏≠‡∏™‡∏ã‡∏¥‡∏•',
        '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô‡∏Å‡πä‡∏≤‡∏ã', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô',
        '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Å‡πä‡∏≤‡∏ã', '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πä‡∏≤‡∏ã', '‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤',
        '‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        'energy', 'electricity', 'power', 'gas', 'oil', 'fuel',
        'power plant', 'renewable', 'solar', 'wind', 'biomass',
        'energy policy', 'energy project', 'energy investment'
    ]
    
    # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ñ‡∏∂‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à/‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
    BUSINESS_KEYWORDS = [
        '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£', '‡∏•‡∏á‡∏ó‡∏∏‡∏ô', '‡∏™‡∏±‡∏ç‡∏ç‡∏≤', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô', '‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤',
        '‡∏•‡πâ‡∏≤‡∏ô', '‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô', '‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå', '‡∏ö‡∏≤‡∏ó', '‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç',
        '‡∏û‡∏ö', '‡∏™‡∏≥‡∏£‡∏ß‡∏à', '‡∏Ç‡∏∏‡∏î‡πÄ‡∏à‡∏≤‡∏∞', '‡∏ú‡∏•‡∏¥‡∏ï', '‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å', '‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤',
        '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡πÅ‡∏ñ‡∏•‡∏á', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',
        '‡∏Ç‡∏¢‡∏≤‡∏¢', '‡∏û‡∏±‡∏í‡∏ô‡∏≤', '‡∏™‡∏£‡πâ‡∏≤‡∏á', '‡∏Å‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á', '‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á',
        'project', 'investment', 'contract', 'agreement', 'deal',
        'discovery', 'exploration', 'drilling', 'production', 'export',
        'announce', 'report', 'financial', 'revenue', 'expand',
        'development', 'construction', 'installation'
    ]
    
    # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á (‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏±‡∏á‡∏Ñ‡∏°)
    EXCLUDE_KEYWORDS = [
        '‡∏ï‡∏•‡∏≤‡∏î‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå', '‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå', '‡∏£‡∏ñ', '‡∏£‡∏ñ‡πÉ‡∏´‡∏°‡πà', '‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÉ‡∏´‡∏°‡πà',
        '‡∏¢‡∏≤‡∏ô‡∏¢‡∏ô‡∏ï‡πå', '‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏¢‡∏≤‡∏ô‡∏¢‡∏ô‡∏ï‡πå',
        '‡∏î‡∏≤‡∏£‡∏≤', '‡∏®‡∏¥‡∏•‡∏õ‡∏¥‡∏ô', '‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á', '‡∏ô‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏á', '‡∏Ñ‡∏ô‡∏î‡∏±‡∏á',
        '‡∏£‡πà‡∏ß‡∏°‡∏ö‡∏∏‡∏ç', '‡∏Å‡∏≤‡∏£‡∏Å‡∏∏‡∏®‡∏•', '‡∏à‡∏¥‡∏ï‡∏≠‡∏≤‡∏™‡∏≤', '‡∏°‡∏≠‡∏ö', '‡πÉ‡∏´‡πâ', '‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠',
        '‡∏´‡∏π', '‡∏™‡∏∞‡πÄ‡∏î‡πá‡∏î', '‡∏à‡∏≤ ‡∏û‡∏ô‡∏°', '‡∏ä‡∏ß‡∏ô', '‡∏°‡∏≠‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢',
        'celebrity', 'actor', 'singer', 'donation', 'charity', 'philanthropy',
        'car', 'automotive', 'vehicle', 'automobile'
    ]
    
    # ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥
    MAIN_KEYWORDS_FOR_GROUPING = [
        'murphy', 'shell', '‡πÅ‡∏ö‡∏£‡πá‡∏Ñ ‡∏≠‡∏¥‡∏•‡∏™‡πå', 'black hills',
        'appraisal', 'oil field', 'LNG', 'terminal', 'supplier',
        '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏•‡∏á‡∏ó‡∏∏‡∏ô', 'investment', 'discovery', 'found'
    ]
    
    @classmethod
    def is_valid_energy_news(cls, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        text_lower = text.lower()
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        for exclude in cls.EXCLUDE_KEYWORDS:
            if exclude.lower() in text_lower:
                print(f"  ‚úó ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏±‡∏á‡∏Ñ‡∏°: {exclude}")
                return False
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
        has_energy = any(keyword.lower() in text_lower for keyword in cls.ENERGY_KEYWORDS)
        if not has_energy:
            return False
        
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ñ‡∏∂‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à/‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
        has_business = any(keyword.lower() in text_lower for keyword in cls.BUSINESS_KEYWORDS)
        
        return has_business
    
    @classmethod
    def detect_country(cls, text: str) -> str:
        """Detect country from text"""
        text_lower = text.lower()
        
        country_patterns = {
            "Thailand": ['‡πÑ‡∏ó‡∏¢', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢', 'thailand', 'bangkok'],
            "Myanmar": ['‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤', 'myanmar', '‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á', 'yangon'],
            "Malaysia": ['‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢', 'malaysia', '‡∏Å‡∏±‡∏ß‡∏•‡∏≤‡∏•‡∏±‡∏°‡πÄ‡∏õ‡∏≠‡∏£‡πå', 'kuala lumpur'],
            "Vietnam": ['‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏°', 'vietnam', '‡∏Æ‡∏≤‡∏ô‡∏≠‡∏¢', 'hanoi'],
            "Indonesia": ['‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢', 'indonesia', '‡∏à‡∏≤‡∏Å‡∏≤‡∏£‡πå‡∏ï‡∏≤', 'jakarta'],
            "Kazakhstan": ['‡∏Ñ‡∏≤‡∏ã‡∏±‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô', 'kazakhstan', 'astana'],
            "Oman": ['‡πÇ‡∏≠‡∏°‡∏≤‡∏ô', 'oman', 'muscat'],
            "UAE": ['‡∏¢‡∏π‡πÄ‡∏≠‡∏≠‡∏µ', 'uae', '‡∏î‡∏π‡πÑ‡∏ö', 'dubai', '‡∏≠‡∏≤‡∏ö‡∏π‡∏î‡∏≤‡∏ö‡∏µ', 'abu dhabi']
        }
        
        for country, patterns in country_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                return country
        
        return ""

# =============================================================================
# FEEDS - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á
# =============================================================================
def gnews_rss(q: str, hl="en", gl="US", ceid="US:en") -> str:
    return f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl={hl}&gl={gl}&ceid={ceid}"

FEEDS = [
    ("GoogleNewsTH", "thai", gnews_rss(
        '(‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR "‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü" OR ‡∏Å‡πä‡∏≤‡∏ã OR LNG OR ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô OR ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR "‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤" OR "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô" OR "‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô") -"‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå" -"‡∏ï‡∏•‡∏≤‡∏î‡∏£‡∏ñ" -"‡∏î‡∏≤‡∏£‡∏≤" -"‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á"',
        hl="th", gl="TH", ceid="TH:th"
    )),
    ("GoogleNewsEN", "international", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas OR "power plant" OR "energy project") AND (Thailand OR Vietnam OR Malaysia OR Indonesia) -car -automotive -celebrity',
        hl="en", gl="US", ceid="US:en"
    )),
    ("EnergyNewsCenter", "direct", "https://www.energynewscenter.com/feed/"),
]

# =============================================================================
# UTILITIES
# =============================================================================
def now_tz() -> datetime:
    return datetime.now(TZ)

def normalize_url(url: str) -> str:
    url = (url or "").strip()
    if not url:
        return url
    try:
        u = urlparse(url)
        return u._replace(fragment="").geturl()
    except Exception:
        return url

def extract_domain(url: str) -> str:
    """Extract domain name from URL"""
    url = normalize_url(url)
    if not url:
        return ""
    try:
        domain = urlparse(url).netloc.lower()
        # Remove www. prefix
        if domain.startswith("www."):
            domain = domain[4:]
        return domain
    except Exception:
        return ""

def is_allowed_source(url: str) -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    if not ALLOWED_NEWS_SOURCES_LIST:  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î allowed sources = ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        return True
    
    domain = extract_domain(url)
    if not domain:
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ domain ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
    for allowed_source in ALLOWED_NEWS_SOURCES_LIST:
        if allowed_source in domain:  # ‡πÉ‡∏ä‡πâ partial match ‡πÄ‡∏ä‡πà‡∏ô "reuters" ‡∏à‡∏∞ match "reuters.com"
            return True
    
    return False

def shorten_google_news_url(url: str) -> str:
    """Extract actual URL from Google News redirect"""
    url = normalize_url(url)
    if not url:
        return url
    try:
        u = urlparse(url)
        if "news.google.com" in u.netloc:
            qs = parse_qs(u.query)
            if "url" in qs and qs["url"]:
                return normalize_url(unquote(qs["url"][0]))
    except Exception:
        pass
    return url

def read_sent_links() -> set:
    sent = set()
    for fn in os.listdir(SENT_DIR):
        if not fn.endswith(".txt"):
            continue
        fp = os.path.join(SENT_DIR, fn)
        try:
            with open(fp, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        sent.add(line)
        except Exception:
            continue
    return sent

def append_sent_link(url: str):
    url = normalize_url(url)
    if not url:
        return
    fn = os.path.join(SENT_DIR, now_tz().strftime("%Y-%m-%d") + ".txt")
    with open(fn, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def in_time_window(published_dt: datetime, hours: int) -> bool:
    if not published_dt:
        return False
    return published_dt >= (now_tz() - timedelta(hours=hours))

def cut(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else s[: n - 1].rstrip() + "‚Ä¶"

def create_simple_summary(text: str, max_length: int = 150) -> str:
    """Create a simple summary from text if LLM is not available"""
    text = (text or "").strip()
    if not text:
        return ""
    
    # Remove extra whitespace and newlines
    text = ' '.join(text.split())
    
    # Find first sentence or truncate
    sentences = re.split(r'[.!?]', text)
    if sentences and len(sentences[0]) > 10:
        summary = sentences[0].strip()
        if len(summary) > max_length:
            summary = summary[:max_length-1] + "‚Ä¶"
        return summary + "."
    
    # Fallback: simple truncation
    if len(text) > max_length:
        return text[:max_length-1] + "‚Ä¶"
    return text

# =============================================================================
# RSS PARSING
# =============================================================================
def fetch_feed_with_retry(name: str, url: str, retries: int = 3):
    """‡∏î‡∏∂‡∏á feed ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö retry"""
    for attempt in range(retries):
        try:
            print(f"[FEED] ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {name} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt+1}/{retries})...")
            d = feedparser.parse(url)
            entries = d.entries or []
            print(f"[FEED] {name}: ‡∏û‡∏ö {len(entries)} entries")
            return entries
        except Exception as e:
            print(f"[FEED] {name}: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î - {str(e)}")
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            else:
                return []

def parse_entry(e, feed_name: str, section: str):
    title = (getattr(e, "title", "") or "").strip()
    link = (getattr(e, "link", "") or "").strip()
    summary = (getattr(e, "summary", "") or "").strip()
    published = getattr(e, "published", None) or getattr(e, "updated", None)

    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ published_parsed
    if not published and hasattr(e, 'published_parsed'):
        try:
            import time as time_module
            published = time_module.strftime('%Y-%m-%dT%H:%M:%SZ', e.published_parsed)
        except:
            pass

    try:
        published_dt = dateutil_parser.parse(published) if published else None
        if published_dt and published_dt.tzinfo is None:
            published_dt = TZ.localize(published_dt)
        if published_dt:
            published_dt = published_dt.astimezone(TZ)
    except Exception:
        published_dt = None

    canon = shorten_google_news_url(link)

    return {
        "title": title,
        "url": normalize_url(link),
        "canon_url": normalize_url(canon),
        "summary": summary,
        "published_dt": published_dt,
        "feed": feed_name,
        "section": section,
    }

# =============================================================================
# LLM ANALYZER (‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô)
# =============================================================================
class LLMAnalyzer:
    def __init__(self, api_key: str, model: str, endpoint: str):
        self.api_key = api_key
        self.model = model
        self.endpoint = endpoint
    
    def analyze_news(self, title: str, summary: str) -> dict:
        """Analyze news using LLM"""
        if not self.api_key:
            return self._get_default_analysis(title, summary)
        
        system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
        ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:
        {
            "relevant": true/false,
            "country": "‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á",
            "summary_th": "‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ",
            "topics": ["‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠1", "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠2"]
        }
        
        ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö:"""
        
        user_prompt = f"""‡∏Ç‡πà‡∏≤‡∏ß: {title}
        
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {summary[:500]}
        
        ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ:"""
        
        try:
            response = requests.post(
                self.endpoint,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 300
                },
                timeout=30
            )
            
            if response.status_code != 200:
                print(f"[LLM] HTTP Error {response.status_code}")
                return self._get_default_analysis(title, summary)
            
            data = response.json()
            content = data["choices"][0]["message"]["content"].strip()
            
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
                
                # Validate and clean up
                return {
                    "relevant": bool(analysis.get("relevant", True)),
                    "country": str(analysis.get("country", "")).strip(),
                    "summary_th": str(analysis.get("summary_th", "")).strip()[:150],
                    "topics": [str(t).strip() for t in analysis.get("topics", []) if t]
                }
                
        except json.JSONDecodeError:
            print("[LLM] Failed to parse JSON response")
        except Exception as e:
            print(f"[LLM] Error: {str(e)}")
        
        return self._get_default_analysis(title, summary)
    
    def _get_default_analysis(self, title: str, summary: str):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        combined = f"{title} {summary}"
        simple_summary = create_simple_summary(combined, 100)
        
        return {
            "relevant": True,
            "country": "",
            "summary_th": simple_summary if simple_summary else "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "topics": []
        }

# =============================================================================
# ENHANCED NEWS PROCESSOR (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥)
# =============================================================================
class EnhancedNewsProcessor:
    def __init__(self):
        self.sent_links = read_sent_links()
        self.llm_analyzer = LLMAnalyzer(GROQ_API_KEY, GROQ_MODEL, GROQ_ENDPOINT) if GROQ_API_KEY else None
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß
        self.news_sources = {
            'reuters.com': 'Reuters',
            'bloomberg.com': 'Bloomberg',
            'bangkokpost.com': 'Bangkok Post',
            'thansettakij.com': '‡∏ê‡∏≤‡∏ô‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',
            'posttoday.com': 'Post Today',
            'prachachat.net': '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à',
            'mgronline.com': '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå',
            'komchadluek.net': '‡∏Ñ‡∏°‡∏ä‡∏±‡∏î‡∏•‡∏∂‡∏Å',
            'nationthailand.com': 'The Nation Thailand',
            'naewna.com': '‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡πâ‡∏≤',
            'dailynews.co.th': '‡πÄ‡∏î‡∏•‡∏¥‡∏ô‡∏¥‡∏ß‡∏™‡πå',
            'thairath.co.th': '‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê',
            'khaosod.co.th': '‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏î',
            'matichon.co.th': '‡∏°‡∏ï‡∏¥‡∏ä‡∏ô',
            'sanook.com': '‡∏™‡∏ô‡∏∏‡∏Å‡∏î‡∏≠‡∏ó‡∏Ñ‡∏≠‡∏°',
            'kapook.com': '‡∏Å‡∏∞‡∏õ‡∏∏‡∏Å',
            'manager.co.th': '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£',
            'energynewscenter.com': 'Energy News Center',
        }
        
        # Cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡πÉ‡∏ô session ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        self._title_cache = []
        self._processed_items = []
        self._group_cache = set()
    
    def get_source_name(self, url: str) -> str:
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å URL"""
        domain = extract_domain(url)
        if not domain:
            return domain
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ domain ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        for source_domain, source_name in self.news_sources.items():
            if source_domain in domain:
                return source_name
        
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ domain ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠
        return domain
    
    def _is_similar_title(self, title1: str, title2: str, threshold: float = 0.8) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß"""
        similarity = SequenceMatcher(None, title1, title2).ratio()
        return similarity > threshold
    
    def _create_group_key(self, item: dict) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πà‡∏≤‡∏ß (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥)"""
        title_lower = item.get('title', '').lower()
        country = item.get('country', '')
        
        # ‡∏´‡∏≤‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß
        for keyword in EnhancedKeywordFilter.MAIN_KEYWORDS_FOR_GROUPING:
            if keyword.lower() in title_lower:
                return f"{country}_{keyword}"
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 3 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        words = title_lower.split()[:3]
        return f"{country}_{'_'.join(words)}"
    
    def _score_news_item(self, item: dict) -> int:
        """‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"""
        score = 0
        
        # ‡∏°‡∏µ URL ‡∏à‡∏£‡∏¥‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà google news)
        canon_url = item.get('canon_url') or ''
        if 'news.google.com' not in canon_url and canon_url:
            score += 10
        
        # ‡∏°‡∏µ summary ‡∏¢‡∏≤‡∏ß
        if len(item.get('summary', '')) > 50:
            score += 5
        
        # ‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if item.get('published_dt'):
            score += 3
        
        # ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ
        domain = extract_domain(canon_url)
        if domain in ['reuters.com', 'bloomberg.com', 'bangkokpost.com']:
            score += 5
        
        return score
    
    def _select_better_news(self, item1: dict, item2: dict) -> dict:
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô"""
        score1 = self._score_news_item(item1)
        score2 = self._score_news_item(item2)
        
        print(f"  [DEDUP] ‡∏Ç‡πà‡∏≤‡∏ß 1: {score1} ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô | ‡∏Ç‡πà‡∏≤‡∏ß 2: {score2} ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô")
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤
        return item1 if score1 >= score2 else item2
    
    def fetch_and_filter_news(self):
        """Fetch and filter news from all feeds"""
        all_news = []
        
        for feed_name, feed_type, feed_url in FEEDS:
            print(f"\n[Fetching] {feed_name} ({feed_type})...")
            
            try:
                entries = fetch_feed_with_retry(feed_name, feed_url)
                
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏á MAX_PER_FEED ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                limit = 20 if feed_type == "direct" else MAX_PER_FEED
                
                for entry in entries[:limit]:
                    news_item = self._process_entry(entry, feed_name, feed_type)
                    if news_item:
                        all_news.append(news_item)
                        print(f"  ‚úì {news_item['title'][:50]}...")
                        
            except Exception as e:
                print(f"  ‚úó Error: {str(e)}")
        
        # Step 1.5: Remove group duplicates
        all_news = self._remove_group_duplicates(all_news)
        
        # Sort by date (‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
        all_news.sort(key=lambda x: -((x.get('published_dt') or datetime.min).timestamp()))
        
        return all_news
    
    def _process_entry(self, entry, feed_name: str, feed_type: str):
        """Process individual news entry"""
        item = parse_entry(entry, feed_name, feed_type)
        
        # Basic validation
        if not item["title"] or not item["url"]:
            return None
        
        # Check if already sent
        if item["canon_url"] in self.sent_links or item["url"] in self.sent_links:
            return None
        
        # Check time window
        if item["published_dt"] and not in_time_window(item["published_dt"], WINDOW_HOURS):
            return None
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á (direct) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ALLOWED_NEWS_SOURCES
        if feed_type != "direct":
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            display_url = item["canon_url"] or item["url"]
            if not is_allowed_source(display_url):
                return None
        
        # Combine text for analysis
        full_text = f"{item['title']} {item['summary']}"
        
        # Step 1: Enhanced keyword filtering
        if not EnhancedKeywordFilter.is_valid_energy_news(full_text):
            print(f"  ‚úó ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á: {item['title'][:40]}...")
            return None
        
        # Step 2: Detect country
        country = EnhancedKeywordFilter.detect_country(full_text)
        if not country:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Thailand ‡πÄ‡∏õ‡πá‡∏ô default
            if feed_type == "direct":
                country = "Thailand"
            else:
                return None
        
        # Step 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡πÉ‡∏ô session ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        title_lower = item['title'].lower()
        for existing_title in self._title_cache:
            if self._is_similar_title(title_lower, existing_title, threshold=0.7):
                print(f"  ‚úó ‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡πÉ‡∏ô session: {item['title'][:40]}...")
                return None
        self._title_cache.append(title_lower)
        
        # Step 4: Check for similar existing news
        existing_item = self._find_similar_news(item, country)
        if existing_item:
            print(f"  ! ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô: {item['title'][:40]}...")
            return self._select_better_news(item, existing_item)
        
        # Step 5: LLM analysis (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
        llm_summary = ""
        if USE_LLM_SUMMARY and self.llm_analyzer:
            llm_analysis = self.llm_analyzer.analyze_news(item['title'], item['summary'])
            
            # ‡πÉ‡∏ä‡πâ LLM country ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
            if llm_analysis['country'] and llm_analysis['country'] in PROJECTS_BY_COUNTRY:
                country = llm_analysis['country']
            
            # ‡πÉ‡∏ä‡πâ summary ‡∏à‡∏≤‡∏Å LLM
            if llm_analysis.get('summary_th'):
                llm_summary = llm_analysis['summary_th']
        
        # Get project hints for this country
        project_hints = PROJECTS_BY_COUNTRY.get(country, [])[:2]
        
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß
        display_url = item["canon_url"] or item["url"]
        source_name = self.get_source_name(display_url)
        
        # Build final news item
        final_item = {
            'title': item['title'][:100],
            'url': item['url'],
            'canon_url': item['canon_url'],
            'source_name': source_name,
            'domain': extract_domain(display_url),
            'summary': item['summary'][:200],
            'published_dt': item['published_dt'],
            'country': country,
            'project_hints': project_hints,
            'llm_summary': llm_summary,
            'feed': feed_name,
            'feed_type': feed_type,
            'simple_summary': create_simple_summary(full_text, 100)
        }
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô processed items
        self._processed_items.append(final_item)
        
        return final_item
    
    def _find_similar_news(self, new_item: dict, country: str):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô"""
        for existing in self._processed_items:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
            if existing.get('country') != country:
                continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
            title_similarity = self._is_similar_title(
                new_item['title'].lower(),
                existing['title'].lower(),
                threshold=0.7
            )
            
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å
            if title_similarity:
                return existing
        
        return None
    
    def _remove_group_duplicates(self, news_items):
        """‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô"""
        unique_items = []
        
        for item in news_items:
            group_key = self._create_group_key(item)
            
            if group_key in self._group_cache:
                continue
            
            self._group_cache.add(group_key)
            unique_items.append(item)
        
        print(f"[DEDUP] ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥: {len(news_items)} -> {len(unique_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
        return unique_items

# =============================================================================
# ENHANCED LINE MESSAGE BUILDER
# =============================================================================
class EnhancedLineMessageBuilder:
    @staticmethod
    def create_flex_bubble(news_item):
        """Create a LINE Flex Bubble for a news item"""
        title = cut(news_item.get('title', ''), 80)
        
        # Format timestamp
        pub_dt = news_item.get('published_dt')
        time_str = pub_dt.strftime("%d/%m/%Y %H:%M") if pub_dt else ""
        
        # ‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        colors = {
            "Thailand": "#FF6B6B",
            "Vietnam": "#4ECDC4",
            "Myanmar": "#FFD166",
            "Malaysia": "#06D6A0",
            "Indonesia": "#118AB2",
            "UAE": "#9D4EDD",
            "Oman": "#F15BB5",
            "Kazakhstan": "#00BBF9",
            "International": "#888888"
        }
        
        color = colors.get(news_item.get('country', 'International'), "#888888")
        
        # Build bubble contents
        contents = [
            {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": title,
                        "weight": "bold",
                        "size": "md",
                        "wrap": True,
                        "color": "#FFFFFF"
                    }
                ],
                "backgroundColor": color,
                "paddingAll": "12px",
                "cornerRadius": "8px"
            }
        ]
        
        # Add metadata - ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        metadata_parts = []
        if time_str:
            metadata_parts.append(time_str)
        if news_item.get('feed'):
            metadata_parts.append(news_item['feed'])
        
        if metadata_parts:
            contents.append({
                "type": "text",
                "text": " | ".join(metadata_parts),
                "size": "xs",
                "color": "#888888",
                "margin": "sm"
            })
        
        # ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà**
        if news_item.get('source_name'):
            # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å dictionary ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
            contents.append({
                "type": "text",
                "text": f"üì∞ {news_item['source_name']}",
                "size": "xs",
                "color": "#666666",
                "margin": "sm"
            })
        elif news_item.get('domain'):
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ domain
            contents.append({
                "type": "text",
                "text": f"üåê {cut(news_item['domain'], 30)}",
                "size": "xs",
                "color": "#666666",
                "margin": "sm"
            })
        
        # Add country
        contents.append({
            "type": "text",
            "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {news_item.get('country', 'N/A')}",
            "size": "sm",
            "margin": "xs",
            "color": color,
            "weight": "bold"
        })
        
        # Add project hints
        if news_item.get('project_hints'):
            hints_text = ", ".join(news_item['project_hints'][:2])
            contents.append({
                "type": "text",
                "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {hints_text}",
                "size": "sm",
                "color": "#2E7D32",
                "wrap": True,
                "margin": "xs"
            })
        
        # ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢**
        summary_text = ""
        
        # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å LLM ‡∏Å‡πà‡∏≠‡∏ô
        if news_item.get('llm_summary'):
            summary_text = news_item['llm_summary']
        # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏≤‡∏Å LLM ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ simple summary
        elif news_item.get('simple_summary'):
            summary_text = news_item['simple_summary']
        # 3. Fallback ‡πÉ‡∏ä‡πâ summary ‡∏à‡∏≤‡∏Å RSS
        elif news_item.get('summary'):
            summary_text = create_simple_summary(news_item['summary'], 120)
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏£‡∏∏‡∏õ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å title
        if not summary_text or len(summary_text.strip()) < 10:
            summary_text = f"{news_item.get('title', '‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô')[:60]}..."
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏™‡∏£‡∏∏‡∏õ (‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢)
        if summary_text:
            contents.append({
                "type": "text",
                "text": cut(summary_text, 120),
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#424242"
            })
        
        # Create bubble
        bubble = {
            "type": "bubble",
            "size": "kilo",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": contents,
                "paddingAll": "12px",
                "spacing": "sm"
            }
        }
        
        # Add button if URL exists
        url = news_item.get('canon_url') or news_item.get('url')
        if url and len(url) < 1000:
            bubble["footer"] = {
                "type": "box",
                "layout": "vertical",
                "spacing": "sm",
                "contents": [
                    {
                        "type": "button",
                        "style": "primary",
                        "height": "sm",
                        "action": {
                            "type": "uri",
                            "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°",
                            "uri": url
                        }
                    }
                ]
            }
        
        return bubble
    
    @staticmethod
    def create_carousel_message(news_items):
        """Create LINE carousel message from news items"""
        bubbles = []
        
        for item in news_items[:BUBBLES_PER_CAROUSEL]:
            bubble = EnhancedLineMessageBuilder.create_flex_bubble(item)
            if bubble:
                bubbles.append(bubble)
        
        if not bubbles:
            return None
        
        return {
            "type": "flex",
            "altText": f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô {datetime.now(TZ).strftime('%d/%m/%Y')} ({len(bubbles)} ‡∏Ç‡πà‡∏≤‡∏ß)",
            "contents": {
                "type": "carousel",
                "contents": bubbles
            }
        }

# =============================================================================
# LINE SENDER
# =============================================================================
class LineSender:
    def __init__(self, access_token):
        self.access_token = access_token
        self.headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
    
    def send_message(self, message_obj):
        """Send message to LINE"""
        if DRY_RUN:
            print("\n" + "="*60)
            print("DRY RUN - Would send the following news:")
            print("="*60)
            
            contents = message_obj.get('contents', {}).get('contents', [])
            for i, bubble in enumerate(contents):
                body_contents = bubble.get('body', {}).get('contents', [])
                title = ""
                source = ""
                country = ""
                
                for content in body_contents:
                    if content.get('type') == 'text':
                        text = content.get('text', '')
                        if len(text) > 10 and not title:
                            title = text[:60]
                        elif 'üì∞' in text or 'üåê' in text:
                            source = text
                        elif text.startswith("‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®:"):
                            country = text.replace("‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: ", "")
                
                print(f"{i+1}. {title}")
                if country:
                    print(f"   ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {country}")
                if source:
                    print(f"   ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß: {source}")
                print()
            
            print(f"Total: {len(contents)} news items")
            return True
        
        url = "https://api.line.me/v2/bot/message/broadcast"
        
        try:
            response = requests.post(
                url,
                headers=self.headers,
                json={"messages": [message_obj]},
                timeout=30
            )
            
            if response.status_code == 200:
                print("[LINE] Message sent successfully!")
                return True
            else:
                print(f"[LINE] Error {response.status_code}: {response.text[:200]}")
                return False
                
        except Exception as e:
            print(f"[LINE] Exception: {str(e)}")
            return False

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    print("="*60)
    print("‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
    print("="*60)
    
    # Configuration check
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("[ERROR] LINE_CHANNEL_ACCESS_TOKEN is required")
        return
    
    if USE_LLM_SUMMARY and not GROQ_API_KEY:
        print("[WARNING] LLM summary enabled but no GROQ_API_KEY provided")
        print("[INFO] Will use simple summary for all news")
    
    print(f"\n[CONFIG] Use LLM: {'Yes' if USE_LLM_SUMMARY and GROQ_API_KEY else 'No (simple summary)'}")
    print(f"[CONFIG] Time window: {WINDOW_HOURS} hours")
    print(f"[CONFIG] Dry run: {'Yes' if DRY_RUN else 'No'}")
    print(f"[CONFIG] Allowed news sources: {ALLOWED_NEWS_SOURCES_LIST if ALLOWED_NEWS_SOURCES_LIST else 'All sources'}")
    print(f"[CONFIG] ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feed ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(FEEDS)}")
    print(f"[CONFIG] Feed ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£: {[f[0] for f in FEEDS]}")
    
    # Initialize components
    processor = EnhancedNewsProcessor()
    line_sender = LineSender(LINE_CHANNEL_ACCESS_TOKEN)
    
    # Step 1: Fetch and filter news
    print("\n[1] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß...")
    news_items = processor.fetch_and_filter_news()
    
    if not news_items:
        print("\n[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
        return
    
    print(f"\n[2] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # Count statistics
    llm_summary_count = sum(1 for item in news_items if item.get('llm_summary'))
    direct_count = sum(1 for item in news_items if item.get('feed_type') == 'direct')
    
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß
    source_counts = {}
    country_counts = {}
    for item in news_items:
        source = item.get('source_name') or item.get('domain', 'Unknown')
        source_counts[source] = source_counts.get(source, 0) + 1
        
        country = item.get('country', 'Unknown')
        country_counts[country] = country_counts.get(country, 0) + 1
    
    print(f"   - ‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢ AI: {llm_summary_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á: {direct_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏û‡∏ö:")
    for source, count in sorted(source_counts.items()):
        print(f"     ‚Ä¢ {source}: {count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®:")
    for country, count in sorted(country_counts.items()):
        print(f"     ‚Ä¢ {country}: {count} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # Step 2: Create LINE message
    print("\n[3] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LINE...")
    line_message = EnhancedLineMessageBuilder.create_carousel_message(news_items)
    
    if not line_message:
        print("[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ")
        return
    
    # Step 3: Send message
    print("\n[4] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
    success = line_sender.send_message(line_message)
    
    # Step 4: Mark as sent if successful
    if success and not DRY_RUN:
        for item in news_items:
            append_sent_link(item.get('canon_url') or item.get('url'))
        print("\n[SUCCESS] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    
    print("\n" + "="*60)
    print("‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    print("="*60)

if __name__ == "__main__":
    main()
