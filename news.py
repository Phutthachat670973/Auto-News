# -*- coding: utf-8 -*-

import os
import re
import json
import time
import random
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote
from typing import List, Dict, Tuple, Optional

import requests
import feedparser
import pytz
from dateutil import parser as dateutil_parser

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# =============================================================================
# ENV / CONFIG
# =============================================================================
TZ = pytz.timezone(os.getenv("TZ", "Asia/Bangkok"))

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE_CHANNEL_ACCESS_TOKEN")

# Groq (OpenAI-compatible)
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-70b-versatile").strip()  # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
GROQ_ENDPOINT = os.getenv("GROQ_ENDPOINT", "https://api.groq.com/openai/v1/chat/completions").strip()
USE_LLM_ANALYSIS = os.getenv("USE_LLM_ANALYSIS", "1").strip().lower() in ["1", "true", "yes", "y"]

WINDOW_HOURS = int(os.getenv("WINDOW_HOURS", "72"))  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤ window ‡πÄ‡∏õ‡πá‡∏ô 72 ‡∏ä‡∏°.
MAX_PER_FEED = int(os.getenv("MAX_PER_FEED", "50"))  # ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠ feed

# LLM configuration
LLM_ANALYSIS_MAX_TOKENS = int(os.getenv("LLM_ANALYSIS_MAX_TOKENS", "1500"))
LLM_BATCH_SIZE = int(os.getenv("LLM_BATCH_SIZE", "5"))  # ‡∏•‡∏î batch size ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
LLM_MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", "5"))
LLM_BASE_BACKOFF = float(os.getenv("LLM_BASE_BACKOFF", "2.0"))

# =============================================================================
# PROJECT DATABASE (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô)
# =============================================================================
PROJECTS_DETAILED = {
    "Thailand": {
        "projects": [
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 2/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå", "Arthit",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏™ 1", "S1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4", "Contract 4",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ‡∏≠‡∏µ‡∏û‡∏µ 1", "PTTEP 1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 6/27",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 22/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏µ 5", "E5",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏†‡∏π‡∏Æ‡πà‡∏≠‡∏°", "Sinphuhorm",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 8/32", "B8/32", "9A", "9‡πÄ‡∏≠",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/48", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 12/48",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/65", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 3/65",
            "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 53/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 54/43"
        ],
        "keywords": [
            "‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡∏Å‡∏£‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡∏Å‡∏ü‡∏ú.", "‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ",
            "‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÄ‡∏û‡∏•‡∏¥‡∏á", "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô",
            "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ú‡∏•‡∏¥‡∏ï‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "PDP", "‡∏Ñ‡πà‡∏≤ Ft", "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤",
            "‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô", "‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå‡πÄ‡∏ã‡∏•‡∏•‡πå", "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏°",
            "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ä‡∏µ‡∏ß‡∏°‡∏ß‡∏•", "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô‡πÉ‡∏ï‡πâ‡∏û‡∏¥‡∏†‡∏û"
        ],
        "entities": [
            "‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡∏≠‡∏ò‡∏¥‡∏ö‡∏î‡∏µ‡∏Å‡∏£‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡∏ú‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ",
            "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏Å‡∏û.", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏õ‡∏ï‡∏ó. ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏Å‡∏ü‡∏ú. ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)"
        ]
    },
    # ... ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÉ‡∏ô‡∏ó‡∏≥‡∏ô‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
}

# =============================================================================
# FEEDS (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)
# =============================================================================
def gnews_rss(q: str, hl="en", gl="US", ceid="US:en") -> str:
    return f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl={hl}&gl={gl}&ceid={ceid}"

FEEDS = [
    ("GoogleNewsTH_EnergyOfficial", "official_thai", gnews_rss(
        '(site:ratchakitcha.soc.go.th OR site:energy.go.th OR site:egat.co.th OR site:pptplc.com OR site:pttep.com) AND (‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR ‡∏Å‡πä‡∏≤‡∏ã OR ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô)',
        hl="th", gl="TH", ceid="TH:th"
    )),
    ("GoogleNewsTH_FinanceEnergy", "finance_thai", gnews_rss(
        '(site:bangkokbiznews.com OR site:thunhoon.com OR site:posttoday.com OR site:manager.co.th) AND (‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR ‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤)',
        hl="th", gl="TH", ceid="TH:th"
    )),
    ("GoogleNewsEN_EnergyPolicy", "policy_international", gnews_rss(
        '(energy policy OR electricity tariff OR power regulation OR LNG OR natural gas) AND (Thailand OR Malaysia OR Vietnam OR Indonesia OR Middle East)',
        hl="en", gl="US", ceid="US:en"
    )),
    ("Reuters_Energy", "international", "https://www.reutersagency.com/feed/?best-topics=energy-environment&post_type=best"),
    ("Bloomberg_Energy", "international", "https://news.google.com/rss/search?q=site:bloomberg.com+energy+policy&hl=en&gl=US&ceid=US:en"),
]

# =============================================================================
# LLM ANALYZER CLASS
# =============================================================================
class LLMNewsAnalyzer:
    def __init__(self, api_key: str, endpoint: str, model: str):
        self.api_key = api_key
        self.endpoint = endpoint
        self.model = model
        
    def _call_api_with_retry(self, messages: List[Dict], max_tokens: int = 1500) -> str:
        """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Groq API ‡∏û‡∏£‡πâ‡∏≠‡∏° retry mechanism"""
        if not self.api_key:
            return ""
            
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.2,  # ‡∏•‡∏î temperature ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            "max_tokens": max_tokens,
            "top_p": 0.95
        }
        
        for attempt in range(LLM_MAX_RETRIES):
            try:
                response = requests.post(
                    self.endpoint,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 429:
                    wait_time = (LLM_BASE_BACKOFF ** (attempt + 1)) + random.uniform(0.0, 1.0)
                    print(f"[LLM] Rate limited, waiting {wait_time:.1f}s (attempt {attempt + 1})")
                    time.sleep(wait_time)
                    continue
                    
                response.raise_for_status()
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
                
            except requests.exceptions.RequestException as e:
                if attempt == LLM_MAX_RETRIES - 1:
                    print(f"[LLM] Error after {LLM_MAX_RETRIES} attempts: {e}")
                    return ""
                wait_time = (LLM_BASE_BACKOFF ** (attempt + 1))
                time.sleep(wait_time)
                
        return ""
    
    def analyze_news_relevance(self, title: str, summary: str, full_text: str = "") -> Dict:
        """
        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡πâ‡∏ß‡∏¢ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
        """
        if not self.api_key:
            return self._get_fallback_analysis()
            
        system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
        ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏≤‡∏° format ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á:
        
        {
            "is_relevant": boolean,  // ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô/‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            "relevance_score": 0-100,  // ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏™‡∏π‡∏á = ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å)
            "country": "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",  // ‡πÄ‡∏ä‡πà‡∏ô Thailand, Malaysia, Vietnam
            "project_names": ["‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"],  // ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            "topics": ["‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å"],  // ‡πÄ‡∏ä‡πà‡∏ô ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πä‡∏≤‡∏ã, ‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤
            "is_official_news": boolean,  // ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            "impact_level": "high|medium|low",  // ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
            "summary_analysis": "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏±‡πâ‡∏ô‡πÜ"  // ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        }
        
        ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô:
        1. ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£: ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤, ‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ, ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á, ‡∏Å‡∏≤‡∏£‡πÅ‡∏ñ‡∏•‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
        2. ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤, ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö
        3. ‡∏Ç‡πà‡∏≤‡∏ß‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: ‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£, ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ú‡∏ô, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á
        4. ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πä‡∏≤‡∏ã/‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô, ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢, ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÉ‡∏´‡∏°‡πà
        """
        
        user_content = f"""‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
        
        ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {title}
        
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏£‡∏∏‡∏õ: {summary}
        
        {f'‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {full_text[:1000]}' if full_text else ''}
        
        ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ä‡∏±‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
        
        result = self._call_api_with_retry(messages, LLM_ANALYSIS_MAX_TOKENS)
        
        if result:
            try:
                # ‡πÅ‡∏¢‡∏Å JSON ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                json_match = re.search(r'\{.*\}', result, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                    return self._validate_analysis(analysis)
            except json.JSONDecodeError:
                print(f"[LLM] Failed to parse JSON: {result[:200]}")
                
        return self._get_fallback_analysis()
    
    def _validate_analysis(self, analysis: Dict) -> Dict:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å LLM"""
        validated = {
            "is_relevant": bool(analysis.get("is_relevant", False)),
            "relevance_score": min(100, max(0, int(analysis.get("relevance_score", 0)))),
            "country": str(analysis.get("country", "")).strip(),
            "project_names": [str(p).strip() for p in analysis.get("project_names", []) if p],
            "topics": [str(t).strip() for t in analysis.get("topics", []) if t],
            "is_official_news": bool(analysis.get("is_official_news", False)),
            "impact_level": str(analysis.get("impact_level", "low")).lower(),
            "summary_analysis": str(analysis.get("summary_analysis", "")).strip()[:200]
        }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        if validated["country"] and validated["country"] not in PROJECTS_DETAILED:
            validated["country"] = ""
            
        return validated
    
    def _get_fallback_analysis(self) -> Dict:
        """‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ"""
        return {
            "is_relevant": False,
            "relevance_score": 0,
            "country": "",
            "project_names": [],
            "topics": [],
            "is_official_news": False,
            "impact_level": "low",
            "summary_analysis": ""
        }

# =============================================================================
# CONTENT FETCHER (‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å URL)
# =============================================================================
class ContentFetcher:
    @staticmethod
    def fetch_article_content(url: str) -> Tuple[str, bool]:
        """‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å URL ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'th,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            html_content = response.text
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            is_official_source = ContentFetcher._check_official_source(url, html_content)
            
            # ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å
            content = ContentFetcher._extract_main_content(html_content)
            
            return content[:3000], is_official_source  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
            
        except Exception as e:
            print(f"[Fetcher] Error fetching {url}: {e}")
            return "", False
    
    @staticmethod
    def _check_official_source(url: str, html: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"""
        official_domains = [
            'ratchakitcha.soc.go.th',
            'energy.go.th',
            'egat.co.th',
            'pptplc.com',
            'pttep.com',
            'reuters.com',
            'bloomberg.com',
            'iea.org',
            'worldbank.org'
        ]
        
        domain = urlparse(url).netloc.lower()
        return any(domain.endswith(official_domain) for official_domain in official_domains)
    
    @staticmethod
    def _extract_main_content(html: str) -> str:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å HTML"""
        # ‡πÉ‡∏ä‡πâ regex pattern ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        patterns = [
            r'<article[^>]*>(.*?)</article>',
            r'<div[^>]*class=["\'][^"\']*article[^"\']*["\'][^>]*>(.*?)</div>',
            r'<div[^>]*class=["\'][^"\']*content[^"\']*["\'][^>]*>(.*?)</div>',
            r'<main[^>]*>(.*?)</main>',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html, re.DOTALL | re.IGNORECASE)
            if match:
                # ‡∏•‡∏ö tag HTML
                content = re.sub(r'<[^>]+>', ' ', match.group(1))
                content = re.sub(r'\s+', ' ', content)
                return content.strip()
        
        return ""

# =============================================================================
# MAIN PROCESSING PIPELINE
# =============================================================================
class NewsProcessor:
    def __init__(self):
        self.llm_analyzer = LLMNewsAnalyzer(GROQ_API_KEY, GROQ_ENDPOINT, GROQ_MODEL)
        self.content_fetcher = ContentFetcher()
        self.sent_links = set()
        
    def process_feeds(self) -> List[Dict]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        all_news = []
        
        for feed_name, feed_type, feed_url in FEEDS:
            print(f"[Processing] Feed: {feed_name}")
            entries = self._fetch_feed_entries(feed_url)
            
            for entry in entries[:MAX_PER_FEED]:
                news_item = self._process_news_entry(entry, feed_name, feed_type)
                if news_item and self._should_include(news_item):
                    all_news.append(news_item)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        all_news.sort(key=lambda x: (
            -x.get('analysis', {}).get('relevance_score', 0),
            -x.get('analysis', {}).get('is_official_news', False)
        ))
        
        return all_news[:MAX_MESSAGES_PER_RUN]
    
    def _fetch_feed_entries(self, feed_url: str):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RSS feed"""
        try:
            feed = feedparser.parse(feed_url)
            return feed.entries
        except Exception as e:
            print(f"[Error] Failed to parse feed {feed_url}: {e}")
            return []
    
    def _process_news_entry(self, entry, feed_name: str, feed_type: str) -> Optional[Dict]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß"""
        title = getattr(entry, 'title', '').strip()
        url = getattr(entry, 'link', '').strip()
        summary = getattr(entry, 'summary', '').strip()
        
        if not title or not url:
            return None
        
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
        full_content, is_official_source = self.content_fetcher.fetch_article_content(url)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM
        if USE_LLM_ANALYSIS:
            analysis = self.llm_analyzer.analyze_news_relevance(title, summary, full_content)
        else:
            analysis = self.llm_analyzer._get_fallback_analysis()
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° flag ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
        analysis['is_official_source'] = is_official_source
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏°
        if not analysis['is_relevant'] or analysis['relevance_score'] < 40:
            return None
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß
        return {
            'title': title,
            'url': url,
            'summary': summary,
            'analysis': analysis,
            'feed_name': feed_name,
            'feed_type': feed_type,
            'timestamp': datetime.now(TZ)
        }
    
    def _should_include(self, news_item: Dict) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏£‡∏ß‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        analysis = news_item.get('analysis', {})
        
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
        criteria = [
            analysis.get('relevance_score', 0) >= 50,  # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            analysis.get('is_official_news', False) or analysis.get('is_official_source', False),  # ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
            len(analysis.get('project_names', [])) > 0 or analysis.get('impact_level') in ['high', 'medium'],  # ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö
        ]
        
        return any(criteria)

# =============================================================================
# LINE MESSAGE BUILDER (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
# =============================================================================
class LineMessageBuilder:
    @staticmethod
    def create_flex_message(news_items: List[Dict]) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Flex Message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE"""
        bubbles = []
        
        for item in news_items:
            bubble = LineMessageBuilder._create_news_bubble(item)
            if bubble:
                bubbles.append(bubble)
        
        if not bubbles:
            return None
        
        return {
            "type": "flex",
            "altText": f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ {datetime.now(TZ).strftime('%d/%m/%Y')}",
            "contents": {
                "type": "carousel",
                "contents": bubbles[:BUBBLES_PER_CAROUSEL]
            }
        }
    
    @staticmethod
    def _create_news_bubble(news_item: Dict) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á bubble ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß"""
        analysis = news_item.get('analysis', {})
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö
        color_map = {
            'high': '#FF6B6B',
            'medium': '#FFA726',
            'low': '#42A5F5'
        }
        impact_color = color_map.get(analysis.get('impact_level', 'low'), '#42A5F5')
        
        # ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß
        header = {
            "type": "box",
            "layout": "vertical",
            "backgroundColor": impact_color,
            "paddingAll": "10px",
            "contents": [
                {
                    "type": "text",
                    "text": "üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£",
                    "color": "#FFFFFF",
                    "weight": "bold",
                    "size": "sm"
                } if analysis.get('is_official_news') else {
                    "type": "text",
                    "text": "üìä ‡∏Ç‡πà‡∏≤‡∏ß‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå",
                    "color": "#FFFFFF",
                    "weight": "bold",
                    "size": "sm"
                }
            ]
        }
        
        # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        body_contents = [
            {
                "type": "text",
                "text": news_item['title'],
                "weight": "bold",
                "size": "lg",
                "wrap": True,
                "margin": "md"
            },
            {
                "type": "text",
                "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {analysis.get('country', 'N/A')}",
                "size": "sm",
                "color": "#666666",
                "margin": "sm"
            }
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
        if analysis.get('project_names'):
            projects_text = ", ".join(analysis['project_names'][:3])
            body_contents.append({
                "type": "text",
                "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {projects_text}",
                "size": "sm",
                "color": "#2E7D32",
                "wrap": True,
                "margin": "sm"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        if analysis.get('topics'):
            topics_text = ", ".join(analysis['topics'][:3])
            body_contents.append({
                "type": "text",
                "text": f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {topics_text}",
                "size": "sm",
                "color": "#5D4037",
                "wrap": True,
                "margin": "sm"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏£‡∏∏‡∏õ
        if analysis.get('summary_analysis'):
            body_contents.append({
                "type": "text",
                "text": analysis['summary_analysis'],
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#424242"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        body_contents.append({
            "type": "box",
            "layout": "baseline",
            "margin": "md",
            "contents": [
                {
                    "type": "text",
                    "text": "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:",
                    "size": "sm",
                    "color": "#666666",
                    "flex": 2
                },
                {
                    "type": "text",
                    "text": f"{analysis.get('relevance_score', 0)}/100",
                    "size": "sm",
                    "color": impact_color,
                    "weight": "bold",
                    "flex": 1,
                    "align": "end"
                }
            ]
        })
        
        body = {
            "type": "box",
            "layout": "vertical",
            "contents": body_contents
        }
        
        # ‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á (‡∏õ‡∏∏‡πà‡∏°)
        footer = {
            "type": "box",
            "layout": "vertical",
            "spacing": "sm",
            "contents": [
                {
                    "type": "button",
                    "style": "primary",
                    "height": "sm",
                    "action": {
                        "type": "uri",
                        "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°",
                        "uri": news_item['url']
                    }
                }
            ]
        }
        
        return {
            "type": "bubble",
            "size": "kilo",
            "header": header,
            "body": body,
            "footer": footer
        }

# =============================================================================
# MAIN EXECUTION
# =============================================================================
def main():
    print("=" * 60)
    print("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ LLM")
    print("=" * 60)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö configuration
    if not GROQ_API_KEY and USE_LLM_ANALYSIS:
        print("[Warning] GROQ_API_KEY ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ ‡πÅ‡∏ï‡πà USE_LLM_ANALYSIS ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà")
        print("[Info] ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏ó‡∏ô")
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô processor
    processor = NewsProcessor()
    
    # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πà‡∏≤‡∏ß
    print("\n[Status] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß...")
    relevant_news = processor.process_feeds()
    
    print(f"\n[Result] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(relevant_news)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    if not relevant_news:
        print("[Info] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ")
        return
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE
    print("\n[Status] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE...")
    message_builder = LineMessageBuilder()
    flex_message = message_builder.create_flex_message(relevant_news)
    
    if not flex_message:
        print("[Error] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ")
        return
    
    # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE
    if DRY_RUN:
        print("\n=== DRY RUN - Flex Message Preview ===")
        print(json.dumps(flex_message, ensure_ascii=False, indent=2))
    else:
        print("\n[Status] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE...")
        success = send_line_message(flex_message)
        if success:
            print("[Success] ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        else:
            print("[Error] ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")

if __name__ == "__main__":
    main()
