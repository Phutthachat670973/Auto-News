# news.py
# ============================================================================================================
# AUTO NEWS BOT (RSS -> Groq LLM -> LINE Broadcast)
# - Uses GroqCloud LLM (OpenAI-compatible "chat.completions")
# - Batch tagging to reduce API calls
# - Dedup via sent_links directory
# - Bangkok timezone + time window filter (21:00 - 06:00) optional
# ============================================================================================================

import os
import re
import json
import time
import hashlib
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import feedparser
import pytz
import requests
from dateutil import parser as dateutil_parser
from groq import Groq

# ============================================================================================================
# ENV / CONFIG
# ============================================================================================================

BKK_TZ = pytz.timezone(os.getenv("TIMEZONE", "Asia/Bangkok"))

GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
if not GROQ_API_KEY:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY")

GROQ_MODEL = os.getenv("GROQ_MODEL_NAME", "llama-3.1-8b-instant").strip()

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_CHANNEL_ACCESS_TOKEN")

# LLM behavior
BATCH_SIZE = int(os.getenv("LLM_BATCH_SIZE", "10"))           # 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠ 1 request
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "6"))
SLEEP_BETWEEN_CALLS_MIN = float(os.getenv("SLEEP_MIN", "0.15"))
SLEEP_BETWEEN_CALLS_MAX = float(os.getenv("SLEEP_MAX", "0.45"))
TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.35"))

# Filtering / runtime
MAX_ITEMS_TO_LLM = int(os.getenv("MAX_ITEMS_TO_LLM", "80"))   # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM (‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô)
LOOKBACK_HOURS = int(os.getenv("LOOKBACK_HOURS", "36"))       # ‡∏Ç‡πà‡∏≤‡∏ß‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô RSS ‡∏ó‡∏µ‡πà‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô)
USE_TIME_WINDOW = os.getenv("USE_TIME_WINDOW", "true").lower() in ("1", "true", "yes", "y")

# Time window (21:00 -> 06:00)
WINDOW_START_HOUR = int(os.getenv("WINDOW_START_HOUR", "21"))  # 21:00
WINDOW_END_HOUR = int(os.getenv("WINDOW_END_HOUR", "6"))       # 06:00

# Line options
DRY_RUN = os.getenv("DRY_RUN", "false").lower() in ("1", "true", "yes", "y")
LINE_BROADCAST_ENDPOINT = "https://api.line.me/v2/bot/message/broadcast"

# Storage
SENT_DIR = os.getenv("SENT_DIR", "sent_links")
os.makedirs(SENT_DIR, exist_ok=True)

groq_client = Groq(api_key=GROQ_API_KEY)

# ============================================================================================================
# RSS SOURCES (‡∏õ‡∏£‡∏±‡∏ö/‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ)
# - feed_country: ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≠‡∏ô‡∏Ñ‡∏±‡∏î‡∏Ç‡πà‡∏≤‡∏ß (‡πÄ‡∏Ç‡πâ‡∏°: ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á)
# ============================================================================================================

RSS_SOURCES = [
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
    {"name": "OilPrice", "url": "https://oilprice.com/rss/main", "feed_country": "Global"},
    {"name": "Reuters Energy", "url": "https://www.reutersagency.com/feed/?best-topics=energy&post_type=best", "feed_country": "Global"},
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ...
]

# ============================================================================================================
# PROJECT MAP (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á) : ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® -> ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
# - ‡πÉ‡∏™‡πà‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
# ============================================================================================================

PROJECTS_BY_COUNTRY = {
    "Thailand": ["Erawan", "Bongkot", "Arthit", "G1/61", "G2/61"],
    "Malaysia": ["Block H", "SK410B", "SK417"],
    "UAE": ["Sharjah", "Abu Dhabi (Onshore/Offshore)"],
    "Oman": ["Block 61", "Block 6"],
    "Qatar": ["Qatar LNG Related"],
    "Indonesia": ["Natuna", "Bongkot (TH) - example remove", "Mahakam"],
    "Global": ["ALL"],
}

def projects_for_country(country: str):
    c = (country or "").strip()
    return PROJECTS_BY_COUNTRY.get(c, [])

# ============================================================================================================
# UTIL: URL normalize / dedupe
# ============================================================================================================

TRACKING_PARAMS_PREFIX = ("utm_", "fbclid", "gclid", "yclid", "mc_cid", "mc_eid")

def normalize_url(url: str) -> str:
    if not url:
        return ""
    try:
        u = url.strip()
        parsed = urlparse(u)
        q = parse_qsl(parsed.query, keep_blank_values=True)
        q2 = []
        for k, v in q:
            lk = k.lower()
            if lk.startswith(TRACKING_PARAMS_PREFIX) or lk in TRACKING_PARAMS_PREFIX:
                continue
            q2.append((k, v))
        new_query = urlencode(q2, doseq=True)
        normalized = parsed._replace(query=new_query, fragment="")
        return urlunparse(normalized)
    except Exception:
        return url.strip()

def link_key(url: str) -> str:
    h = hashlib.sha1(url.encode("utf-8", errors="ignore")).hexdigest()
    return h

def was_sent(url: str) -> bool:
    k = link_key(url)
    return os.path.exists(os.path.join(SENT_DIR, k + ".txt"))

def mark_sent(url: str):
    k = link_key(url)
    path = os.path.join(SENT_DIR, k + ".txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write(url)

# ============================================================================================================
# UTIL: time filtering
# ============================================================================================================

def now_bkk() -> datetime:
    return datetime.now(BKK_TZ)

def within_time_window(dt: datetime) -> bool:
    if not USE_TIME_WINDOW:
        return True
    # window 21:00 -> 06:00 (cross midnight)
    h = dt.hour
    if WINDOW_START_HOUR <= WINDOW_END_HOUR:
        return WINDOW_START_HOUR <= h < WINDOW_END_HOUR
    return (h >= WINDOW_START_HOUR) or (h < WINDOW_END_HOUR)

def parse_entry_datetime(entry) -> datetime | None:
    # Try published / updated fields
    for key in ("published", "updated", "created"):
        if getattr(entry, key, None):
            try:
                d = dateutil_parser.parse(getattr(entry, key))
                if d.tzinfo is None:
                    d = BKK_TZ.localize(d)
                return d.astimezone(BKK_TZ)
            except Exception:
                pass

    # Try feedparser struct_time
    for key in ("published_parsed", "updated_parsed"):
        if getattr(entry, key, None):
            try:
                d = datetime.fromtimestamp(time.mktime(getattr(entry, key)), tz=pytz.UTC).astimezone(BKK_TZ)
                return d
            except Exception:
                pass
    return None

# ============================================================================================================
# GROQ: JSON extraction + retries
# ============================================================================================================

def _extract_json_object(text: str):
    """
    Robust JSON extractor:
    - find first {...} block
    - remove trailing junk
    """
    if not text:
        return None
    s = text.strip()

    # Remove code fences if any
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s, flags=re.IGNORECASE | re.MULTILINE).strip()

    # Fast path
    try:
        return json.loads(s)
    except Exception:
        pass

    # Find first json object
    m = re.search(r"\{.*\}", s, flags=re.DOTALL)
    if not m:
        return None
    candidate = m.group(0).strip()

    # Try clean common trailing commas
    candidate = re.sub(r",\s*([}\]])", r"\1", candidate)

    try:
        return json.loads(candidate)
    except Exception:
        return None

def _is_429(e: Exception) -> bool:
    s = str(e).lower()
    return ("429" in s) or ("too many requests" in s) or ("rate limit" in s)

def call_groq(prompt: str, temperature: float = TEMPERATURE) -> str:
    time.sleep(random.uniform(SLEEP_BETWEEN_CALLS_MIN, SLEEP_BETWEEN_CALLS_MAX))
    resp = groq_client.chat.completions.create(
        model=GROQ_MODEL,
        messages=[
            {"role": "system", "content": "You must output STRICT JSON only. No markdown, no prose."},
            {"role": "user", "content": prompt},
        ],
        temperature=temperature,
    )
    return (resp.choices[0].message.content or "").strip()

def call_groq_with_retries(prompt: str, temperature: float = TEMPERATURE) -> str:
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            return call_groq(prompt, temperature=temperature)
        except Exception as e:
            last_err = e
            if _is_429(e):
                # backoff harder on 429
                sleep_s = min(20.0, 1.8 ** attempt + random.uniform(0.5, 1.5))
                print(f"[Groq] 429 TooManyRequests -> backoff {sleep_s:.1f}s (attempt {attempt}/{MAX_RETRIES})")
                time.sleep(sleep_s)
                continue
            sleep_s = min(12.0, 1.5 ** attempt + random.uniform(0.3, 1.0))
            print(f"[Groq] error -> sleep {sleep_s:.1f}s (attempt {attempt}/{MAX_RETRIES}) : {e}")
            time.sleep(sleep_s)

    # If still failing, raise (caller can catch)
    raise last_err

# ============================================================================================================
# LLM: Batch tagging & impact
# ============================================================================================================

def diversify_bullets(bullets: list[str]) -> list[str]:
    # simple dedupe by lowercase
    seen = set()
    out = []
    for b in bullets:
        k = re.sub(r"\s+", " ", b.strip().lower())
        if not k or k in seen:
            continue
        seen.add(k)
        out.append(b.strip())
    return out

def has_meaningful_impact(bullets: list[str]) -> bool:
    if not bullets:
        return False
    joined = " ".join(bullets).lower()
    bad = ["no impact", "none", "unclear", "not relevant", "n/a"]
    return not any(x in joined for x in bad)

def groq_batch_tag_and_filter(news_list: list[dict], chunk_size: int = BATCH_SIZE) -> list[dict]:
    """
    1 request ‡∏ï‡πà‡∏≠ chunk (‡πÄ‡∏ä‡πà‡∏ô 10 ‡∏Ç‡πà‡∏≤‡∏ß) -> ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô requests ‡πÅ‡∏ö‡∏ö‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•
    """
    results: list[dict] = []
    for i in range(0, len(news_list), chunk_size):
        chunk = news_list[i:i + chunk_size]

        payload = []
        for idx, n in enumerate(chunk):
            payload.append({
                "id": idx,
                "feed_country": (n.get("feed_country") or "").strip(),
                "title": n.get("title", ""),
                "summary": n.get("summary", ""),
            })

        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à/‡∏ú‡∏•‡∏¥‡∏ï

‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤‡πÄ‡∏Ç‡πâ‡∏°:
- ‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏î‡πâ‡∏≤‡∏ô: ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê/‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö/‡∏†‡∏≤‡∏©‡∏µ/PSC, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢, ‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å‡∏™‡πå/‡∏ó‡πà‡∏≤‡πÄ‡∏£‡∏∑‡∏≠, ‡∏Ñ‡∏ß‡πà‡∏≥‡∏ö‡∏≤‡∏ï‡∏£, ‡∏Ñ‡πà‡∏≤‡πÄ‡∏á‡∏¥‡∏ô/‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏∏‡∏ô, ‡πÄ‡∏´‡∏ï‡∏∏‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ã‡∏±‡∏û‡∏û‡∏•‡∏≤‡∏¢‡πÄ‡∏ä‡∏ô
- "‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® feed_country ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á
- ‡∏ñ‡πâ‡∏≤ is_relevant=true:
  - country ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö feed_country ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (strict)
  - projects ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 0‚Äì2 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏±‡πâ‡∏ô (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏´‡πâ ["ALL"])
  - evidence 1‚Äì2 ‡∏ß‡∏•‡∏µ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏ö‡πÉ‡∏ô title/summary (‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°)
  - impact_bullets 2‚Äì4 bullets ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ "‡∏Å‡∏•‡πÑ‡∏Å‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö" ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡πÄ‡∏ä‡πà‡∏ô ‡πÉ‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï/‡∏†‡∏≤‡∏©‡∏µ-PSC/‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å‡∏™‡πå/‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡πÄ‡∏´‡∏°‡∏≤/‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô/FX/‡∏®‡∏∏‡∏•‡∏Å‡∏≤‡∏Å‡∏£/‡∏Ñ‡∏ß‡πà‡∏≥‡∏ö‡∏≤‡∏ï‡∏£/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
  - why_relevant ‡∏™‡∏±‡πâ‡∏ô ‡πÜ 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ

‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
{{
  "items": [
    {{
      "id": 0,
      "is_relevant": true/false,
      "country": "Thailand",
      "projects": ["ALL"] or ["..."],
      "impact_bullets": ["...","..."],
      "impact_level": "low|medium|high|unknown",
      "evidence": ["...","..."],
      "why_relevant": "..."
    }}
  ]
}}

‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ:
{json.dumps(payload, ensure_ascii=False)}
"""
        text = call_groq_with_retries(prompt, temperature=TEMPERATURE)
        data = _extract_json_object(text)

        if not (isinstance(data, dict) and isinstance(data.get("items"), list)):
            # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô chunk ‡∏ô‡∏µ‡πâ (‡πÑ‡∏°‡πà‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏á‡∏≤‡∏ô)
            for _ in chunk:
                results.append({"is_relevant": False})
            continue

        by_id = {}
        for it in data["items"]:
            if isinstance(it, dict) and "id" in it:
                by_id[it.get("id")] = it

        for idx, _n in enumerate(chunk):
            results.append(by_id.get(idx, {"is_relevant": False}))

    return results

# ============================================================================================================
# RSS FETCH
# ============================================================================================================

def fetch_rss() -> list[dict]:
    items: list[dict] = []
    cutoff = now_bkk() - timedelta(hours=LOOKBACK_HOURS)

    for src in RSS_SOURCES:
        url = src["url"]
        name = src.get("name", "RSS")
        feed_country = src.get("feed_country", "Global")

        try:
            d = feedparser.parse(url)
        except Exception as e:
            print(f"[RSS] failed: {name} -> {e}")
            continue

        for e in (d.entries or []):
            link = normalize_url(getattr(e, "link", "") or "")
            if not link:
                continue

            dt = parse_entry_datetime(e)
            if dt is None:
                dt = now_bkk()

            if dt < cutoff:
                continue

            title = (getattr(e, "title", "") or "").strip()
            summary = (getattr(e, "summary", "") or "").strip()

            if not title and not summary:
                continue

            # window filter based on entry time (optional)
            if not within_time_window(dt):
                continue

            items.append({
                "source": name,
                "feed_country": feed_country,
                "title": title,
                "summary": summary[:1200],  # ‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏ß
                "link": link,
                "published_at": dt.isoformat(),
            })

    # dedupe by normalized link
    seen = set()
    deduped = []
    for n in items:
        if n["link"] in seen:
            continue
        seen.add(n["link"])
        deduped.append(n)
    return deduped

# ============================================================================================================
# LINE SEND
# ============================================================================================================

def line_broadcast_text(messages: list[dict]):
    if DRY_RUN:
        print("[DRY_RUN] LINE broadcast payload:")
        print(json.dumps({"messages": messages}, ensure_ascii=False, indent=2))
        return

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}",
    }
    payload = {"messages": messages}
    r = requests.post(LINE_BROADCAST_ENDPOINT, headers=headers, json=payload, timeout=30)
    if r.status_code >= 300:
        raise RuntimeError(f"LINE error {r.status_code}: {r.text}")

def build_line_messages(final_items: list[dict]) -> list[dict]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö Text (‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏∏‡∏î/‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏™‡∏∏‡∏î)
    ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡∏™‡πà‡∏á Flex Message ‡πÄ‡∏î‡∏¥‡∏° ‡∏ö‡∏≠‡∏Å‡πÑ‡∏î‡πâ ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ú‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ
    """
    messages = []
    for n in final_items:
        country = n.get("country") or n.get("feed_country") or "-"
        projects = n.get("projects") or ["ALL"]
        level = n.get("impact_level", "unknown")
        evidence = n.get("evidence") or []
        bullets = n.get("impact_bullets") or []

        text_lines = []
        text_lines.append(f"üóûÔ∏è {n.get('title','').strip()}")
        text_lines.append(f"üåç Country: {country}")
        text_lines.append(f"üß© Project: {', '.join(projects[:6])}")
        text_lines.append(f"‚ö†Ô∏è Impact Level: {level}")
        if evidence:
            text_lines.append(f"üîé Evidence: {', '.join(evidence[:2])}")
        if bullets:
            text_lines.append("üìå Impacts:")
            for b in bullets[:6]:
                text_lines.append(f"- {b}")
        text_lines.append(f"üîó {n.get('link','')}")
        messages.append({"type": "text", "text": "\n".join(text_lines)})

    # LINE broadcast ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠ request (‡∏õ‡∏Å‡∏ï‡∏¥ <= 5 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°/‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    return messages

# ============================================================================================================
# MAIN
# ============================================================================================================

def main():
    print("‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å RSS ...")
    news = fetch_rss()
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏•‡∏±‡∏á dedupe+‡πÄ‡∏ß‡∏•‡∏≤: {len(news)}")

    # ‡∏ï‡∏±‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏≠‡∏Å
    fresh = []
    for n in news:
        if was_sent(n["link"]):
            continue
        fresh.append(n)

    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á): {len(fresh)}")
    if not fresh:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà")
        return

    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM ‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô
    selected = fresh[:MAX_ITEMS_TO_LLM]
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM: {len(selected)} (MAX_ITEMS_TO_LLM={MAX_ITEMS_TO_LLM})")

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡πÅ‡∏ö‡∏ö batch
    final_items: list[dict] = []
    try:
        tags = groq_batch_tag_and_filter(selected, chunk_size=BATCH_SIZE)
    except Exception as e:
        # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏°/429 ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏á‡∏≤‡∏ô
        if _is_429(e):
            print("Groq 429: ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤/‡πÄ‡∏£‡∏ó‡∏•‡∏¥‡∏°‡∏¥‡∏ï‡πÄ‡∏ï‡πá‡∏° -> ‡∏à‡∏∞‡∏á‡∏î LLM ‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ")
            tags = [{"is_relevant": False} for _ in selected]
        else:
            raise

    for n, tag in zip(selected, tags):
        if not isinstance(tag, dict) or not tag.get("is_relevant"):
            continue

        feed_country = (n.get("feed_country") or "").strip()
        country = (tag.get("country") or "").strip()

        # strict: country must match feed_country
        if country != feed_country:
            continue

        projects = tag.get("projects") or ["ALL"]
        if projects == ["ALL"]:
            cp = projects_for_country(country)
            if cp:
                projects = cp

        bullets = tag.get("impact_bullets") or []
        if not isinstance(bullets, list):
            bullets = [str(bullets)]
        bullets = diversify_bullets([str(x).strip() for x in bullets if str(x).strip()])

        if not has_meaningful_impact(bullets):
            continue

        n2 = dict(n)
        n2["country"] = country
        n2["projects"] = projects[:6] if isinstance(projects, list) else ["ALL"]
        n2["impact_level"] = tag.get("impact_level", "unknown")
        n2["evidence"] = (tag.get("evidence") or [])[:2]
        n2["why_relevant"] = (tag.get("why_relevant") or "").strip()
        n2["impact_bullets"] = bullets[:6]
        final_items.append(n2)

    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô LLM: {len(final_items)}")
    if not final_items:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå")
        return

    # ‡∏™‡πà‡∏á LINE (‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡πÉ‡∏´‡πâ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î ‡πÜ)
    messages = build_line_messages(final_items)

    # LINE broadcast ‡∏õ‡∏Å‡∏ï‡∏¥‡∏™‡πà‡∏á‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢ message ‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡∏±‡∏ß‡∏£‡πå ‡πÅ‡∏ö‡πà‡∏á‡∏ó‡∏µ‡∏•‡∏∞ 5
    CHUNK = 5
    for i in range(0, len(messages), CHUNK):
        batch = messages[i:i + CHUNK]
        line_broadcast_text(batch)

    # mark sent ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    for n in final_items:
        mark_sent(n["link"])

    print("‡∏™‡πà‡∏á LINE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å sent_links ‡πÅ‡∏•‡πâ‡∏ß")

if __name__ == "__main__":
    main()
