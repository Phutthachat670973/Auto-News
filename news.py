# -*- coding: utf-8 -*-
"""
‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡πà‡∏ß‡∏á 21:00 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô ‚Üí 06:00 ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ
‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Upstream Business Group Subsidiary Management Department"
‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ Gemini (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON) ‚Üí ‡∏™‡∏£‡∏∏‡∏õ/‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô/‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≤‡∏¢ Upstream
‡∏™‡∏£‡πâ‡∏≤‡∏á Flex Message ‡πÅ‡∏•‡∏∞ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) Broadcast ‡πÑ‡∏õ LINE OA

‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:
- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å candidates.parts ‡πÄ‡∏°‡∏∑‡πà‡∏≠ resp.text ‡∏ß‡πà‡∏≤‡∏á
- ‡∏ã‡πà‡∏≠‡∏°/‡∏Å‡∏π‡πâ JSON ‡∏ó‡∏µ‡πà‡πÇ‡∏î‡∏ô‡∏ï‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ö‡∏ö‡∏ô‡∏∏‡πà‡∏°‡∏ô‡∏ß‡∏• (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏°‡∏¥‡∏ô‡∏¥‡∏ü‡∏≤‡∏¢)
- ‡πÄ‡∏û‡∏¥‡πà‡∏° max_output_tokens ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ + retry ‡∏î‡πâ‡∏ß‡∏¢ prompt ‡∏ó‡∏µ‡πà ‚Äú‡∏ï‡∏±‡∏î detail‚Äù ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
- ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß detail ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM (~3500‚Äì4000 chars) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô finish_reason=2 ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å
"""

import os, re, json, time, random
from datetime import datetime, timedelta
import feedparser
from dateutil import parser as dateutil_parser
import pytz
from newspaper import Article
import requests
import google.generativeai as genai

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ========================= CONFIG =========================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not GEMINI_API_KEY: raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Environment/Secrets")
if not LINE_CHANNEL_ACCESS_TOKEN: raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_CHANNEL_ACCESS_TOKEN ‡πÉ‡∏ô Environment/Secrets")

genai.configure(api_key=GEMINI_API_KEY)
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-2.5-flash").strip() or "gemini-2.5-flash"
model = genai.GenerativeModel(GEMINI_MODEL_NAME)

GEMINI_DAILY_BUDGET = int(os.getenv("GEMINI_DAILY_BUDGET", "250"))
MAX_RETRIES = 6
SLEEP_BETWEEN_CALLS = (6.0, 7.0)
DRY_RUN = os.getenv("DRY_RUN", "false").lower() == "true"

bangkok_tz = pytz.timezone("Asia/Bangkok")
now = datetime.now(bangkok_tz)

SENT_LINKS_DIR = "sent_links"
os.makedirs(SENT_LINKS_DIR, exist_ok=True)

def get_sent_links_file(date=None):
    if date is None:
        date = datetime.now(bangkok_tz).strftime("%Y-%m-%d")
    return os.path.join(SENT_LINKS_DIR, f"{date}.txt")

def load_sent_links_today_yesterday():
    sent_links = set()
    for i in range(2):
        date = (now - timedelta(days=i)).strftime("%Y-%m-%d")
        path = get_sent_links_file(date)
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    url = line.strip()
                    if url:
                        sent_links.add(url)
    return sent_links

def save_sent_links(new_links, date=None):
    path = get_sent_links_file(date)
    with open(path, "a", encoding="utf-8") as f):
        for url in new_links:
            f.write(url.strip() + "\n")

# ========================= FEEDS =========================
news_sources = {
    "Oilprice": {"type": "rss", "url": "https://oilprice.com/rss/main", "category": "Energy", "site": "Oilprice"},
    "CleanTechnica": {"type": "rss", "url": "https://cleantechnica.com/feed/", "category": "Energy", "site": "CleanTechnica"},
    "HydrogenFuelNews": {"type": "rss", "url": "https://www.hydrogenfuelnews.com/feed/", "category": "Energy", "site": "Hydrogen Fuel News"},
    "Economist-Latest": {"type": "rss", "url": "https://www.economist.com/latest/rss.xml", "category": "Economy", "site": "Economist"},
    "YahooFinance-News": {"type": "rss", "url": "https://finance.yahoo.com/news/rssindex", "category": "Economy", "site": "Yahoo Finance"},
}

DEFAULT_ICON_URL = "https://scdn.line-apps.com/n/channel_devcenter/img/fx/01_1_cafe.png"
UA = {"User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36"}
GEMINI_CALLS = 0

# ========================= CONTEXT (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô) =========================
UPSTREAM_SUBSIDIARY_CONTEXT = """
[‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô]
‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô "Upstream Business Group Subsidiary Management Department" 
‡∏ã‡∏∂‡πà‡∏á‡∏î‡∏π‡πÅ‡∏•/‡∏Å‡∏≥‡∏Å‡∏±‡∏ö "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢" ‡πÉ‡∏ô‡∏™‡∏≤‡∏¢‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏™‡∏≥‡∏£‡∏ß‡∏à‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏° (Upstream: oil & gas E&P) 
‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô/‡∏ò‡∏£‡∏£‡∏°‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏•‡∏∏‡πà‡∏° Upstream

[‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ]
- ‡∏î‡∏µ‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ã‡∏∑‡πâ‡∏≠/‡∏Ç‡∏≤‡∏¢/‡∏£‡πà‡∏ß‡∏°‡∏ó‡∏∏‡∏ô (M&A/JV) ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≤‡∏¢ Upstream ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÉ‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ú‡∏•‡∏¥‡∏ï
- ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö/‡∏û‡∏±‡∏í‡∏ô‡∏≤/‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏¥‡∏ï/‡∏´‡∏¢‡∏∏‡∏î‡∏ú‡∏•‡∏¥‡∏ï ‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô/‡∏Å‡πä‡∏≤‡∏ã‡∏ó‡∏µ‡πà‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å‡∏ñ‡∏∑‡∏≠‡∏™‡∏¥‡∏ó‡∏ò‡∏¥
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏™‡∏£‡∏£‡∏´‡∏≤/‡∏ò‡∏£‡∏£‡∏°‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•/‡∏Å‡∏é‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å
- ‡∏†‡∏π‡∏°‡∏¥‡∏£‡∏±‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®/‡∏†‡∏≤‡∏©‡∏µ/‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô/PSC ‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å
- ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô/‡∏Å‡πä‡∏≤‡∏ã/JKM/Brent/WTI ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡πÑ‡∏Å‡∏ï‡πà‡∏≠‡∏û‡∏≠‡∏£‡πå‡∏ï upstream ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å
- ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå supply disruption ‡∏ó‡πà‡∏≠/‡πÅ‡∏ó‡πà‡∏ô/‡πÇ‡∏£‡∏á‡πÅ‡∏¢‡∏Å/‡∏ó‡πà‡∏≤‡πÄ‡∏£‡∏∑‡∏≠ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï/‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å
- ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£/‡∏á‡∏ö‡∏•‡∏á‡∏ó‡∏∏‡∏ô/re-phasing ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ upstream ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å

[‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà "‡πÑ‡∏°‡πà" ‡πÇ‡∏ü‡∏Å‡∏±‡∏™]
- ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏•‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å upstream
- ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ/‡πÄ‡∏°‡∏Å‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡πÑ‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢ upstream
"""

# ========================= HELPERS =========================
def _truncate(s: str, n: int) -> str:  # [PATCH: minimal-impact]
    if not s: return ""
    s = re.sub(r"\s{2,}", " ", s).strip()
    return (s[:n-1] + "‚Ä¶") if len(s) > n else s

def _safe_resp_text(resp) -> str:  # [PATCH: minimal-impact]
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å response.text; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å candidates.parts"""
    try:
        txt = getattr(resp, "text", None)
        if txt: return txt
    except Exception:
        pass
    parts = []
    try:
        for c in getattr(resp, "candidates", []) or []:
            if getattr(c, "content", None):
                for p in getattr(c.content, "parts", []) or []:
                    t = getattr(p, "text", None)
                    if t: parts.append(t)
    except Exception:
        return ""
    return "\n".join(parts).strip()

def _replace_smart_quotes(s: str) -> str:  # [PATCH: minimal-impact]
    if not s: return s
    trans = {
        '\u201c':'"', '\u201d':'"', '\u201e':'"', '\u201f':'"',
        '\u2018':"'", '\u2019':"'", '\u201a':"'", '\u201b':"'",
        '\u00A0':" ", '\u200B':"", '\u200C':"", '\u200D':"", '\uFEFF':""
    }
    for k,v in trans.items():
        s = s.replace(k,v)
    return s

def _strip_code_fences(s: str) -> str:  # [PATCH: minimal-impact]
    if not s: return s
    s = re.sub(r"^```(?:json)?\s*", "", s.strip(), flags=re.I)
    s = re.sub(r"\s*```$", "", s.strip(), flags=re.I)
    return s

def _balanced_json_substring(s: str) -> str | None:  # [PATCH: minimal-impact]
    """‡∏î‡∏∂‡∏á JSON substring ‡∏ó‡∏µ‡πà‡∏õ‡∏¥‡∏î‡∏ß‡∏á‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤‡πÑ‡∏î‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏• (‡∏ä‡πà‡∏ß‡∏¢‡∏ã‡πà‡∏≠‡∏°‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏î‡∏ô‡∏ï‡∏±‡∏î)"""
    if not s: return None
    start = s.find('{')
    if start == -1: return None
    i = start; depth = 0; in_str = False; esc = False
    while i < len(s):
        ch = s[i]
        if in_str:
            if esc: esc = False
            elif ch == '\\': esc = True
            elif ch == '"': in_str = False
        else:
            if ch == '"': in_str = True
            elif ch == '{': depth += 1
            elif ch == '}':
                depth -= 1
                if depth == 0:
                    return s[start:i+1]
        i += 1
    if depth > 0:
        return s[start:] + ("}" * depth)
    return None

def _extract_json_robust(text: str) -> dict | None:  # [PATCH: minimal-impact]
    if not text: return None
    text = _replace_smart_quotes(_strip_code_fences(text))
    cand = _balanced_json_substring(text)
    if not cand: return None
    cand = re.sub(r",\s*([}\]])", r"\1", cand)  # ‡∏ï‡∏±‡∏î comma ‡∏ó‡πâ‡∏≤‡∏¢
    try:
        return json.loads(cand)
    except Exception:
        return None

# ========================= FETCHERS =========================
def fetch_news_9pm_to_6am(days_back=1):
    now = datetime.now(bangkok_tz)
    start_time = (now - timedelta(days=days_back)).replace(hour=21, minute=0, second=0, microsecond=0)
    end_time   = now.replace(hour=6, minute=0, second=0, microsecond=0)
    if end_time < start_time: end_time += timedelta(days=1)
    print("‡∏ä‡πà‡∏ß‡∏á fetch:", start_time, "‡∏ñ‡∏∂‡∏á", end_time)
    all_news = []
    for _, info in news_sources.items():
        try:
            feed = feedparser.parse(info["url"])
            for entry in feed.entries:
                pub_str = getattr(entry, "published", None) or getattr(entry, "updated", None)
                if not pub_str: continue
                pub_dt = dateutil_parser.parse(pub_str).astimezone(bangkok_tz)
                if not (start_time <= pub_dt <= end_time): continue
                all_news.append({
                    "site": info["site"], "category": info["category"],
                    "title": getattr(entry, "title", "-"),
                    "summary": getattr(entry, "summary", "-"),
                    "link": getattr(entry, "link", ""),
                    "published": pub_dt,
                    "date": pub_dt.strftime("%d/%m/%Y %H:%M")
                })
        except Exception as e:
            print(f"[WARN] ‡∏≠‡πà‡∏≤‡∏ô‡∏ü‡∏µ‡∏î {info['site']} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
    print("‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á:", len(all_news))
    return all_news

def fetch_article_detail_and_image(url, timeout=15):
    # 1) newspaper3k ‡∏Å‡πà‡∏≠‡∏ô
    try:
        art = Article(url); art.download(); art.parse()
        text = (art.text or "").strip()
        img  = (art.top_image or "").strip()
        if text or img: return text, img
    except Exception: pass
    # 2) fallback: requests + meta
    try:
        r = requests.get(url, headers=UA, timeout=timeout)
        r.raise_for_status()
        html = r.text
        import lxml.html as LH
        doc = LH.fromstring(html)
        paragraphs = [p.text_content().strip() for p in doc.xpath("//p") if p.text_content()]
        text2 = "\n".join(paragraphs[:60]).strip()  # ‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏£‡∏á
        og = doc.xpath("//meta[@property='og:image']/@content") or doc.xpath("//meta[@name='twitter:image']/@content")
        img2 = (og[0].strip() if og else "")
        return text2, img2
    except Exception:
        return "", ""

# ========================= GEMINI WRAPPER =========================
# [PATCH: minimal-impact] ‚Äî ‡πÄ‡∏û‡∏¥‡πà‡∏° max_output_tokens ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡πà‡∏≤‡∏ô text ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
GENCFG = genai.GenerationConfig(
    temperature=0.35,
    max_output_tokens=1024,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ JSON ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏£‡∏ö
    response_mime_type="application/json"  # ‡∏¢‡∏±‡∏á‡∏Ç‡∏≠ JSON ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏¥‡∏°
)

GEMINI_CALLS_FILE = os.path.join(SENT_LINKS_DIR, f"gemini_calls_{now.strftime('%Y-%m-%d')}.txt")
def _load_calls():
    try: return int(open(GEMINI_CALLS_FILE,"r",encoding="utf-8").read().strip())
    except Exception: return 0
def _save_calls(n):
    try: open(GEMINI_CALLS_FILE,"w",encoding="utf-8").write(str(n))
    except Exception: pass
GEMINI_CALLS = _load_calls()

def _call_and_parse_json(prompt) -> dict:  # [PATCH: minimal-impact]
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Å‡∏π‡πâ JSON ‡∏à‡∏≤‡∏Å text/parts"""
    resp = model.generate_content(prompt, generation_config=GENCFG)
    raw = _safe_resp_text(resp)
    out = _extract_json_robust(raw)
    if out is not None:
        return out
    # ‡πÉ‡∏™‡πà‡∏î‡∏µ‡∏ö‡∏±‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (‡πÑ‡∏°‡πà‡πÇ‡∏¢‡∏ô raw ‡∏¢‡∏≤‡∏ß ‡πÜ)
    fr = None
    try:
        fr = getattr(resp.candidates[0], "finish_reason", None)
    except Exception:
        pass
    raise RuntimeError(f"Gemini ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á JSON ‡∏ó‡∏µ‡πà parse ‡πÑ‡∏î‡πâ (finish_reason={fr})")

def call_gemini_json(prompt, max_retries=MAX_RETRIES):  # [PATCH: minimal-impact]
    global GEMINI_CALLS
    if GEMINI_CALLS >= GEMINI_DAILY_BUDGET:
        raise RuntimeError(f"‡∏ñ‡∏∂‡∏á‡∏á‡∏ö Gemini ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß ({GEMINI_CALLS}/{GEMINI_DAILY_BUDGET})")
    last_error = None
    for attempt in range(1, max_retries+1):
        try:
            out = _call_and_parse_json(prompt)
            GEMINI_CALLS += 1
            _save_calls(GEMINI_CALLS)
            return out
        except Exception as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(4 * attempt)
                continue
            raise last_error

# ========================= PROMPTS (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô) =========================
def llm_is_relevant_for_department(news):
    # [PATCH: minimal-impact] ‡∏ï‡∏±‡∏î detail ‡πÅ‡∏Ñ‡∏õ (~3500) ‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏ô‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡∏ï‡πå‡∏û‡∏∏‡∏ï ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏™‡∏≤‡∏£‡∏∞
    title   = _truncate(news['title'], 300)
    summary = _truncate(news.get('summary',''), 800)
    detail  = _truncate(news.get('detail',''), 3800)

    prompt = f"""
{UPSTREAM_SUBSIDIARY_CONTEXT}

‡∏à‡∏á‡∏ï‡∏≠‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏≤‡∏° schema:
{{"relevant": true|false}}

‡∏Ç‡πà‡∏≤‡∏ß:
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {title}
‡∏™‡∏£‡∏∏‡∏õ: {summary}
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {detail}
"""
    try:
        if DRY_RUN:
            s = (title + " " + summary).lower()
            keys = ["e&p","exploration","production","oil","gas","brent","wti","lng","field","rig","psc","concession","m&a","acquisition","portfolio","subsidiary","joint venture","supply"]
            return any(k in s for k in keys)
        out = call_gemini_json(prompt)
        return bool(out.get("relevant", False))
    except Exception as e:
        print("[ERROR] LLM Filter:", e)
        return False

def llm_summary_for_department(news):
    """‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß + ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô + ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (JSON) ‚Äî fallback ‡πÅ‡∏ö‡∏ö '‡∏•‡∏î detail' ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    title   = _truncate(news['title'], 300)
    summary = _truncate(news.get('summary',''), 800)
    detail  = _truncate(news.get('detail',''), 3800)  # ‡∏™‡πà‡∏á‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏Ñ‡∏ß‡∏£ (‡πÑ‡∏°‡πà‡∏´‡∏±‡πà‡∏ô‡∏™‡∏≤‡∏£‡∏∞)

    def _make_prompt(_title, _summary, _detail):
        return f"""
{UPSTREAM_SUBSIDIARY_CONTEXT}

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
{{
  "summary": "‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πâ‡∏ô 1‚Äì2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏´‡∏•‡∏±‡∏Å+‡∏Å‡∏•‡πÑ‡∏Å (‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Å‡∏ß‡πâ‡∏≤‡∏á)",
  "importance": 1,
  "importance_reasons": ["‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πâ‡∏ô‡πÜ 1-3 ‡∏Ç‡πâ‡∏≠ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏•‡πÑ‡∏Å‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à"],
  "department_relevance": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢ upstream ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ (‡∏î‡∏µ‡∏•/‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á/‡∏û‡∏≠‡∏£‡πå‡∏ï/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô)",
  "tags": ["‡πÄ‡∏ä‡πà‡∏ô upstream","m&a","psc","geo-risk","price","supply"]
}}

‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
- importance ‚àà [1,5] ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON (‡∏´‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô/‡πÇ‡∏Ñ‡πâ‡∏î‡∏ö‡∏•‡πá‡∏≠‡∏Å)

‡∏Ç‡πà‡∏≤‡∏ß:
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {_title}
‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡πà‡∏≠: {_summary}
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ): {_detail}
"""

    if DRY_RUN:
        s = (summary or title)
        return {
            "summary": _truncate(s, 240),
            "importance": 4,
            "importance_reasons": ["‡∏Ç‡πà‡∏≤‡∏ß upstream ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å", "‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏û‡∏≠‡∏£‡πå‡∏ï/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á"],
            "department_relevance": "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢ upstream (‡∏û‡∏≠‡∏£‡πå‡∏ï/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô)",
            "tags": ["upstream","portfolio"]
        }

    # ‡∏£‡∏≠‡∏ö 1: ‡∏™‡πà‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° detail ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠ (‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏Ñ‡∏ß‡∏£)
    try:
        return call_gemini_json(_make_prompt(title, summary, detail))
    except Exception as e1:
        print("Analyze warn (full detail):", e1)

    # ‡∏£‡∏≠‡∏ö 2: ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î detail ‡∏≠‡∏≠‡∏Å (‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Ç‡πà‡∏≤‡∏ß '‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î' ‡πÅ‡∏ï‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏¥‡∏î JSON)
    try:
        return call_gemini_json(_make_prompt(title, summary, ""))
    except Exception as e2:
        print("Analyze warn (no detail):", e2)

    # ‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: fallback‚Äî‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡πÄ‡∏î‡∏¥‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ pipeline ‡πÑ‡∏õ‡∏ï‡πà‡∏≠
    return {
        "summary": _truncate(summary or title, 240),
        "importance": 3,
        "importance_reasons": ["Fallback: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏´‡∏£‡∏∑‡∏≠ JSON ‡∏û‡∏±‡∏á"],
        "department_relevance": "Fallback: ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠",
        "tags": ["fallback"]
    }

# ========================= RANK / FLEX =========================
def rank_candidates(news_list):
    ranked = []
    for n in news_list:
        age_h = (now - n["published"]).total_seconds() / 3600.0
        recency = max(0.0, (72.0 - min(72.0, age_h))) / 72.0 * 3.0
        cat_w = {"Energy": 3.0, "Economy": 2.0}.get(n["category"], 1.0)
        length = min(len(n.get("summary","")) / 500.0, 1.0)
        score = recency + cat_w + length
        ranked.append((score, n))
    ranked.sort(key=lambda x: x[0], reverse=True)
    return [n for _, n in ranked]

def create_flex_message(news_items):
    now_thai = datetime.now(bangkok_tz).strftime("%d/%m/%Y")
    bubbles = []
    for item in news_items:
        title = (item.get("title","-"))  # ‡πÑ‡∏°‡πà‡∏´‡∏±‡πà‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        summary_txt = (item.get("dept_summary") or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß")
        rel_txt = (item.get("dept_relevance") or "-")
        reasons = item.get("dept_reasons") or []
        score_line = f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô: {item.get('dept_importance','-')}"
        bd_clean = "\n".join([f"- {r}" for r in reasons]) if reasons else "-"

        img = item.get("image") or DEFAULT_ICON_URL
        if not (str(img).startswith("http://") or str(img).startswith("https://")):
            img = DEFAULT_ICON_URL

        body_contents = [
            {"type":"text","text": title,"weight":"bold","size":"lg","wrap":True,"color":"#111111"},
            {"type":"box","layout":"horizontal","margin":"sm","contents":[
                {"type":"text","text": f"üóì {item.get('date','-')}", "size":"xs","color":"#aaaaaa","flex":5},
                {"type":"text","text": f"üìå {item.get('category','')}", "size":"xs","color":"#888888","align":"end","flex":5}
            ]},
            {"type":"text","text": f"üåç {item.get('site','')}", "size":"xs","color":"#448AFF","margin":"sm"},
            {"type":"text","text": summary_txt,"size":"md","wrap":True,"margin":"md","color":"#1A237E","weight":"bold"},
            {"type":"box","layout":"vertical","margin":"lg","contents":[
                {"type":"text","text":"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Upstream Subsidiary Management","weight":"bold","size":"lg","color":"#D32F2F"},
                {"type":"text","text": rel_txt,"size":"md","wrap":True,"color":"#C62828","weight":"bold"},
                {"type":"text","text": score_line,"size":"lg","wrap":True,"color":"#000000","weight":"bold"},
                {"type":"text","text": bd_clean,"size":"sm","wrap":True,"color":"#8E0000","weight":"bold"}
            ]}
        ]

        bubbles.append({
            "type":"bubble","size":"mega",
            "hero":{"type":"image","url":img,"size":"full","aspectRatio":"16:9","aspectMode":"cover"},
            "body":{"type":"box","layout":"vertical","spacing":"md","contents": body_contents},
            "footer":{
                "type":"box","layout":"vertical","spacing":"sm",
                "contents":[
                    {"type":"text","text":"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å","size":"xs","color":"#FF0000","wrap":True,"margin":"md"},
                    {"type":"button","style":"primary","color":"#1DB446","action":{"type":"uri","label":"‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠","uri": item.get("link","#")}}
                ]
            }
        })

    carousels = []
    for i in range(0, len(bubbles), 10):
        carousels.append({
            "type":"flex",
            "altText": f"‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Upstream Subsidiary Mgmt {now_thai}",
            "contents":{"type":"carousel","contents": bubbles[i:i+10]}
        })
    return carousels

def broadcast_flex_message(access_token, flex_carousels):
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {"Content-Type":"application/json","Authorization": f"Bearer {access_token}"}
    for idx, carousel in enumerate(flex_carousels, 1):
        payload = {"messages":[carousel]}
        if DRY_RUN:
            print(f"[DRY_RUN] ‡∏à‡∏∞‡∏™‡πà‡∏á Carousel #{idx}: {json.dumps(payload)[:500]}...")
            continue
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        print(f"Broadcast #{idx} status:", resp.status_code, getattr(resp, "text", ""))
        if resp.status_code >= 300:
            print("LINE Error:", resp.status_code, resp.text[:500]); break

# ========================= MAIN =========================
def main():
    all_news = fetch_news_9pm_to_6am()
    print(f"‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡πà‡∏ß‡∏á 21:00 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô ‡∏ñ‡∏∂‡∏á 06:00 ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ: {len(all_news)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    if not all_news:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß"); return

    SLEEP_MIN, SLEEP_MAX = SLEEP_BETWEEN_CALLS

    # -------- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î/‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á --------
    filtered_news = []
    for news in all_news:
        if len(news.get('summary','')) < 50:
            txt, _ = fetch_article_detail_and_image(news['link'])
            # [PATCH: minimal-impact] ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ï‡∏±‡∏î ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏≤‡∏£‡∏∞‡πÑ‡∏î‡πâ‡∏î‡∏µ
            news['detail'] = _truncate((txt or "").strip() or news['title'], 3800)
        else:
            news['detail'] = ""

        if llm_is_relevant_for_department(news):
            filtered_news.append(news)
        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))

    print(f"‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô: {len(filtered_news)} ‡∏Ç‡πà‡∏≤‡∏ß")
    if not filtered_news:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"); return

    # -------- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô --------
    ranked = rank_candidates(filtered_news)
    top_candidates = ranked[:min(10, len(ranked))]
    print(f"‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏µ‡∏¢‡∏á {len(top_candidates)} ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≥‡∏Å‡∏±‡∏î 10)")

    dept_news = []
    for news in top_candidates:
        try:
            out = llm_summary_for_department(news)
        except Exception as e:
            print("Error: Analyze:", e)
            continue

        news['dept_summary']    = out.get('summary','‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß')
        news['dept_importance'] = int(out.get('importance', 3) or 3)
        news['dept_reasons']    = list(out.get('importance_reasons') or [])
        news['dept_relevance']  = out.get('department_relevance','-')
        news['dept_tags']       = out.get('tags') or []

        dept_news.append(news)
        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))

    print(f"‡πÉ‡∏ä‡πâ Gemini ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß: {_load_calls()}/{GEMINI_DAILY_BUDGET} calls")

    if not dept_news:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏á"); return

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà
    dept_news.sort(key=lambda n: (n.get('dept_importance',0), n.get('published', datetime.min)), reverse=True)
    top_news = dept_news[:10]

    # ‡∏Å‡∏±‡∏ô‡∏™‡πà‡∏á‡∏ã‡πâ‡∏≥
    sent_links = load_sent_links_today_yesterday()
    top_news_to_send = [n for n in top_news if n["link"] not in sent_links]
    if not top_news_to_send:
        print("‡∏Ç‡πà‡∏≤‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡∏™‡πà‡∏á‡∏Ñ‡∏£‡∏ö‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà"); return

    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏£‡∏π‡∏õ
    for item in top_news_to_send:
        _, img = fetch_article_detail_and_image(item["link"])
        if not (str(img).startswith("http://") or str(img).startswith("https://")):
            img = DEFAULT_ICON_URL
        item["image"] = img

    # Flex + ‡∏™‡πà‡∏á
    carousels = create_flex_message(top_news_to_send)
    broadcast_flex_message(LINE_CHANNEL_ACCESS_TOKEN, carousels)
    save_sent_links([n["link"] for n in top_news_to_send])
    print("‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ERROR]", e)
