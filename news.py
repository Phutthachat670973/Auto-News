# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote

import requests
import feedparser
import pytz
from dateutil import parser as dateutil_parser

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# =============================================================================
# ENV / CONFIG
# =============================================================================
TZ = pytz.timezone(os.getenv("TZ", "Asia/Bangkok"))

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE_CHANNEL_ACCESS_TOKEN")

# Groq Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant").strip()
GROQ_ENDPOINT = os.getenv("GROQ_ENDPOINT", "https://api.groq.com/openai/v1/chat/completions").strip()
USE_LLM_SUMMARY = os.getenv("USE_LLM_SUMMARY", "1").strip().lower() in ["1", "true", "yes", "y"]

WINDOW_HOURS = int(os.getenv("WINDOW_HOURS", "48"))
MAX_PER_FEED = int(os.getenv("MAX_PER_FEED", "30"))
DRY_RUN = os.getenv("DRY_RUN", "0").strip().lower() in ["1", "true", "yes", "y"]
MAX_MESSAGES_PER_RUN = int(os.getenv("MAX_MESSAGES_PER_RUN", "10"))
BUBBLES_PER_CAROUSEL = int(os.getenv("BUBBLES_PER_CAROUSEL", "10"))

# Sent links tracking
SENT_DIR = os.getenv("SENT_DIR", "sent_links")
os.makedirs(SENT_DIR, exist_ok=True)

# =============================================================================
# PROJECT DATABASE (Enhanced)
# =============================================================================
PROJECTS_BY_COUNTRY = {
    "Thailand": [
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/61", "G 1/61", "G1/61", "‡∏à‡∏µ 1/61"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ G 1/61",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 1
        },
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 2/61", "G 2/61", "G2/61", "‡∏à‡∏µ 2/61"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ G 2/61",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 1
        },
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå", "Arthit", "‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 2
        },
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏™ 1", "S1", "S 1"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ S1",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 2
        },
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4", "Contract 4", "‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4",
            "category": "‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏°",
            "priority": 2
        },
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ‡∏≠‡∏µ‡∏û‡∏µ 1", "PTTEP 1", "PTTEP1"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ PTTEP 1",
            "category": "‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏°",
            "priority": 1
        }
    ],
    "Vietnam": [
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° 16-1", "Vietnam 16-1", "16-1", "Block 16-1"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° 16-1",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 1
        },
        {
            "patterns": ["Block B", "‡∏ö‡∏•‡πá‡∏≠‡∏Å B"],
            "official_name": "Block B",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 2
        }
    ],
    "Myanmar": [
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏ï‡∏¥‡∏Å‡πâ‡∏≤", "Zawtika", "‡∏ã‡∏≠‡∏ï‡∏¥‡∏Å‡πâ‡∏≤"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏ï‡∏¥‡∏Å‡πâ‡∏≤",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 1
        },
        {
            "patterns": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏≤‡∏î‡∏≤‡∏ô‡∏≤", "Yadana", "‡∏¢‡∏≤‡∏î‡∏≤‡∏ô‡∏≤"],
            "official_name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏≤‡∏î‡∏≤‡∏ô‡∏≤",
            "category": "‡∏Å‡πä‡∏≤‡∏ã‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
            "priority": 1
        }
    ]
}

# =============================================================================
# LANGUAGE DETECTOR
# =============================================================================
class LanguageDetector:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    
    @staticmethod
    def detect_language(text: str) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not text:
            return "unknown"
        
        # ‡∏ô‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        thai_pattern = re.compile(r'[‡∏Å-‡∏Æ‡∏∞-‡πå]')
        thai_count = len(thai_pattern.findall(text))
        total_chars = len(re.findall(r'\w', text))
        
        if total_chars == 0:
            return "unknown"
        
        if thai_count / total_chars > 0.3:
            return "th"
        else:
            return "en"
    
    @staticmethod
    def normalize_thai_text(text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if not text:
            return text
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
        text = re.sub(r'\s+', ' ', text)
        
        # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏™‡∏µ‡∏¢
        text = re.sub(r'[‚Ä¢‚ñ™‚ñ∂‚ñ∫‚óè]', '', text)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏™‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ‡∏ñ‡πâ‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å
            if LanguageDetector.detect_language(line) == "th":
                # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
                line = re.sub(r'\([^)]*[A-Za-z]+[^)]*\)', '', line)
                line = re.sub(r'\[[^\]]*[A-Za-z]+[^\]]*\]', '', line)
            
            if line:
                cleaned_lines.append(line)
        
        return ' '.join(cleaned_lines)

# =============================================================================
# NEWS QUALITY SCORER
# =============================================================================
class NewsQualityScorer:
    """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πà‡∏≤‡∏ß"""
    
    QUALITY_TIERS = {
        "high": [
            "reuters.com", "bloomberg.com", "energy.go.th", 
            "ratchakitcha.soc.go.th", "egat.co.th", "pptplc.com"
        ],
        "medium": [
            "thansettakij.com", "prachachat.net", "bangkokbiznews.com",
            "komchadluek.net", "matichon.co.th", "dailynews.co.th"
        ],
        "low": ["google.com", "news.google.com"]
    }
    
    @classmethod
    def get_source_tier(cls, url: str) -> str:
        """‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß"""
        domain = urlparse(url).netloc.lower()
        
        for tier, domains in cls.QUALITY_TIERS.items():
            if any(d in domain for d in domains):
                return tier
        return "unknown"
    
    @classmethod
    def score_news(cls, news_item: dict) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πà‡∏≤‡∏ß (0-1)"""
        url = news_item.get('url', '')
        
        # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        score = 0.5
        
        # ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        source_tier = cls.get_source_tier(url)
        if source_tier == "high":
            score += 0.3
        elif source_tier == "medium":
            score += 0.1
        
        # ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
        if news_item.get('is_official'):
            score += 0.2
        
        # ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
        if news_item.get('has_project_ref'):
            score += 0.15
        
        # ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM
        if news_item.get('llm_analysis'):
            score += 0.1
        
        # ‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if news_item.get('published_dt'):
            score += 0.05
        
        return min(1.0, max(0.0, score))

# =============================================================================
# PROJECT NORMALIZER
# =============================================================================
class ProjectNormalizer:
    """‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£"""
    
    @staticmethod
    def normalize_project_name(name: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"""
        if not name or not isinstance(name, str):
            return ""
        
        name = name.strip()
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
        name = re.sub(r'\s+', ' ', name)
        
        # ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        # ‡πÅ‡∏õ‡∏•‡∏á G1/61 -> G 1/61
        name = re.sub(r'([A-Z])(\d)', r'\1 \2', name)
        
        # ‡πÅ‡∏õ‡∏•‡∏á ‡∏à‡∏µ1/61 -> ‡∏à‡∏µ 1/61
        name = re.sub(r'([‡∏Å-‡∏Æ])(\d)', r'\1 \2', name)
        
        # ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ slash
        name = re.sub(r'(\d)\s*/\s*(\d)', r'\1/\2', name)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£" ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        if not name.startswith('‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£') and LanguageDetector.detect_language(name) == "th":
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
            pattern = re.compile(r'^(‡∏à‡∏µ|‡πÄ‡∏≠‡∏™|‡πÄ‡∏≠|‡∏ö‡∏µ|‡∏ã‡∏µ|‡∏î‡∏µ)\s+\d')
            if pattern.match(name):
                name = f'‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ {name}'
        
        return name
    
    @staticmethod
    def find_matching_projects(text: str, country: str) -> list:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if country not in PROJECTS_BY_COUNTRY:
            return []
        
        matches = []
        text_lower = text.lower()
        
        for project in PROJECTS_BY_COUNTRY[country]:
            for pattern in project["patterns"]:
                pattern_lower = pattern.lower()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                if pattern_lower in text_lower:
                    matches.append({
                        "official_name": project["official_name"],
                        "category": project.get("category", ""),
                        "priority": project.get("priority", 3),
                        "matched_pattern": pattern
                    })
                    break  # ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ pattern ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° priority
        matches.sort(key=lambda x: x["priority"])
        return matches[:3]  # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 3 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£

# =============================================================================
# TIME FORMATTER
# =============================================================================
class TimeFormatter:
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ß‡∏•‡∏≤"""
    
    @staticmethod
    def format_publish_time(published_dt: datetime) -> str:
        """‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£"""
        if not published_dt:
            return "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ß‡∏•‡∏≤"
        
        now = datetime.now(TZ)
        diff = now - published_dt
        
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≤‡∏°‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
        published_dt = published_dt.astimezone(TZ)
        
        if diff.days > 30:
            return f"‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà {published_dt.strftime('%d/%m/%Y')}"
        elif diff.days > 0:
            return f"{diff.days} ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß"
        elif diff.seconds > 3600:
            hours = diff.seconds // 3600
            return f"{hours} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß"
        elif diff.seconds > 60:
            minutes = diff.seconds // 60
            return f"{minutes} ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß"
        else:
            return "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà"
    
    @staticmethod
    def get_time_emoji(published_dt: datetime) -> str:
        """‡πÑ‡∏î‡πâ‡∏≠‡∏¥‡πÇ‡∏°‡∏à‡∏¥‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤"""
        if not published_dt:
            return "üïê"
        
        hour = published_dt.hour
        
        if 5 <= hour < 12:
            return "üåÖ"  # ‡πÄ‡∏ä‡πâ‡∏≤
        elif 12 <= hour < 17:
            return "‚òÄÔ∏è"   # ‡∏ö‡πà‡∏≤‡∏¢
        elif 17 <= hour < 21:
            return "üåá"   # ‡πÄ‡∏¢‡πá‡∏ô
        else:
            return "üåô"   # ‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô

# =============================================================================
# KEYWORD FILTERS (Enhanced)
# =============================================================================
class KeywordFilter:
    # ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    OFFICIAL_SOURCES = [
        'ratchakitcha.soc.go.th', 'energy.go.th', 'egat.co.th', 
        'pptplc.com', 'pttep.com', 'reuters.com', 'bloomberg.com',
        'bangchak.co.th', 'bangkokbiznews.com', 'thansettakij.com',
        'prachachat.net', 'posttoday.com'
    ]
    
    OFFICIAL_KEYWORDS = [
        '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏£‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏ü‡∏ú', '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤',
        '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏Å‡∏û', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        '‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏Ñ‡∏£‡∏°.', '‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤',
        'minister', 'ministry', 'regulation', 'policy', 'tariff', 'approval',
        '‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥', '‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï', '‡πÉ‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô', '‡∏™‡∏±‡∏ç‡∏ç‡∏≤'
    ]
    
    ENERGY_KEYWORDS = [
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü', '‡∏Å‡πä‡∏≤‡∏ã', 'LNG', '‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÄ‡∏û‡∏•‡∏¥‡∏á',
        '‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô', '‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏°', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ä‡∏µ‡∏ß‡∏°‡∏ß‡∏•',
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏™‡∏á‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡πâ‡∏≥', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô',
        'energy', 'electricity', 'power', 'gas', 'oil', 'fuel',
        'power plant', 'renewable', 'solar', 'wind', 'biomass'
    ]
    
    PROJECT_KEYWORDS = [
        '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô', '‡∏ö‡∏•‡πá‡∏≠‡∏Å', 'block', '‡∏™‡∏±‡∏ç‡∏ç‡∏≤', '‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥',
        '‡∏Å‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á', '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£', '‡∏û‡∏±‡∏í‡∏ô‡∏≤', '‡∏™‡∏≥‡∏£‡∏ß‡∏à', '‡∏Ç‡∏∏‡∏î‡πÄ‡∏à‡∏≤‡∏∞', '‡πÅ‡∏´‡∏•‡πà‡∏á',
        'project', 'concession', 'contract', 'approval', 'construction'
    ]
    
    @classmethod
    def is_official_source(cls, url: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        domain = urlparse(url).netloc.lower()
        return any(official in domain for official in cls.OFFICIAL_SOURCES)
    
    @classmethod
    def contains_official_keywords(cls, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in cls.OFFICIAL_KEYWORDS)
    
    @classmethod
    def is_energy_related(cls, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in cls.ENERGY_KEYWORDS)
    
    @classmethod
    def contains_project_reference(cls, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in cls.PROJECT_KEYWORDS)
    
    @classmethod
    def detect_country(cls, text: str) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        text_lower = text.lower()
        
        country_patterns = {
            "Thailand": ['‡πÑ‡∏ó‡∏¢', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢', 'thailand', 'bangkok', '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û'],
            "Myanmar": ['‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤', 'myanmar', '‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á', 'yangon', '‡∏û‡∏°‡πà‡∏≤'],
            "Malaysia": ['‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢', 'malaysia', '‡∏Å‡∏±‡∏ß‡∏•‡∏≤‡∏•‡∏±‡∏°‡πÄ‡∏õ‡∏≠‡∏£‡πå', 'kuala lumpur'],
            "Vietnam": ['‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏°', 'vietnam', '‡∏Æ‡∏≤‡∏ô‡∏≠‡∏¢', 'hanoi', '‡πÄ‡∏ß‡∏µ‡∏¢‡∏î'],
            "Indonesia": ['‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢', 'indonesia', '‡∏à‡∏≤‡∏Å‡∏≤‡∏£‡πå‡∏ï‡∏≤', 'jakarta'],
            "Kazakhstan": ['‡∏Ñ‡∏≤‡∏ã‡∏±‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô', 'kazakhstan', 'astana'],
            "Oman": ['‡πÇ‡∏≠‡∏°‡∏≤‡∏ô', 'oman', 'muscat'],
            "UAE": ['‡∏¢‡∏π‡πÄ‡∏≠‡∏≠‡∏µ', 'uae', '‡∏î‡∏π‡πÑ‡∏ö', 'dubai', '‡∏≠‡∏≤‡∏ö‡∏π‡∏î‡∏≤‡∏ö‡∏µ', 'abu dhabi']
        }
        
        for country, patterns in country_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                return country
        
        return ""

# =============================================================================
# FEEDS
# =============================================================================
def gnews_rss(q: str, hl="en", gl="US", ceid="US:en") -> str:
    return f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl={hl}&gl={gl}&ceid={ceid}"

FEEDS = [
    ("GoogleNewsTH", "thai", gnews_rss(
        '(‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR "‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü" OR ‡∏Å‡πä‡∏≤‡∏ã OR LNG OR ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô OR ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR "‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤" OR "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô" OR "‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå")',
        hl="th", gl="TH", ceid="TH:th"
    )),
    ("GoogleNewsEN", "international", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas OR renewable OR solar) AND (Thailand OR Vietnam OR Malaysia OR Indonesia OR Myanmar)',
        hl="en", gl="US", ceid="US:en"
    )),
]

# =============================================================================
# UTILITIES (Enhanced)
# =============================================================================
def now_tz() -> datetime:
    return datetime.now(TZ)

def normalize_url(url: str) -> str:
    url = (url or "").strip()
    if not url:
        return url
    try:
        u = urlparse(url)
        # ‡∏•‡∏ö fragment ‡πÅ‡∏•‡∏∞ query string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        query_params = parse_qs(u.query)
        # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ query ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        keep_params = ['p', 'id', 'article', 'news']
        filtered_query = {k: v for k, v in query_params.items() if k in keep_params}
        
        if filtered_query:
            from urllib.parse import urlencode
            new_query = urlencode(filtered_query, doseq=True)
            u = u._replace(query=new_query, fragment="")
        else:
            u = u._replace(query="", fragment="")
        
        return u.geturl()
    except Exception:
        return url

def shorten_google_news_url(url: str) -> str:
    """‡∏î‡∏∂‡∏á URL ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Google News redirect"""
    url = normalize_url(url)
    if not url:
        return url
    try:
        u = urlparse(url)
        if "news.google.com" in u.netloc:
            qs = parse_qs(u.query)
            if "url" in qs and qs["url"]:
                actual_url = unquote(qs["url"][0])
                # ‡∏•‡∏ö tracking parameters
                tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'fbclid']
                parsed = urlparse(actual_url)
                query_params = parse_qs(parsed.query)
                
                # ‡∏•‡∏ö tracking parameters
                for param in tracking_params:
                    query_params.pop(param, None)
                
                if query_params:
                    from urllib.parse import urlencode
                    new_query = urlencode(query_params, doseq=True)
                    parsed = parsed._replace(query=new_query)
                else:
                    parsed = parsed._replace(query="")
                
                return parsed.geturl()
    except Exception:
        pass
    return url

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def read_sent_links() -> set:
    sent = set()
    today_file = os.path.join(SENT_DIR, now_tz().strftime("%Y-%m-%d") + ".txt")
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ
    if os.path.exists(today_file):
        try:
            with open(today_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        sent.add(line)
        except Exception:
            pass
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ã‡πâ‡∏≥)
    yesterday = now_tz() - timedelta(days=1)
    yesterday_file = os.path.join(SENT_DIR, yesterday.strftime("%Y-%m-%d") + ".txt")
    if os.path.exists(yesterday_file):
        try:
            with open(yesterday_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        sent.add(line)
        except Exception:
            pass
    
    return sent

def append_sent_link(url: str):
    url = normalize_url(url)
    if not url:
        return
    fn = os.path.join(SENT_DIR, now_tz().strftime("%Y-%m-%d") + ".txt")
    with open(fn, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def in_time_window(published_dt: datetime, hours: int) -> bool:
    if not published_dt:
        return False
    return published_dt >= (now_tz() - timedelta(hours=hours))

def cut(s: str, n: int) -> str:
    s = (s or "").strip()
    if len(s) <= n:
        return s
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    if LanguageDetector.detect_language(s) == "th":
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        if len(s) > n:
            # ‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î
            space_pos = s[:n].rfind(' ')
            if space_pos > n * 0.7:  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£
                return s[:space_pos] + "‚Ä¶"
    return s[: n - 1].rstrip() + "‚Ä¶"

# =============================================================================
# SAFE FEED FETCHER
# =============================================================================
def safe_fetch_feed(name: str, section: str, url: str, retries: int = 3):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• feed ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà"""
    for attempt in range(retries):
        try:
            print(f"[FEED] {name}: ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt + 1}")
            d = feedparser.parse(url)
            entries = d.entries or []
            print(f"[FEED] {name}: ‡∏û‡∏ö {len(entries)} ‡∏Ç‡πà‡∏≤‡∏ß")
            return entries
        except Exception as e:
            print(f"[ERROR] ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á {name} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt + 1}/{retries}): {str(e)}")
            if attempt < retries - 1:
                wait_time = 2 ** attempt  # exponential backoff
                print(f"[WAIT] ‡∏£‡∏≠ {wait_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
                time.sleep(wait_time)
    return []

# =============================================================================
# RSS PARSING (Enhanced)
# =============================================================================
def parse_entry(e, feed_name: str, section: str):
    title = (getattr(e, "title", "") or "").strip()
    link = (getattr(e, "link", "") or "").strip()
    summary = (getattr(e, "summary", "") or "").strip()
    published = getattr(e, "published", None) or getattr(e, "updated", None)
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    title = LanguageDetector.normalize_thai_text(title)
    summary = LanguageDetector.normalize_thai_text(summary)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤
    language = LanguageDetector.detect_language(title + " " + summary)
    
    try:
        published_dt = dateutil_parser.parse(published) if published else None
        if published_dt and published_dt.tzinfo is None:
            published_dt = TZ.localize(published_dt)
        if published_dt:
            published_dt = published_dt.astimezone(TZ)
    except Exception:
        published_dt = None
    
    canon = shorten_google_news_url(link)
    
    return {
        "title": title,
        "url": normalize_url(link),
        "canon_url": normalize_url(canon),
        "summary": summary,
        "published_dt": published_dt,
        "feed": feed_name,
        "section": section,
        "language": language,
        "original_published": published
    }

# =============================================================================
# LLM ANALYZER (Enhanced for Thai)
# =============================================================================
class LLMAnalyzer:
    def __init__(self, api_key: str, model: str, endpoint: str):
        self.api_key = api_key
        self.model = model
        self.endpoint = endpoint
    
    def analyze_news(self, title: str, summary: str, language: str = "th") -> dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡πâ‡∏ß‡∏¢ LLM"""
        if not self.api_key:
            return self._get_default_analysis()
        
        # ‡∏õ‡∏£‡∏±‡∏ö system prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        if language == "th":
            system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
            ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:
            {
                "relevant": true/false,
                "country": "‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á",
                "official": true/false,
                "summary_th": "‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô 1-2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ",
                "topics": ["‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠1", "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠2"],
                "impact_level": "‡∏™‡∏π‡∏á/‡∏Å‡∏•‡∏≤‡∏á/‡∏ï‡πà‡∏≥",
                "project_mentioned": true/false
            }
            
            ‡πÄ‡∏Å‡∏ì‡∏ë‡πå:
            - relevant: ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
            - country: ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            - official: ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£ ‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ
            - summary_th: ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            - topics: ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô, ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤, ‡∏Å‡πä‡∏≤‡∏ã, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
            - impact_level: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö (‡∏™‡∏π‡∏á=‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏£‡∏≤‡∏Ñ‡∏≤, ‡∏Å‡∏•‡∏≤‡∏á=‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤, ‡∏ï‡πà‡∏≥=‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)
            - project_mentioned: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        else:
            system_prompt = """You are a news analyzer for energy news.
            Respond only in JSON format:
            {
                "relevant": true/false,
                "country": "country name or empty",
                "official": true/false,
                "summary_th": "summary in Thai (1-2 sentences)",
                "topics": ["topic1", "topic2"],
                "impact_level": "high/medium/low",
                "project_mentioned": true/false
            }
            
            Criteria:
            - relevant: related to energy, energy projects, energy policies
            - country: identify country from content
            - official: official news, government announcements, cabinet resolutions
            - summary_th: short summary in Thai language
            - topics: topics like energy, electricity, gas, policy, project
            - impact_level: impact level (high=affects policy/prices, medium=progress update, low=general news)
            - project_mentioned: mentions specific energy projects or not"""
        
        user_prompt = f"""‡∏Ç‡πà‡∏≤‡∏ß: {title[:200]}
        
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {summary[:500]}
        
        ‡∏†‡∏≤‡∏©‡∏≤: {language}
        
        ‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î:"""
        
        try:
            response = requests.post(
                self.endpoint,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 500,
                    "response_format": {"type": "json_object"}
                },
                timeout=30
            )
            
            if response.status_code != 200:
                print(f"[LLM] HTTP Error {response.status_code}: {response.text[:200]}")
                return self._get_default_analysis()
            
            data = response.json()
            content = data["choices"][0]["message"]["content"].strip()
            
            try:
                analysis = json.loads(content)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                return {
                    "relevant": bool(analysis.get("relevant", False)),
                    "country": str(analysis.get("country", "")).strip(),
                    "official": bool(analysis.get("official", False)),
                    "summary_th": str(analysis.get("summary_th", "")).strip()[:200],
                    "topics": [str(t).strip() for t in analysis.get("topics", []) if t and len(str(t).strip()) > 0],
                    "impact_level": str(analysis.get("impact_level", "‡∏ï‡πà‡∏≥")).strip(),
                    "project_mentioned": bool(analysis.get("project_mentioned", False))
                }
                
            except json.JSONDecodeError as je:
                print(f"[LLM] JSON Parse Error: {je}")
                print(f"[LLM] Response: {content[:200]}")
                
        except requests.exceptions.Timeout:
            print("[LLM] Request timeout")
        except Exception as e:
            print(f"[LLM] Error: {str(e)}")
        
        return self._get_default_analysis()
    
    def _get_default_analysis(self):
        return {
            "relevant": False,
            "country": "",
            "official": False,
            "summary_th": "",
            "topics": [],
            "impact_level": "‡∏ï‡πà‡∏≥",
            "project_mentioned": False
        }

# =============================================================================
# NEWS PROCESSOR (Enhanced)
# =============================================================================
class NewsProcessor:
    def __init__(self):
        self.sent_links = read_sent_links()
        self.llm_analyzer = LLMAnalyzer(GROQ_API_KEY, GROQ_MODEL, GROQ_ENDPOINT) if GROQ_API_KEY else None
        print(f"[INIT] ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß {len(self.sent_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")
    
    def fetch_and_filter_news(self):
        """‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        all_news = []
        
        for feed_name, feed_type, feed_url in FEEDS:
            print(f"\n[‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•] {feed_name}...")
            
            try:
                entries = safe_fetch_feed(feed_name, feed_type, feed_url)
                
                processed_count = 0
                for entry in entries[:MAX_PER_FEED]:
                    news_item = self._process_entry(entry, feed_name, feed_type)
                    if news_item:
                        all_news.append(news_item)
                        processed_count += 1
                        print(f"  ‚úì {news_item['title'][:50]}...")
                
                print(f"  ‡∏£‡∏ß‡∏°: {processed_count} ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å {len(entries[:MAX_PER_FEED])} ‡∏Ç‡πà‡∏≤‡∏ß")
                        
            except Exception as e:
                print(f"  ‚úó ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        all_news.sort(key=lambda x: (
            -x.get('is_official', 0),
            -NewsQualityScorer.score_news(x),
            -(x.get('published_dt') or datetime.min).timestamp()
        ))
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á
        if len(all_news) > MAX_MESSAGES_PER_RUN:
            print(f"\n[‡∏Å‡∏£‡∏≠‡∏á] ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {len(all_news)} ‡πÄ‡∏õ‡πá‡∏ô {MAX_MESSAGES_PER_RUN} ‡∏Ç‡πà‡∏≤‡∏ß")
            all_news = all_news[:MAX_MESSAGES_PER_RUN]
        
        return all_news
    
    def _process_entry(self, entry, feed_name: str, feed_type: str):
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"""
        item = parse_entry(entry, feed_name, feed_type)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not item["title"] or not item["url"]:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        if item["canon_url"] in self.sent_links or item["url"] in self.sent_links:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
        if item["published_dt"] and not in_time_window(item["published_dt"], WINDOW_HOURS):
            return None
        
        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        full_text = f"{item['title']} {item['summary']}"
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
        if not KeywordFilter.is_energy_related(full_text):
            return None
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        country = KeywordFilter.detect_country(full_text)
        if not country:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ö‡∏≠‡∏Å
            if item["language"] == "th":
                country = "Thailand"
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö feed name
                if "TH" in feed_name:
                    country = "Thailand"
                else:
                    return None  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
        is_official = (
            KeywordFilter.is_official_source(item['url']) or 
            KeywordFilter.contains_official_keywords(full_text)
        )
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
        has_project_ref = KeywordFilter.contains_project_reference(full_text)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        matched_projects = ProjectNormalizer.find_matching_projects(full_text, country)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
        llm_analysis = None
        if USE_LLM_SUMMARY and self.llm_analyzer and (is_official or has_project_ref or matched_projects):
            llm_analysis = self.llm_analyzer.analyze_news(
                item['title'], 
                item['summary'],
                item["language"]
            )
            
            # ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏≤‡∏Å LLM ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
            if llm_analysis['country'] and llm_analysis['country'] in PROJECTS_BY_COUNTRY:
                country = llm_analysis['country']
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å LLM
            if llm_analysis['official']:
                is_official = True
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å LLM
            if llm_analysis['project_mentioned']:
                has_project_ref = True
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        quality_score = NewsQualityScorer.score_news({
            'url': item['url'],
            'is_official': is_official,
            'has_project_ref': has_project_ref or bool(matched_projects),
            'llm_analysis': llm_analysis,
            'published_dt': item['published_dt']
        })
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 8: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á
        project_hints = []
        if matched_projects:
            project_hints = [p["official_name"] for p in matched_projects[:2]]
        elif has_project_ref:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            project_hints = ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß
        return {
            'title': cut(item['title'], 100),
            'url': item['url'],
            'canon_url': item['canon_url'],
            'summary': cut(item['summary'], 200),
            'published_dt': item['published_dt'],
            'country': country,
            'project_hints': project_hints,
            'matched_projects': matched_projects,
            'is_official': is_official,
            'has_project_ref': has_project_ref or bool(matched_projects),
            'quality_score': quality_score,
            'llm_analysis': llm_analysis,
            'feed': feed_name,
            'language': item['language'],
            'source_tier': NewsQualityScorer.get_source_tier(item['url'])
        }

# =============================================================================
# LINE MESSAGE BUILDER (Enhanced)
# =============================================================================
class LineMessageBuilder:
    @staticmethod
    def create_flex_bubble(news_item):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á LINE Flex Bubble ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"""
        title = cut(news_item.get('title', ''), 80)
        
        # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
        pub_dt = news_item.get('published_dt')
        time_str = TimeFormatter.format_publish_time(pub_dt)
        time_emoji = TimeFormatter.get_time_emoji(pub_dt) if pub_dt else "üïê"
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πà‡∏≤‡∏ß
        if news_item.get('is_official'):
            color = "#4CAF50"  # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
            badge = "üì¢ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"
            emoji = "üèõÔ∏è"
        elif news_item.get('llm_analysis'):
            color = "#2196F3"  # ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI
            badge = "ü§ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå AI"
            emoji = "ü§ñ"
        elif news_item.get('quality_score', 0) > 0.7:
            color = "#9C27B0"  # ‡∏™‡∏µ‡∏°‡πà‡∏ß‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
            badge = "‚≠ê ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á"
            emoji = "‚≠ê"
        else:
            color = "#FF9800"  # ‡∏™‡∏µ‡∏™‡πâ‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
            badge = "üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
            emoji = "üì∞"
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å impact_level
        impact_level = "‡∏ï‡πà‡∏≥"
        if news_item.get('llm_analysis'):
            impact_level = news_item['llm_analysis'].get('impact_level', '‡∏ï‡πà‡∏≥')
        
        impact_colors = {
            "‡∏™‡∏π‡∏á": "#F44336",  # ‡∏™‡∏µ‡πÅ‡∏î‡∏á
            "‡∏Å‡∏•‡∏≤‡∏á": "#FF9800",  # ‡∏™‡∏µ‡∏™‡πâ‡∏°
            "‡∏ï‡πà‡∏≥": "#4CAF50",   # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
            "high": "#F44336",
            "medium": "#FF9800",
            "low": "#4CAF50"
        }
        
        impact_color = impact_colors.get(impact_level, "#4CAF50")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ bubble
        contents = [
            {
                "type": "box",
                "layout": "horizontal",
                "contents": [
                    {
                        "type": "text",
                        "text": emoji,
                        "size": "sm",
                        "flex": 0
                    },
                    {
                        "type": "text",
                        "text": f" {time_emoji} {time_str}",
                        "size": "xs",
                        "color": "#666666",
                        "flex": 1,
                        "margin": "sm"
                    }
                ],
                "margin": "xs"
            },
            {
                "type": "text",
                "text": title,
                "weight": "bold",
                "size": "md",
                "wrap": True,
                "margin": "md"
            }
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        metadata = []
        country = news_item.get('country', 'N/A')
        feed = news_item.get('feed', '')
        source_tier = news_item.get('source_tier', 'unknown')
        
        country_text = f"üáπüá≠ {country}" if country == "Thailand" else f"üåç {country}"
        metadata.append(country_text)
        
        if feed:
            metadata.append(feed)
        
        if source_tier != "unknown":
            tier_text = {"high": "‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥", "medium": "‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "low": "‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå"}
            metadata.append(tier_text.get(source_tier, ""))
        
        if metadata:
            contents.append({
                "type": "text",
                "text": " | ".join(filter(None, metadata)),
                "size": "xs",
                "color": "#888888",
                "margin": "sm",
                "wrap": True
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        if news_item.get('project_hints'):
            hints_text = ", ".join(news_item['project_hints'])
            contents.append({
                "type": "text",
                "text": f"üîó ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {hints_text}",
                "size": "sm",
                "color": "#2E7D32",
                "wrap": True,
                "margin": "xs"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏≤‡∏Å LLM
        if news_item.get('llm_analysis') and news_item['llm_analysis'].get('topics'):
            topics = news_item['llm_analysis']['topics'][:3]
            topics_text = "üè∑Ô∏è " + ", ".join(topics)
            contents.append({
                "type": "text",
                "text": topics_text,
                "size": "xs",
                "color": "#757575",
                "wrap": True,
                "margin": "xs"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å LLM
        if news_item.get('llm_analysis') and news_item['llm_analysis'].get('summary_th'):
            contents.append({
                "type": "text",
                "text": news_item['llm_analysis']['summary_th'],
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#424242"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö
        contents.append({
            "type": "box",
            "layout": "horizontal",
            "contents": [
                {
                    "type": "text",
                    "text": f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö: {impact_level}",
                    "size": "xs",
                    "color": impact_color,
                    "weight": "bold",
                    "flex": 1
                },
                {
                    "type": "text",
                    "text": badge,
                    "size": "xs",
                    "color": color,
                    "align": "end"
                }
            ],
            "margin": "sm"
        })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bubble
        bubble = {
            "type": "bubble",
            "size": "kilo",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": contents,
                "paddingAll": "12px",
                "spacing": "sm"
            }
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏∏‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°
        url = news_item.get('canon_url') or news_item.get('url')
        if url and len(url) < 1000:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE
            bubble["footer"] = {
                "type": "box",
                "layout": "vertical",
                "spacing": "sm",
                "contents": [
                    {
                        "type": "button",
                        "style": "primary",
                        "height": "sm",
                        "action": {
                            "type": "uri",
                            "label": "üìñ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°",
                            "uri": url
                        },
                        "color": color
                    }
                ]
            }
        
        return bubble
    
    @staticmethod
    def create_carousel_message(news_items):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° carousel LINE ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß"""
        if not news_items:
            return None
        
        bubbles = []
        
        for item in news_items[:BUBBLES_PER_CAROUSEL]:
            bubble = LineMessageBuilder.create_flex_bubble(item)
            if bubble:
                bubbles.append(bubble)
        
        if not bubbles:
            return None
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á header ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö carousel
        total_official = sum(1 for item in news_items[:BUBBLES_PER_CAROUSEL] if item.get('is_official'))
        total_projects = sum(1 for item in news_items[:BUBBLES_PER_CAROUSEL] if item.get('has_project_ref'))
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ
        summary_text = f"üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ({len(bubbles)} ‡∏Ç‡πà‡∏≤‡∏ß)"
        if total_official > 0:
            summary_text += f" | üì¢ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£: {total_official}"
        if total_projects > 0:
            summary_text += f" | üîó ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {total_projects}"
        
        return {
            "type": "flex",
            "altText": summary_text,
            "contents": {
                "type": "carousel",
                "contents": bubbles
            }
        }

# =============================================================================
# LINE SENDER
# =============================================================================
class LineSender:
    def __init__(self, access_token):
        self.access_token = access_token
        self.headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
    
    def send_message(self, message_obj):
        """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE"""
        if DRY_RUN:
            print("\n" + "="*60)
            print("‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö - ‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:")
            print("="*60)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            contents = message_obj.get('contents', {}).get('contents', [])
            for i, bubble in enumerate(contents):
                title_elements = bubble.get('body', {}).get('contents', [{}])
                title = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"
                for element in title_elements:
                    if element.get('type') == 'text' and element.get('weight') == 'bold':
                        title = element.get('text', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠')
                        break
                
                # ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
                country = "N/A"
                for element in title_elements:
                    if element.get('type') == 'text' and '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®:' in element.get('text', ''):
                        country = element.get('text', 'N/A')
                        break
                
                print(f"{i+1}. {title[:50]}...")
                print(f"   üìç {country}")
            
            print(f"\n‡∏£‡∏ß‡∏°: {len(contents)} ‡∏Ç‡πà‡∏≤‡∏ß")
            print("="*60)
            return True
        
        url = "https://api.line.me/v2/bot/message/broadcast"
        
        try:
            response = requests.post(
                url,
                headers=self.headers,
                json={"messages": [message_obj]},
                timeout=30
            )
            
            if response.status_code == 200:
                print("[LINE] ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                return True
            else:
                print(f"[LINE] ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î {response.status_code}: {response.text[:200]}")
                return False
                
        except Exception as e:
            print(f"[LINE] ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
            return False

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    print("="*60)
    print("‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô (Enhanced Version)")
    print("="*60)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("[‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î LINE_CHANNEL_ACCESS_TOKEN")
        return
    
    if USE_LLM_SUMMARY and not GROQ_API_KEY:
        print("[‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô] ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢ LLM ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ GROQ_API_KEY")
        print("[‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•] ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
    
    print(f"\n[‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤] ‡πÉ‡∏ä‡πâ LLM: {'‡πÉ‡∏ä‡πà' if USE_LLM_SUMMARY and GROQ_API_KEY else '‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà'}")
    print(f"[‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤] ‡∏Å‡∏£‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤: {WINDOW_HOURS} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á")
    print(f"[‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤] ‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {'‡πÉ‡∏ä‡πà' if DRY_RUN else '‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà'}")
    print(f"[‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤] ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°: {', '.join(PROJECTS_BY_COUNTRY.keys())}")
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
    processor = NewsProcessor()
    line_sender = LineSender(LINE_CHANNEL_ACCESS_TOKEN)
    
    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß
    print("\n[1] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß...")
    news_items = processor.fetch_and_filter_news()
    
    if not news_items:
        print("\n[‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
        return
    
    print(f"\n[2] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # ‡∏ô‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    official_count = sum(1 for item in news_items if item.get('is_official'))
    project_count = sum(1 for item in news_items if item.get('has_project_ref'))
    llm_count = sum(1 for item in news_items if item.get('llm_analysis'))
    high_quality = sum(1 for item in news_items if item.get('quality_score', 0) > 0.7)
    
    print(f"   üì¢ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£: {official_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   üîó ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {project_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   ü§ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI: {llm_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   ‚≠ê ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á: {high_quality} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
    countries = {}
    for item in news_items:
        country = item.get('country', 'Unknown')
        countries[country] = countries.get(country, 0) + 1
    
    print(f"   üåç ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {', '.join([f'{c}:{n}' for c, n in countries.items()])}")
    
    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LINE
    print("\n[3] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LINE...")
    line_message = LineMessageBuilder.create_carousel_message(news_items)
    
    if not line_message:
        print("[‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ")
        return
    
    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    print("\n[4] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
    success = line_sender.send_message(line_message)
    
    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ó‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡πâ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    if success and not DRY_RUN:
        for item in news_items:
            append_sent_link(item.get('canon_url') or item.get('url'))
        print("\n[‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    
    print("\n" + "="*60)
    print("‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    print("="*60)

if __name__ == "__main__":
    main()
