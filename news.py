# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote

import requests
import feedparser
import pytz
from dateutil import parser as dateutil_parser

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# =============================================================================
# ENV / CONFIG
# =============================================================================
TZ = pytz.timezone(os.getenv("TZ", "Asia/Bangkok"))

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE_CHANNEL_ACCESS_TOKEN")

# Groq Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant").strip()
GROQ_ENDPOINT = os.getenv("GROQ_ENDPOINT", "https://api.groq.com/openai/v1/chat/completions").strip()
USE_LLM_SUMMARY = os.getenv("USE_LLM_SUMMARY", "1").strip().lower() in ["1", "true", "yes", "y"]

WINDOW_HOURS = int(os.getenv("WINDOW_HOURS", "72"))
MAX_PER_FEED = int(os.getenv("MAX_PER_FEED", "20"))  # ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
DRY_RUN = os.getenv("DRY_RUN", "0").strip().lower() in ["1", "true", "yes", "y"]
MAX_MESSAGES_PER_RUN = int(os.getenv("MAX_MESSAGES_PER_RUN", "10"))
BUBBLES_PER_CAROUSEL = int(os.getenv("BUBBLES_PER_CAROUSEL", "10"))

# Sent links tracking
SENT_DIR = os.getenv("SENT_DIR", "sent_links")
os.makedirs(SENT_DIR, exist_ok=True)

# =============================================================================
# URL VALIDATOR - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏•‡∏≤‡∏™‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå
# =============================================================================
class URLValidator:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡∏¥‡∏á‡∏Å‡πå URL"""
    
    @staticmethod
    def is_valid_url(url: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ"""
        if not url or not isinstance(url, str):
            return False
        
        url = url.strip()
        if not url:
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß (LINE ‡∏à‡∏≥‡∏Å‡∏±‡∏î 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
        if len(url) > 1000:
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö URL
        try:
            result = urlparse(url)
            if not all([result.scheme, result.netloc]):
                return False
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö scheme
            if result.scheme not in ['http', 'https']:
                return False
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö domain
            if len(result.netloc) < 3:  # ‡πÄ‡∏ä‡πà‡∏ô a.co ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏ï‡∏±‡∏ß
                return False
                
            return True
        except:
            return False
    
    @staticmethod
    def extract_actual_url(google_news_url: str) -> str:
        """‡∏î‡∏∂‡∏á URL ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Google News URL"""
        if not google_news_url:
            return ""
        
        try:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Google News URL ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á URL ‡∏à‡∏£‡∏¥‡∏á
            if "news.google.com" in google_news_url:
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å query parameter
                parsed = urlparse(google_news_url)
                query_params = parse_qs(parsed.query)
                
                if 'url' in query_params:
                    actual_url = unquote(query_params['url'][0])
                    if URLValidator.is_valid_url(actual_url):
                        return actual_url
                
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏≤‡∏° redirect 1 ‡∏£‡∏∞‡∏î‡∏±‡∏ö
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Accept': 'text/html,application/xhtml+xml',
                    }
                    response = requests.get(
                        google_news_url, 
                        headers=headers, 
                        timeout=5, 
                        allow_redirects=False
                    )
                    
                    if response.status_code in [301, 302, 303, 307, 308]:
                        location = response.headers.get('Location')
                        if location and URLValidator.is_valid_url(location):
                            return location
                except:
                    pass
        except:
            pass
        
        return google_news_url
    
    @staticmethod
    def shorten_url_if_needed(url: str) -> str:
        """‡∏¢‡πà‡∏≠ URL ‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ"""
        if not url:
            return ""
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 800 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏¢‡πà‡∏≠
        if len(url) > 800:
            try:
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ path ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                parsed = urlparse(url)
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Google News ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ extract
                if "news.google.com" in parsed.netloc:
                    actual_url = URLValidator.extract_actual_url(url)
                    if actual_url and len(actual_url) < len(url):
                        return actual_url
                
                # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á query parameters
                if parsed.query:
                    # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                    params = parse_qs(parsed.query)
                    important_params = {}
                    
                    for key in ['id', 'p', 'article', 'story', 'url']:
                        if key in params:
                            important_params[key] = params[key][0]
                    
                    if important_params:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á query string ‡πÉ‡∏´‡∏°‡πà
                        new_query = '&'.join([f"{k}={v}" for k, v in important_params.items()])
                        new_url = parsed._replace(query=new_query, fragment="").geturl()
                        
                        if len(new_url) < len(url):
                            return new_url
            except:
                pass
        
        return url

# =============================================================================
# TEXT CLEANER
# =============================================================================
class TextCleaner:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    
    BAD_PHRASES = [
        "‡∏™‡∏≤‡∏¢‡∏ö‡∏≥‡∏£‡∏ì‡∏µ‡∏°", "‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏≥‡∏£‡∏ì‡∏µ‡∏°", "‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏™‡∏±‡∏ô‡∏ã‡∏¥‡∏õ‡πÑ‡∏ï‡∏¢‡πÄ‡∏™‡∏µ‡∏¢‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≤‡∏¢‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡∏ü‡πâ‡∏≤",
        "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°", "‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°",
        "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡πà‡∏≤‡∏ß", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß", "‡πÅ‡∏ä‡∏£‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ",
        "Advertisement", "Promoted", "Sponsored", "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤",
        "Click here to read more", "Read full story", "Continue reading"
    ]
    
    @staticmethod
    def clean_text(text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not text:
            return ""
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        for phrase in TextCleaner.BAD_PHRASES:
            text = text.replace(phrase, "")
        
        # ‡∏•‡∏ö HTML tags ‡πÅ‡∏•‡∏∞ entities
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'&[a-z]+;', '', text)
        
        # ‡∏•‡∏ö URL
        text = re.sub(r'https?://\S+', '', text)
        
        # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        text = re.sub(r'[.,!?;:]{3,}', '...', text)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        return text.strip()
    
    @staticmethod
    def extract_meaningful_summary(text: str, max_length: int = 200) -> str:
        """‡∏î‡∏∂‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        text = TextCleaner.clean_text(text)
        if not text:
            return ""
        
        # ‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        sentences = re.split(r'[.!?]+\s*', text)
        
        # ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£)
        meaningful_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence.split()) >= 5:  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏≥
                meaningful_sentences.append(sentence)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
        if not meaningful_sentences:
            return text[:max_length]
        
        # ‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
        summary = ' '.join(meaningful_sentences[:2])  # ‡πÉ‡∏ä‡πâ 2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÅ‡∏£‡∏Å
        return summary[:max_length]

# =============================================================================
# PROJECT DATABASE
# =============================================================================
PROJECTS_BY_COUNTRY = {
    "Thailand": [
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 2/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå", "Arthit",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏™ 1", "S1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4", "Contract 4",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ‡∏≠‡∏µ‡∏û‡∏µ 1", "PTTEP 1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 6/27",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 22/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏µ 5", "E5",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏†‡∏π‡∏Æ‡πà‡∏≠‡∏°", "Sinphuhorm",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 8/32", "B8/32", "9A", "9‡πÄ‡∏≠",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/48", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 12/48",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/65", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 3/65",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 53/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 54/43"
    ],
    # ... ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
}

# =============================================================================
# ENHANCED RSS FEEDS - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
# =============================================================================
def gnews_rss(q: str, hl="en", gl="US", ceid="US:en") -> str:
    """Generate Google News RSS URL"""
    return f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl={hl}&gl={gl}&ceid={ceid}"

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
QUALITY_FEEDS = [
    # ==================== ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ====================
    ("BangkokBizNews_Energy", "thai_business", 
     "https://www.bangkokbiznews.com/tag/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô/rss"),
    
    ("PostToday_Energy", "thai_business",
     "https://www.posttoday.com/rss/src/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"),
    
    ("Prachachat_Energy", "thai_business",
     "https://www.prachachat.net/feed/tag/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"),
    
    ("Thansettakij_Energy", "thai_business",
     "https://www.thansettakij.com/rss/tag/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"),
    
    ("Manager_Energy", "thai_business",
     "https://mgronline.com/rss/rssfeeds/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô.aspx"),
    
    # ==================== ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ====================
    ("Reuters_Energy", "international",
     "https://www.reutersagency.com/feed/?taxonomy=best-sectors&post_type=best&sector=energy-environment"),
    
    ("Bloomberg_Energy", "international",
     "https://www.bloomberg.com/energy/feed"),
    
    ("OilPrice_Top", "energy_international",
     "https://oilprice.com/feed/op-top-stories.xml"),
    
    ("S&P_Global_Energy", "energy_international",
     "https://www.spglobal.com/_assets/platts/rss-feed/platts-oil.xml"),
    
    # ==================== ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ====================
    ("‡∏Å‡∏£‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "thai_official",
     "https://www.doeb.go.th/2014/th/rss"),
    
    ("EGAT_News", "thai_official",
     "https://www.egat.co.th/home/rss-news/"),
]

# =============================================================================
# ENHANCED RSS PARSER
# =============================================================================
class EnhancedRSSParser:
    """Parser RSS ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
    
    @staticmethod
    def fetch_feed_with_fallback(feed_name: str, feed_url: str):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• RSS ‡∏û‡∏£‡πâ‡∏≠‡∏° fallback ‡∏´‡∏≤‡∏Å URL ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        try:
            print(f"[RSS] Fetching {feed_name}...")
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ headers ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏•‡πá‡∏≠‡∏Å
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'en-US,en;q=0.9,th;q=0.8',
            }
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• RSS
            response = requests.get(feed_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # Parse RSS
            feed = feedparser.parse(response.content)
            
            if feed.bozo:  # ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ parse
                print(f"[RSS WARNING] {feed_name}: Parse issues")
            
            entries = feed.entries or []
            print(f"[RSS] {feed_name}: Found {len(entries)} entries")
            return entries
            
        except requests.exceptions.Timeout:
            print(f"[RSS ERROR] {feed_name}: Timeout")
            return []
        except requests.exceptions.RequestException as e:
            print(f"[RSS ERROR] {feed_name}: {str(e)}")
            # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Google News RSS ‡πÅ‡∏ó‡∏ô
            return EnhancedRSSParser._fallback_to_google_news(feed_name)
        except Exception as e:
            print(f"[RSS ERROR] {feed_name}: Unexpected error - {str(e)}")
            return []
    
    @staticmethod
    def _fallback_to_google_news(feed_name: str):
        """Fallback ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Google News RSS"""
        google_news_map = {
            "BangkokBizNews_Energy": gnews_rss("‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô site:bangkokbiznews.com", hl="th", gl="TH"),
            "PostToday_Energy": gnews_rss("‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô site:posttoday.com", hl="th", gl="TH"),
            "Reuters_Energy": gnews_rss("energy OR oil OR gas site:reuters.com", hl="en", gl="US"),
            "Bloomberg_Energy": gnews_rss("energy OR oil OR gas site:bloomberg.com", hl="en", gl="US"),
        }
        
        if feed_name in google_news_map:
            print(f"[RSS] Using Google News fallback for {feed_name}")
            try:
                feed = feedparser.parse(google_news_map[feed_name])
                return feed.entries or []
            except:
                return []
        
        return []
    
    @staticmethod
    def parse_entry_with_enhancement(entry, feed_name: str, feed_type: str):
        """Parse entry ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"""
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        title = TextCleaner.clean_text(getattr(entry, "title", "") or "")
        link = (getattr(entry, "link", "") or "").strip()
        summary = TextCleaner.clean_text(getattr(entry, "summary", "") or "")
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ title ‡∏´‡∏£‡∏∑‡∏≠ title ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        if not title or len(title) < 10:
            return None
        
        # ‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà
        published = getattr(entry, "published", None) or getattr(entry, "updated", None)
        published_dt = None
        
        try:
            if published:
                published_dt = dateutil_parser.parse(published)
                if published_dt.tzinfo is None:
                    published_dt = TZ.localize(published_dt)
                published_dt = published_dt.astimezone(TZ)
        except:
            published_dt = None
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL
        original_url = link
        actual_url = URLValidator.extract_actual_url(link)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ URL ‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏ô
        if URLValidator.is_valid_url(actual_url):
            final_url = actual_url
        elif URLValidator.is_valid_url(original_url):
            final_url = original_url
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ URL ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å feed
            final_url = EnhancedRSSParser._generate_fallback_url(feed_name, entry)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ URL ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ
        if not final_url or not URLValidator.is_valid_url(final_url):
            print(f"[RSS] Skipping {title[:30]}... - No valid URL")
            return None
        
        # ‡∏¢‡πà‡∏≠ URL ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        final_url = URLValidator.shorten_url_if_needed(final_url)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
        enhanced_summary = TextCleaner.extract_meaningful_summary(summary)
        if not enhanced_summary and hasattr(entry, 'content'):
            # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å content
            content_text = ""
            for content in entry.get('content', []):
                if hasattr(content, 'value'):
                    content_text += content.value + " "
            enhanced_summary = TextCleaner.extract_meaningful_summary(content_text)
        
        return {
            "title": title[:120],
            "url": final_url,
            "original_url": original_url,
            "summary": enhanced_summary[:250],
            "published_dt": published_dt,
            "feed": feed_name,
            "section": feed_type,
            "has_valid_url": URLValidator.is_valid_url(final_url),
            "url_length": len(final_url),
        }
    
    @staticmethod
    def _generate_fallback_url(feed_name: str, entry):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á URL fallback ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ"""
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á feed ‡∏ó‡∏µ‡πà‡∏°‡∏µ guid ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô URL
        guid = getattr(entry, "guid", "")
        if guid and URLValidator.is_valid_url(guid):
            return guid
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á feed ‡∏ó‡∏µ‡πà‡∏°‡∏µ link ‡πÉ‡∏ô content
        if hasattr(entry, 'links'):
            for link in entry.links:
                if hasattr(link, 'href') and URLValidator.is_valid_url(link.href):
                    return link.href
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á feed ‡∏ó‡∏µ‡πà‡∏°‡∏µ ID ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á URL ‡πÑ‡∏î‡πâ
        if hasattr(entry, 'id'):
            entry_id = entry.id
            feed_url_map = {
                "BangkokBizNews_Energy": f"https://www.bangkokbiznews.com/news/{entry_id}",
                "PostToday_Energy": f"https://www.posttoday.com/{entry_id}",
            }
            
            if feed_name in feed_url_map:
                return feed_url_map[feed_name]
        
        return ""

# =============================================================================
# MAIN NEWS PROCESSOR (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
# =============================================================================
class EnhancedNewsProcessor:
    def __init__(self):
        self.sent_links = read_sent_links()
        self.llm_analyzer = None  # ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        self.rss_parser = EnhancedRSSParser()
    
    def fetch_and_filter_news(self):
        """‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å feeds"""
        all_news = []
        
        for feed_name, feed_type, feed_url in QUALITY_FEEDS:
            print(f"\n[Fetching] {feed_name}...")
            
            try:
                entries = self.rss_parser.fetch_feed_with_fallback(feed_name, feed_url)
                processed_count = 0
                
                for entry in entries[:MAX_PER_FEED]:
                    news_item = self._process_entry(entry, feed_name, feed_type)
                    if news_item:
                        all_news.append(news_item)
                        processed_count += 1
                        
                        if processed_count <= 3:  # ‡πÅ‡∏™‡∏î‡∏á 3 ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏£‡∏Å
                            print(f"  ‚úì {news_item['title'][:50]}...")
                
                print(f"  Total processed: {processed_count} items")
                        
            except Exception as e:
                print(f"  ‚úó Error in {feed_name}: {str(e)}")
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ URL ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
        all_news = [item for item in all_news if item.get('has_valid_url', False)]
        
        # Sort by importance
        all_news.sort(key=lambda x: (
            -x.get('is_official', 0),
            -(x.get('published_dt') or datetime.min).timestamp()
        ))
        
        return all_news
    
    def _process_entry(self, entry, feed_name: str, feed_type: str):
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß"""
        item = self.rss_parser.parse_entry_with_enhancement(entry, feed_name, feed_type)
        if not item:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if item["url"] in self.sent_links:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
        if item["published_dt"] and not in_time_window(item["published_dt"], WINDOW_HOURS):
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        full_text = f"{item['title']} {item['summary']}".lower()
        energy_keywords = [
            '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü', '‡∏Å‡πä‡∏≤‡∏ã', 'lng', '‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô',
            'energy', 'electricity', 'power', 'gas', 'oil',
            '‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', 'power plant', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô', 'renewable'
        ]
        
        if not any(keyword in full_text for keyword in energy_keywords):
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        country = self._detect_country(full_text, feed_name)
        if not country:
            country = "Thailand" if feed_type in ['thai_business', 'thai_official'] else "International"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
        is_official = self._is_official_news(item, feed_type)
        
        # ‡∏î‡∏∂‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        project_hints = PROJECTS_BY_COUNTRY.get(country, [])[:2]
        
        # ‡πÉ‡∏ä‡πâ LLM ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        llm_analysis = None
        if USE_LLM_SUMMARY and GROQ_API_KEY:
            if not self.llm_analyzer:
                from .llm_analyzer import LLMAnalyzer  # Import when needed
                self.llm_analyzer = LLMAnalyzer(GROQ_API_KEY, GROQ_MODEL, GROQ_ENDPOINT)
            
            if self.llm_analyzer:
                llm_analysis = self.llm_analyzer.analyze_news(item['title'], item['summary'])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        return {
            'title': item['title'],
            'url': item['url'],
            'summary': item['summary'],
            'published_dt': item['published_dt'],
            'country': country,
            'project_hints': project_hints,
            'is_official': is_official,
            'llm_analysis': llm_analysis,
            'feed': feed_name,
            'feed_type': feed_type,
            'has_valid_url': item.get('has_valid_url', True)
        }
    
    def _detect_country(self, text: str, feed_name: str) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®"""
        text_lower = text.lower()
        
        country_patterns = {
            "Thailand": ['‡πÑ‡∏ó‡∏¢', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢', 'thailand', 'bangkok'],
            "Vietnam": ['‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏°', 'vietnam', 'hanoi'],
            "Malaysia": ['‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢', 'malaysia', 'kuala lumpur'],
            "Indonesia": ['‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢', 'indonesia', 'jakarta'],
            "Myanmar": ['‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤', 'myanmar', 'yangon'],
        }
        
        for country, patterns in country_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                return country
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏à‡∏≤‡∏Å feed name
        for country in country_patterns.keys():
            if country.lower() in feed_name.lower():
                return country
        
        return ""
    
    def _is_official_news(self, item, feed_type: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å feed type
        if feed_type == 'thai_official':
            return True
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å URL
        url = item.get('url', '')
        official_domains = ['.go.th', '.gov', 'egat.co.th', 'doeb.go.th']
        if any(domain in url for domain in official_domains):
            return True
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        text = f"{item['title']} {item['summary']}".lower()
        official_keywords = [
            '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á', '‡∏Å‡∏£‡∏°', '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤',
            'minister', 'ministry', 'regulation', 'official'
        ]
        
        return any(keyword in text for keyword in official_keywords)

# =============================================================================
# ENHANCED LINE MESSAGE BUILDER
# =============================================================================
class EnhancedLineMessageBuilder:
    @staticmethod
    def create_flex_bubble(news_item):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á LINE Flex Bubble"""
        title = cut(news_item.get('title', ''), 100)
        
        # Format timestamp
        pub_dt = news_item.get('published_dt')
        time_str = pub_dt.strftime("%d/%m/%Y %H:%M") if pub_dt else ""
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏î‡∏à‡πå
        if news_item.get('is_official'):
            color = "#4CAF50"
            badge = "üì¢ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"
        elif news_item.get('llm_analysis'):
            color = "#2196F3"
            badge = "ü§ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI"
        else:
            color = "#FF9800"
            badge = "üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        contents = [
            {
                "type": "text",
                "text": title,
                "weight": "bold",
                "size": "md",
                "wrap": True,
                "margin": "md"
            }
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
        metadata = []
        if time_str:
            metadata.append(time_str)
        if news_item.get('feed'):
            # ‡∏¢‡πà‡∏≠‡∏ä‡∏∑‡πà‡∏≠ feed ‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô
            feed_name = news_item['feed']
            if len(feed_name) > 15:
                feed_name = feed_name.split('_')[0]
            metadata.append(feed_name)
        
        if metadata:
            contents.append({
                "type": "text",
                "text": " | ".join(metadata),
                "size": "xs",
                "color": "#888888",
                "margin": "sm"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
        contents.append({
            "type": "text",
            "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {news_item.get('country', 'N/A')}",
            "size": "sm",
            "margin": "xs"
        })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        if news_item.get('project_hints'):
            hints_text = ", ".join(news_item['project_hints'][:2])
            contents.append({
                "type": "text",
                "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£: {hints_text}",
                "size": "sm",
                "color": "#2E7D32",
                "wrap": True,
                "margin": "xs"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å LLM
        if news_item.get('llm_analysis') and news_item['llm_analysis'].get('summary_th'):
            contents.append({
                "type": "text",
                "text": news_item['llm_analysis']['summary_th'],
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#424242"
            })
        elif news_item.get('summary'):
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ LLM summary ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ summary ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°
            contents.append({
                "type": "text",
                "text": news_item['summary'],
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#666666"
            })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ö‡∏î‡∏à‡πå
        contents.append({
            "type": "text",
            "text": badge,
            "size": "xs",
            "color": color,
            "margin": "sm"
        })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bubble
        bubble = {
            "type": "bubble",
            "size": "kilo",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": contents,
                "paddingAll": "12px"
            }
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏∏‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ URL ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        url = news_item.get('url')
        if url and URLValidator.is_valid_url(url):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            if len(url) <= 1000:
                bubble["footer"] = {
                    "type": "box",
                    "layout": "vertical",
                    "spacing": "sm",
                    "contents": [
                        {
                            "type": "button",
                            "style": "primary",
                            "height": "sm",
                            "action": {
                                "type": "uri",
                                "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°",
                                "uri": url
                            }
                        }
                    ]
                }
            else:
                # ‡∏ñ‡πâ‡∏≤ URL ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏ó‡∏ô
                contents.append({
                    "type": "text",
                    "text": "‚ö†Ô∏è ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏î‡πâ",
                    "size": "xs",
                    "color": "#F44336",
                    "margin": "sm"
                })
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ URL ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
            contents.append({
                "type": "text",
                "text": "‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠",
                "size": "xs",
                "color": "#9E9E9E",
                "margin": "sm"
            })
        
        return bubble
    
    @staticmethod
    def create_carousel_message(news_items):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á carousel message"""
        if not news_items:
            return None
        
        bubbles = []
        valid_news_count = 0
        
        for item in news_items:
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ URL
            if not item.get('has_valid_url', True):
                continue
                
            bubble = EnhancedLineMessageBuilder.create_flex_bubble(item)
            if bubble:
                bubbles.append(bubble)
                valid_news_count += 1
                
                if valid_news_count >= BUBBLES_PER_CAROUSEL:
                    break
        
        if not bubbles:
            return None
        
        # ‡πÅ‡∏à‡πâ‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå
        no_link_count = len(news_items) - valid_news_count
        if no_link_count > 0:
            print(f"[INFO] Skipped {no_link_count} news items without valid URLs")
        
        return {
            "type": "flex",
            "altText": f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô {datetime.now(TZ).strftime('%d/%m/%Y')} ({len(bubbles)} ‡∏Ç‡πà‡∏≤‡∏ß)",
            "contents": {
                "type": "carousel",
                "contents": bubbles
            }
        }

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================
def now_tz() -> datetime:
    return datetime.now(TZ)

def normalize_url(url: str) -> str:
    url = (url or "").strip()
    if not url:
        return url
    try:
        u = urlparse(url)
        return u._replace(fragment="").geturl()
    except Exception:
        return url

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def read_sent_links() -> set:
    sent = set()
    for fn in os.listdir(SENT_DIR):
        if not fn.endswith(".txt"):
            continue
        fp = os.path.join(SENT_DIR, fn)
        try:
            with open(fp, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        sent.add(line)
        except Exception:
            continue
    return sent

def append_sent_link(url: str):
    url = normalize_url(url)
    if not url:
        return
    fn = os.path.join(SENT_DIR, now_tz().strftime("%Y-%m-%d") + ".txt")
    with open(fn, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def in_time_window(published_dt: datetime, hours: int) -> bool:
    if not published_dt:
        return False
    return published_dt >= (now_tz() - timedelta(hours=hours))

def cut(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else s[: n - 1].rstrip() + "‚Ä¶"

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    print("="*60)
    print("‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô - Enhanced with URL Fix")
    print("="*60)
    
    # Configuration check
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("[ERROR] LINE_CHANNEL_ACCESS_TOKEN is required")
        return
    
    print(f"\n[CONFIG] Use LLM: {'Yes' if USE_LLM_SUMMARY and GROQ_API_KEY else 'No'}")
    print(f"[CONFIG] Time window: {WINDOW_HOURS} hours")
    print(f"[CONFIG] Dry run: {'Yes' if DRY_RUN else 'No'}")
    print(f"[CONFIG] Feeds: {len(QUALITY_FEEDS)} quality sources")
    
    # Initialize components
    processor = EnhancedNewsProcessor()
    line_sender = LineSender(LINE_CHANNEL_ACCESS_TOKEN)
    
    # Step 1: Fetch and filter news
    print("\n[1] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û...")
    news_items = processor.fetch_and_filter_news()
    
    if not news_items:
        print("\n[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
        return
    
    print(f"\n[2] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # ‡∏ô‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ URL ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
    valid_url_count = sum(1 for item in news_items if item.get('has_valid_url', False))
    print(f"   - ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠: {valid_url_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏±‡∏•‡∏¥‡∏á‡∏Å‡πå: {len(news_items) - valid_url_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # Step 3: Create LINE message (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ URL)
    print("\n[3] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LINE (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå)...")
    line_message = EnhancedLineMessageBuilder.create_carousel_message(news_items)
    
    if not line_message:
        print("[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ)")
        return
    
    # Step 4: Send message
    print("\n[4] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
    success = line_sender.send_message(line_message)
    
    # Step 5: Mark as sent if successful
    if success and not DRY_RUN:
        for item in news_items:
            if item.get('has_valid_url', False):
                append_sent_link(item.get('url'))
        print("\n[SUCCESS] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    
    print("\n" + "="*60)
    print("‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    print("="*60)

# =============================================================================
# LINE SENDER CLASS
# =============================================================================
class LineSender:
    def __init__(self, access_token):
        self.access_token = access_token
        self.headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
    
    def send_message(self, message_obj):
        """Send message to LINE"""
        if DRY_RUN:
            print("\n" + "="*60)
            print("DRY RUN - Would send the following news:")
            print("="*60)
            
            contents = message_obj.get('contents', {}).get('contents', [])
            for i, bubble in enumerate(contents):
                title = bubble.get('body', {}).get('contents', [{}])[0].get('text', 'No title')
                country = ""
                for content in bubble.get('body', {}).get('contents', []):
                    if content.get('text', '').startswith('‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®:'):
                        country = content['text']
                        break
                has_button = 'footer' in bubble
                print(f"{i+1}. {title[:60]}... {country} {'[‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå]' if has_button else '[‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå]'}")
            
            print(f"\nTotal: {len(contents)} news items")
            return True
        
        url = "https://api.line.me/v2/bot/message/broadcast"
        
        try:
            response = requests.post(
                url,
                headers=self.headers,
                json={"messages": [message_obj]},
                timeout=30
            )
            
            if response.status_code == 200:
                print("[LINE] Message sent successfully!")
                return True
            else:
                print(f"[LINE] Error {response.status_code}: {response.text[:200]}")
                return False
                
        except Exception as e:
            print(f"[LINE] Exception: {str(e)}")
            return False

if __name__ == "__main__":
    main()
