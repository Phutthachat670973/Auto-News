import os
import re
import json
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import feedparser
from dateutil import parser as dateutil_parser
import pytz
import requests
import google.generativeai as genai

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ========================= CONFIG =========================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()

if not GEMINI_API_KEY:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Environment/Secrets")
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_CHANNEL_ACCESS_TOKEN ‡πÉ‡∏ô Environment/Secrets")

GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-2.5-flash").strip() or "gemini-2.5-flash"
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel(GEMINI_MODEL_NAME)

# ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤/‡∏£‡∏µ‡∏ó‡πÑ‡∏£
GEMINI_DAILY_BUDGET = int(os.getenv("GEMINI_DAILY_BUDGET", "250"))
MAX_RETRIES = 6
SLEEP_BETWEEN_CALLS = (6.0, 7.0)  # ~10 RPM
DRY_RUN = os.getenv("DRY_RUN", "false").lower() == "true"

bangkok_tz = pytz.timezone("Asia/Bangkok")
now = datetime.now(bangkok_tz)

# HTTP session (‡πÄ‡∏ö‡∏≤ ‡πÄ‡∏£‡πá‡∏ß)
S = requests.Session()
S.headers.update({"User-Agent": "Mozilla/5.0"})
TIMEOUT = 15

# ========== SENT LINKS: ‡∏Å‡∏±‡∏ô‡∏™‡πà‡∏á‡∏ã‡πâ‡∏≥ (‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô) ==========
SENT_LINKS_DIR = "sent_links"
os.makedirs(SENT_LINKS_DIR, exist_ok=True)

def _normalize_link(url: str) -> str:
    try:
        p = urlparse(url)
        netloc = p.netloc.lower()
        scheme = (p.scheme or "https").lower()
        # ‡∏•‡πâ‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        bad_keys = {"fbclid","gclid","ref","ref_","mc_cid","mc_eid"}
        q = []
        for k, v in parse_qsl(p.query, keep_blank_values=True):
            if k.startswith("utm_") or k in bad_keys:
                continue
            q.append((k, v))
        return urlunparse(p._replace(scheme=scheme, netloc=netloc, query=urlencode(q)))
    except Exception:
        return (url or "").strip()

def get_sent_links_file(date=None):
    if date is None:
        date = datetime.now(bangkok_tz).strftime("%Y-%m-%d")
    return os.path.join(SENT_LINKS_DIR, f"{date}.txt")

def load_sent_links_today_yesterday():
    sent_links = set()
    for i in range(2):  # ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ & ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
        date = (now - timedelta(days=i)).strftime("%Y-%m-%d")
        path = get_sent_links_file(date)
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    url = _normalize_link(line.strip())
                    if url:
                        sent_links.add(url)
    return sent_links

def save_sent_links(new_links, date=None):
    path = get_sent_links_file(date)
    with open(path, "a", encoding="utf-8") as f:
        for url in new_links:
            f.write(_normalize_link(url) + "\n")

# ========================= FEEDS =========================
news_sources = {
    "Oilprice": {"url": "https://oilprice.com/rss/main", "category": "Energy", "site": "Oilprice"},
    "CleanTechnica": {"url": "https://cleantechnica.com/feed/", "category": "Energy", "site": "CleanTechnica"},
    "HydrogenFuelNews": {"url": "https://www.hydrogenfuelnews.com/feed/", "category": "Energy", "site": "Hydrogen Fuel News"},
    "Economist": {"url": "https://www.economist.com/latest/rss.xml", "category": "Economy", "site": "Economist"},
    "YahooFinance": {"url": "https://finance.yahoo.com/news/rssindex", "category": "Economy", "site": "Yahoo Finance"},
}

DEFAULT_ICON_URL = "https://scdn.line-apps.com/n/channel_devcenter/img/fx/01_1_cafe.png"
GEMINI_CALLS = 0

# ========================= Helpers =========================
COLON_RX = re.compile(r"[ÔºöÔπïÍûâÔ∏ì‚¶Ç‚∏øÀ∏]")

def _normalize_colons(text: str) -> str:
    return COLON_RX.sub(":", text or "")

def _polish_impact_text(text: str) -> str:
    if not text:
        return text
    text = re.sub(r"\((?:[^)]*(?:‡∏ö‡∏ß‡∏Å|‡∏•‡∏ö|‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô|‡∏™‡∏±‡πâ‡∏ô|‡∏Å‡∏•‡∏≤‡∏á|‡∏¢‡∏≤‡∏ß)[^)]*)\)", "", text)
    text = re.sub(r"\s{2,}", " ", text)
    text = re.sub(r"\s*,\s*,", ", ", text)
    text = re.sub(r"\s*,\s*\.", ".", text)
    return text.strip()

def fetch_article_image(url: str) -> str:
    """‡∏î‡∏∂‡∏á og:image / twitter:image ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏≤ ‡πÜ ‡∏î‡πâ‡∏ß‡∏¢ requests"""
    try:
        r = S.get(url, timeout=TIMEOUT)
        if r.status_code >= 400:
            return ""
        html = r.text
        m = re.search(r'<meta[^>]+property=[\'"]og:image[\'"][^>]+content=[\'"]([^\'"]+)[\'"]', html, re.I)
        if m:
            return m.group(1)
        m = re.search(r'<meta[^>]+name=[\'"]twitter:image[\'"][^>]+content=[\'"]([^\'"]+)[\'"]', html, re.I)
        if m:
            return m.group(1)
        m = re.search(r'<img[^>]+src=[\'"]([^\'"]+)[\'"]', html, re.I)
        if m:
            src = m.group(1)
            if src.startswith("//"):
                parsed = urlparse(url)
                return f"{parsed.scheme}:{src}"
            if src.startswith("/"):
                parsed = urlparse(url)
                return f"{parsed.scheme}://{parsed.netloc}{src}"
            return src
        return ""
    except Exception:
        return ""

# ========================= Upstream Context =========================
PTT_CONTEXT = """
[Department Context ‚Äî Upstream Business Group Subsidiary Management Department]

‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö upstream (PTTEP ‡∏Å‡πà‡∏≠‡∏ô) ‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡πà‡∏≠‡∏¢‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏ú‡∏•‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
‚Ä¢ Brent/WTI/JKM/TTF ‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô
‚Ä¢ ‡πÄ‡∏´‡∏ï‡∏∏‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï/‡∏ó‡πà‡∏≠/‡πÅ‡∏´‡∏•‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
‚Ä¢ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô/PSC/‡∏†‡∏≤‡∏©‡∏µ upstream
‚Ä¢ ‡∏î‡∏µ‡∏• M&A/FID/‡∏ü‡∏≤‡∏£‡πå‡∏°‡∏≠‡∏¥‡∏ô-‡∏ü‡∏≤‡∏£‡πå‡∏°‡πÄ‡∏≠‡∏≤‡∏ó‡πå/‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå
‚Ä¢ OPEC+/‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°/‡∏†‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥

‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö: ‡∏Ç‡πà‡∏≤‡∏ß downstream/PR/EV ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÇ‡∏¢‡∏á‡∏Å‡∏•‡πÑ‡∏Å‡∏™‡∏π‡πà upstream
"""

# ========================= Gemini Wrapper =========================
def call_gemini(prompt, max_retries=MAX_RETRIES):
    global GEMINI_CALLS
    if GEMINI_CALLS >= GEMINI_DAILY_BUDGET:
        raise RuntimeError(f"‡∏ñ‡∏∂‡∏á‡∏á‡∏ö Gemini ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß ({GEMINI_CALLS}/{GEMINI_DAILY_BUDGET})")
    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = model.generate_content(prompt)
            GEMINI_CALLS += 1
            return resp
        except Exception as e:
            err_str = str(e)
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ rate limit/‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡∏•‡∏≠‡∏á backoff
            if attempt < max_retries and any(x in err_str for x in ["429","exhausted","temporarily","unavailable","deadline","500","503"]):
                time.sleep(min(60, 5 * attempt))
                continue
            last_error = e
            if attempt < max_retries:
                time.sleep(3 * attempt)
            else:
                raise last_error
    raise last_error

# ========================= LLM Prompts =========================
def llm_ptt_subsidiary_impact_filter(news):
    """
    **‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö "‡πÉ‡∏ä‡πà" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
    - ‡πÉ‡∏ä‡πà: ‡∏Ç‡πà‡∏≤‡∏ß‡∏Å‡∏£‡∏∞‡∏ó‡∏ö upstream ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏¢‡∏á‡∏Å‡∏•‡πÑ‡∏Å‡∏™‡∏π‡πà PTTEP (‡∏´‡∏£‡∏∑‡∏≠ PTTLNG/PTTGL/PTTNGD ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏π‡πà upstream)
    - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà: ‡∏Ç‡πà‡∏≤‡∏ß downstream/PR ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÇ‡∏¢‡∏á‡∏Å‡∏•‡πÑ‡∏Å
    """
    prompt = f'''
{PTT_CONTEXT}
‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó: News Screener for Upstream Business Group Subsidiary Management Department
‡∏ï‡∏≠‡∏ö‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: "‡πÉ‡∏ä‡πà" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà"
- ‡πÉ‡∏ä‡πà: ‡∏Ç‡πà‡∏≤‡∏ß‡∏Å‡∏£‡∏∞‡∏ó‡∏ö upstream ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏¢‡∏á‡∏Å‡∏•‡πÑ‡∏Å‡∏™‡∏π‡πà PTTEP (‡∏´‡∏£‡∏∑‡∏≠ PTTLNG/PTTGL/PTTNGD ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏π‡πà upstream)
- ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà: ‡∏Ç‡πà‡∏≤‡∏ß downstream/PR ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÇ‡∏¢‡∏á‡∏Å‡∏•‡πÑ‡∏Å

‡∏Ç‡πà‡∏≤‡∏ß:
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {news['title']}
‡∏™‡∏£‡∏∏‡∏õ: {news['summary']}
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {news.get('detail','')}
'''
    try:
        resp = call_gemini(prompt)
        ans = (resp.text or "").strip().replace("\n", "")
        return ans.startswith("‡πÉ‡∏ä‡πà")
    except Exception as e:
        print("[ERROR] LLM Filter:", e)
        return False

def gemini_summary_and_score(news):
    """
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ JSON ‡∏ï‡∏≤‡∏° schema:
      {
        "summary": str,
        "score": int (1..5),
        "score_breakdown": [{"points": int, "reason": str}, ...]  # ‡∏ú‡∏•‡∏£‡∏ß‡∏° points ‡∏ï‡πâ‡∏≠‡∏á = score
        "impact_companies": ["PTTEP","PTTLNG","PTTGL","PTTNGD"] (<= 2 ‡∏£‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ [] ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß)
        "impact_reason": str  # ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏•‡πÑ‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô supply‚Üì ‚Üí Brent‚Üë ‚Üí margin upstream PTTEP‚Üë
      }
    """
    schema = {
        "type": "object",
        "properties": {
            "summary": {"type": "string"},
            "score": {"type": "integer"},
            "score_breakdown": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {"points": {"type": "integer"}, "reason": {"type": "string"}},
                    "required": ["points", "reason"]
                }
            },
            "impact_companies": {
                "type": "array",
                "items": {"type": "string", "enum": ["PTTEP","PTTLNG","PTTGL","PTTNGD"]}
            },
            "impact_reason": {"type": "string"}
        },
        "required": ["summary","score","score_breakdown","impact_companies","impact_reason"]
    }

    prompt = f"""
{PTT_CONTEXT}

‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó: Analyst for Upstream Business Group Subsidiary Management Department

‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏Ç‡πà‡∏≤‡∏ß:
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {news['title']}
‡∏™‡∏£‡∏∏‡∏õ: {news['summary']}
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {news.get('detail','')}

‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö **‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON** ‡∏ï‡∏≤‡∏° schema:
{json.dumps(schema, ensure_ascii=False)}

‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
- summary: ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå+‡∏Å‡∏•‡πÑ‡∏Å (‡πÄ‡∏ä‡πà‡∏ô supply‚Üì ‚Üí Brent‚Üë ‚Üí margin PTTEP‚Üë)
- score: 1‚Äì5, materiality upstream-first
- score_breakdown: ‡∏£‡∏ß‡∏° points = score
- impact_companies: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2, PTTEP ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß supply/price/PSC/production
- impact_reason: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏•‡πÑ‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ñ‡πâ‡∏≠‡∏¢‡∏Ñ‡∏≥‡∏Å‡∏ß‡πâ‡∏≤‡∏á ‡πÜ
"""
    try:
        resp = call_gemini(prompt)
        raw = (resp.text or "").strip()
        if raw.startswith("```"):
            raw = re.sub(r"^```(json)?", "", raw).strip()
            raw = re.sub(r"```$", "", raw).strip()
        data = json.loads(raw)
        return data
    except Exception as e:
        print("[WARN] JSON parse fail:", e)
        return {
            "summary": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á JSON ‡πÑ‡∏î‡πâ",
            "score": 3,
            "score_breakdown": [{"points": 2, "reason": "default"}, {"points": 1, "reason": "fallback"}],
            "impact_companies": [],
            "impact_reason": "-",
        }

# ========================= Logic =========================
def is_ptt_related_from_output(impact_companies) -> bool:
    return bool(impact_companies)

def fetch_news_9pm_to_6am():
    now_local = datetime.now(bangkok_tz)
    start_time = (now_local - timedelta(days=1)).replace(hour=21, minute=0, second=0, microsecond=0)
    end_time = now_local.replace(hour=6, minute=0, second=0, microsecond=0)
    print("‡∏ä‡πà‡∏ß‡∏á fetch:", start_time, "‡∏ñ‡∏∂‡∏á", end_time)
    all_news = []
    for _, info in news_sources.items():
        try:
            feed = feedparser.parse(info["url"])
            for entry in feed.entries:
                # ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà
                pub_str = getattr(entry, "published", None) or getattr(entry, "updated", None)
                if not pub_str and getattr(entry, "published_parsed", None):
                    t = entry.published_parsed
                    pub_dt = datetime(*t[:6], tzinfo=pytz.UTC).astimezone(bangkok_tz)
                else:
                    if not pub_str:
                        continue
                    pub_dt = dateutil_parser.parse(pub_str)
                    if pub_dt.tzinfo is None:
                        pub_dt = pytz.UTC.localize(pub_dt)
                    pub_dt = pub_dt.astimezone(bangkok_tz)

                if not (start_time <= pub_dt <= end_time):
                    continue

                # ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
                summary = getattr(entry, "summary", "") or getattr(entry, "description", "")
                link = getattr(entry, "link", "")
                title = getattr(entry, "title", "-")

                all_news.append({
                    "site": info["site"],
                    "category": info["category"],
                    "title": title,
                    "summary": summary,
                    "link": link,
                    "published": pub_dt,
                    "date": pub_dt.strftime("%d/%m/%Y %H:%M"),
                })
        except Exception as e:
            print(f"[WARN] ‡∏≠‡πà‡∏≤‡∏ô‡∏ü‡∏µ‡∏î {info['site']} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")

    # de-dup link
    seen = set()
    uniq = []
    for n in all_news:
        key = _normalize_link(n.get("link", ""))
        if key and key not in seen:
            seen.add(key)
            uniq.append(n)

    print("‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á (unique):", len(uniq))
    return uniq

def rank_candidates(news_list, use_keyword_boost=False):
    ranked = []
    for n in news_list:
        age_h = (now - n["published"]).total_seconds() / 3600.0
        recency = max(0.0, (72.0 - min(72.0, age_h))) / 72.0 * 3.0
        cat_w = {"Energy": 3.0, "Economy": 2.0, "Politics": 1.0}.get(n["category"], 1.0)
        length = min(len(n.get("summary","")) / 500.0, 1.0)
        score = recency + cat_w + length
        ranked.append((score, n))
    ranked.sort(key=lambda x: x[0], reverse=True)
    return [n for _, n in ranked]

def create_flex_message(news_items):
    now_thai = datetime.now(bangkok_tz).strftime("%d/%m/%Y")

    def join_companies(codes):
        codes = codes or []
        return ", ".join(codes) if codes else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏∏"

    bubbles = []
    for item in news_items:
        bd = item.get("score_breakdown", [])
        bd_lines = [f"- {x.get('points', 0)} ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {x.get('reason', '-')}\n" for x in bd]
        bd_clean = "".join(bd_lines).strip() or "-"

        impact_line = {
            "type": "text",
            "text": f"‡∏Å‡∏£‡∏∞‡∏ó‡∏ö: {join_companies(item.get('ptt_companies'))}",
            "size": "xs",
            "color": "#000000",
            "weight": "bold",
            "wrap": True,
            "margin": "sm",
        }

        img = item.get("image") or DEFAULT_ICON_URL
        if not (str(img).startswith("http://") or str(img).startswith("https://")):
            img = DEFAULT_ICON_URL

        body_contents = [
            {"type": "text","text": item.get("title", "-"),"weight": "bold","size": "lg","wrap": True,"color": "#111111"},
            {
                "type": "box","layout": "horizontal","margin": "sm",
                "contents": [
                    {"type": "text","text": f"üóì {item.get('date','-')}", "size": "xs","color": "#aaaaaa","flex": 5},
                    {"type": "text","text": f"üìå {item.get('category','')}", "size": "xs","color": "#888888","align": "end","flex": 5}
                ]
            },
            {"type": "text","text": f"üåç {item.get('site','')}", "size": "xs","color": "#448AFF","margin": "sm"},
            impact_line,
            {"type": "text","text": item.get("gemini_summary") or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß","size": "md","wrap": True,"margin": "md","color": "#1A237E","weight": "bold"},
            {
                "type": "box","layout": "vertical","margin": "lg",
                "contents": [
                    {"type": "text","text": "‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö / ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô","weight": "bold","size": "lg","color": "#D32F2F"},
                    {"type": "text","text": (item.get("gemini_reason") or "-"),"size": "md","wrap": True,"color": "#C62828","weight": "bold"},
                    {"type": "text","text": f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°: {item.get('gemini_score','-')} ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô","size": "lg","wrap": True,"color": "#000000","weight": "bold"},
                    {"type": "text","text": bd_clean,"size": "sm","wrap": True,"color": "#8E0000","weight": "bold"}
                ]
            }
        ]

        bubble = {
            "type": "bubble","size": "mega",
            "hero": {"type": "image","url": img,"size": "full","aspectRatio": "16:9","aspectMode": "cover"},
            "body": {"type": "box","layout": "vertical","spacing": "md","contents": body_contents},
            "footer": {
                "type": "box","layout": "vertical","spacing": "sm",
                "contents": [
                    {"type": "text","text": "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å","size": "xs","color": "#FF0000","wrap": True,"margin": "md","weight": "regular"},
                    {"type": "button","style": "primary","color": "#1DB446","action": {"type": "uri","label": "‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠","uri": item.get("link", "#")}}
                ]
            }
        }
        bubbles.append(bubble)

    carousels = []
    for i in range(0, len(bubbles), 10):
        carousels.append({
            "type": "flex",
            "altText": f"‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö ‡∏õ‡∏ï‡∏ó. {now_thai}",
            "contents": {"type": "carousel", "contents": bubbles[i:i+10]}
        })
    return carousels

def broadcast_flex_message(access_token, flex_carousels):
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {access_token}"}
    for idx, carousel in enumerate(flex_carousels, 1):
        payload = {"messages": [carousel]}
        if DRY_RUN:
            print(f"[DRY_RUN] ‡∏à‡∏∞‡∏™‡πà‡∏á Carousel #{idx}: {json.dumps(payload)[:500]}...")
            continue
        try:
            resp = S.post(url, headers=headers, json=payload, timeout=TIMEOUT)
            print(f"Broadcast #{idx} status:", resp.status_code, getattr(resp, "text", ""))
            if resp.status_code >= 300:
                break
            time.sleep(1.2)
        except Exception as e:
            print("[LINE ERROR]", e)
            break

# ========================= MAIN =========================
def main():
    all_news = fetch_news_9pm_to_6am()
    print(f"‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ä‡πà‡∏ß‡∏á 21:00 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô ‡∏ñ‡∏∂‡∏á 06:00 ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ: {len(all_news)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    if not all_news:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß")
        return
    SLEEP_MIN, SLEEP_MAX = SLEEP_BETWEEN_CALLS

    # -------- Filter stage --------
    filtered_news = []
    for news in all_news:
        # ‡∏ñ‡πâ‡∏≤ summary ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô ‡πÉ‡∏´‡πâ‡∏¢‡∏±‡∏î title ‡πÄ‡∏õ‡πá‡∏ô detail ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏´‡πâ LLM
        if len((news.get('summary') or '')) < 50:
            news['detail'] = news['title']
        else:
            news['detail'] = ''
        if llm_ptt_subsidiary_impact_filter(news):
            filtered_news.append(news)
        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))

    print(f"‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å PTT (‡∏ú‡πà‡∏≤‡∏ô‡∏ü‡∏¥‡∏•‡πÄ‡∏ï‡∏≠‡∏£‡πå): {len(filtered_news)} ‡∏Ç‡πà‡∏≤‡∏ß")
    if not filtered_news:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å PTT")
        return

    # -------- Analyze stage --------
    ranked = rank_candidates(filtered_news, use_keyword_boost=False)
    top_candidates = ranked[: min(10, len(ranked))]
    print(f"‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏µ‡∏¢‡∏á {len(top_candidates)} ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≥‡∏Å‡∏±‡∏î 10)")

    ptt_related_news = []
    for news in top_candidates:
        data = gemini_summary_and_score(news)
        # unify fields
        news['gemini_summary'] = _normalize_colons(data.get('summary', '')).strip() or '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß'
        score = int(data.get('score', 3))
        news['gemini_score'] = max(1, min(5, score))
        bd = data.get('score_breakdown', [])
        # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏£‡∏ß‡∏° points = score
        total_points = sum(int(x.get('points', 0)) for x in bd)
        if bd and total_points != news['gemini_score']:
            diff = news['gemini_score'] - total_points
            bd[-1]['points'] = int(bd[-1].get('points', 0)) + diff
        news['score_breakdown'] = bd

        reason = _polish_impact_text(data.get('impact_reason', '').strip())
        news['gemini_reason'] = reason or '-'

        companies = [c for c in data.get('impact_companies', []) if c in {"PTTEP","PTTLNG","PTTGL","PTTNGD"}]
        news['ptt_companies'] = list(dict.fromkeys(companies))  # unique, keep order

        if is_ptt_related_from_output(companies):
            ptt_related_news.append(news)

        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))

    print(f"‡πÉ‡∏ä‡πâ Gemini ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß: {GEMINI_CALLS}/{GEMINI_DAILY_BUDGET} calls")

    if not ptt_related_news:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏° PTT ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏á 10 ‡∏Ç‡πà‡∏≤‡∏ß")
        return

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô > ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà
    ptt_related_news.sort(key=lambda n: (n.get('gemini_score', 0), n.get('published', datetime.min)), reverse=True)
    top_news = ptt_related_news[:10]

    sent_links = load_sent_links_today_yesterday()
    top_news_to_send = [n for n in top_news if _normalize_link(n.get('link','')) not in sent_links]
    if not top_news_to_send:
        print("‡∏Ç‡πà‡∏≤‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡∏™‡πà‡∏á‡∏Ñ‡∏£‡∏ö‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà")
        return

    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏†‡∏≤‡∏û
    for item in top_news_to_send:
        img = fetch_article_image(item.get("link", "")) or ""
        if not (str(img).startswith("http://") or str(img).startswith("https://")):
            img = DEFAULT_ICON_URL
        item["image"] = img

    # ‡∏™‡πà‡∏á LINE
    carousels = create_flex_message(top_news_to_send)
    broadcast_flex_message(LINE_CHANNEL_ACCESS_TOKEN, carousels)
    save_sent_links([n.get("link", "") for n in top_news_to_send])
    print("‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ERROR]", e)
