# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote

import requests
import feedparser
import pytz
from dateutil import parser as dateutil_parser

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# =============================================================================
# ENV / CONFIG
# =============================================================================
TZ = pytz.timezone(os.getenv("TZ", "Asia/Bangkok"))

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "").strip()
if not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE_CHANNEL_ACCESS_TOKEN")

# Groq Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant").strip()
GROQ_ENDPOINT = os.getenv("GROQ_ENDPOINT", "https://api.groq.com/openai/v1/chat/completions").strip()
USE_LLM_SUMMARY = os.getenv("USE_LLM_SUMMARY", "1").strip().lower() in ["1", "true", "yes", "y"]

WINDOW_HOURS = int(os.getenv("WINDOW_HOURS", "48"))
MAX_PER_FEED = int(os.getenv("MAX_PER_FEED", "30"))
DRY_RUN = os.getenv("DRY_RUN", "0").strip().lower() in ["1", "true", "yes", "y"]
BUBBLES_PER_CAROUSEL = int(os.getenv("BUBBLES_PER_CAROUSEL", "10"))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
# ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: "reuters.com,bloomberg.com,bangkokpost.com,thansettakij.com"
ALLOWED_NEWS_SOURCES = os.getenv("ALLOWED_NEWS_SOURCES", "").strip()
if ALLOWED_NEWS_SOURCES:
    ALLOWED_NEWS_SOURCES_LIST = [s.strip().lower() for s in ALLOWED_NEWS_SOURCES.split(",") if s.strip()]
    print(f"[CONFIG] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß: {ALLOWED_NEWS_SOURCES_LIST}")
else:
    ALLOWED_NEWS_SOURCES_LIST = []
    print("[CONFIG] ‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß")

# Sent links tracking
SENT_DIR = os.getenv("SENT_DIR", "sent_links")
os.makedirs(SENT_DIR, exist_ok=True)

# =============================================================================
# PROJECT DATABASE
# =============================================================================
PROJECTS_BY_COUNTRY = {
    "Thailand": [
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 2/61", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå", "Arthit",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏™ 1", "S1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô 4", "Contract 4",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏µ‡∏ó‡∏µ‡∏ó‡∏µ‡∏≠‡∏µ‡∏û‡∏µ 1", "PTTEP 1", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 6/27",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 22/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏µ 5", "E5",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏†‡∏π‡∏Æ‡πà‡∏≠‡∏°", "Sinphuhorm",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏µ 8/32", "B8/32", "9A", "9‡πÄ‡∏≠",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 4/48", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 12/48",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 1/65", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏µ 3/65",
        "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 53/43", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏≠‡∏• 54/43"
    ],
    "Myanmar": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏ï‡∏¥‡∏Å‡πâ‡∏≤", "Zawtika", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏≤‡∏î‡∏≤‡∏ô‡∏≤", "Yadana", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤ ‡πÄ‡∏≠‡πá‡∏° 3", "Myanmar M3"],
    "Malaysia": ["Malaysia SK309", "SK309", "Malaysia SK311", "SK311", "Malaysia Block H", "Block H"],
    "Vietnam": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° 16-1", "Vietnam 16-1", "16-1", "Block B", "48/95"],
    "Indonesia": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏ó‡∏π‡∏ô‡πà‡∏≤ ‡∏ã‡∏µ ‡πÄ‡∏≠", "Natuna Sea A"],
    "Kazakhstan": ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∏‡∏á‡∏Å‡∏≤", "Dunga"],
    "Oman": ["Oman Block 61", "Block 61", "Oman Block 6", "PDO"],
    "UAE": ["Abu Dhabi Offshore 1", "Abu Dhabi Offshore 2", "Abu Dhabi Offshore 3"],
}

# =============================================================================
# KEYWORD FILTERS (‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô)
# =============================================================================
class KeywordFilter:
    # ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
    ENERGY_KEYWORDS = [
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü', '‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤',
        '‡∏Å‡πä‡∏≤‡∏ã', 'LNG', '‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÄ‡∏û‡∏•‡∏¥‡∏á', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô',
        '‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏ü‡πâ‡∏≤', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏™‡∏á‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå', '‡πÇ‡∏ã‡∏•‡∏≤‡∏£‡πå', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏°',
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ä‡∏µ‡∏ß‡∏°‡∏ß‡∏•', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡πâ‡∏≥', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô',
        '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡∏¥‡∏ß‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå', '‡∏ñ‡πà‡∏≤‡∏ô‡∏´‡∏¥‡∏ô', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ü‡∏≠‡∏™‡∏ã‡∏¥‡∏•',
        '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô‡∏Å‡πä‡∏≤‡∏ã', '‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô',
        '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Å‡πä‡∏≤‡∏ã', '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô', '‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πä‡∏≤‡∏ã', '‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤',
        '‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô',
        'energy', 'electricity', 'power', 'gas', 'oil', 'fuel',
        'power plant', 'renewable', 'solar', 'wind', 'biomass',
        'energy policy', 'energy project', 'energy investment'
    ]
    
    # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á
    EXCLUDE_KEYWORDS = [
        '‡∏ï‡∏•‡∏≤‡∏î‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå', '‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå', '‡∏£‡∏ñ', '‡∏£‡∏ñ‡πÉ‡∏´‡∏°‡πà', '‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÉ‡∏´‡∏°‡πà',
        '‡∏¢‡∏≤‡∏ô‡∏¢‡∏ô‡∏ï‡πå', '‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏¢‡∏≤‡∏ô‡∏¢‡∏ô‡∏ï‡πå',
        'car', 'automotive', 'vehicle', 'automobile'
    ]
    
    @classmethod
    def is_energy_related(cls, text: str) -> bool:
        """Check if text is energy related"""
        text_lower = text.lower()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á
        for exclude in cls.EXCLUDE_KEYWORDS:
            if exclude.lower() in text_lower:
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                has_energy = any(keyword.lower() in text_lower for keyword in cls.ENERGY_KEYWORDS)
                if not has_energy:
                    return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
        return any(keyword.lower() in text_lower for keyword in cls.ENERGY_KEYWORDS)
    
    @classmethod
    def detect_country(cls, text: str) -> str:
        """Detect country from text"""
        text_lower = text.lower()
        
        country_patterns = {
            "Thailand": ['‡πÑ‡∏ó‡∏¢', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢', 'thailand', 'bangkok'],
            "Myanmar": ['‡πÄ‡∏°‡∏µ‡∏¢‡∏ô‡∏°‡∏≤', 'myanmar', '‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á', 'yangon'],
            "Malaysia": ['‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢', 'malaysia', '‡∏Å‡∏±‡∏ß‡∏•‡∏≤‡∏•‡∏±‡∏°‡πÄ‡∏õ‡∏≠‡∏£‡πå', 'kuala lumpur'],
            "Vietnam": ['‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏°', 'vietnam', '‡∏Æ‡∏≤‡∏ô‡∏≠‡∏¢', 'hanoi'],
            "Indonesia": ['‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏ô‡∏µ‡πÄ‡∏ã‡∏µ‡∏¢', 'indonesia', '‡∏à‡∏≤‡∏Å‡∏≤‡∏£‡πå‡∏ï‡∏≤', 'jakarta'],
            "Kazakhstan": ['‡∏Ñ‡∏≤‡∏ã‡∏±‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô', 'kazakhstan', 'astana'],
            "Oman": ['‡πÇ‡∏≠‡∏°‡∏≤‡∏ô', 'oman', 'muscat'],
            "UAE": ['‡∏¢‡∏π‡πÄ‡∏≠‡∏≠‡∏µ', 'uae', '‡∏î‡∏π‡πÑ‡∏ö', 'dubai', '‡∏≠‡∏≤‡∏ö‡∏π‡∏î‡∏≤‡∏ö‡∏µ', 'abu dhabi']
        }
        
        for country, patterns in country_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                return country
        
        return ""

# =============================================================================
# FEEDS - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á
# =============================================================================
def gnews_rss(q: str, hl="en", gl="US", ceid="US:en") -> str:
    return f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl={hl}&gl={gl}&ceid={ceid}"

FEEDS = [
    ("GoogleNewsTH", "thai", gnews_rss(
        '(‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô OR "‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü" OR ‡∏Å‡πä‡∏≤‡∏ã OR LNG OR ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô OR ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ OR "‡πÇ‡∏£‡∏á‡πÑ‡∏ü‡∏ü‡πâ‡∏≤" OR "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô" OR "‡∏™‡∏±‡∏°‡∏õ‡∏ó‡∏≤‡∏ô") -"‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå" -"‡∏ï‡∏•‡∏≤‡∏î‡∏£‡∏ñ"',
        hl="th", gl="TH", ceid="TH:th"
    )),
    ("GoogleNewsEN", "international", gnews_rss(
        '(energy OR electricity OR power OR oil OR gas OR "power plant" OR "energy project") AND (Thailand OR Vietnam OR Malaysia OR Indonesia) -car -automotive',
        hl="en", gl="US", ceid="US:en"
    )),
    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° feed ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
    ("EnergyNewsCenter", "direct", "https://www.energynewscenter.com/feed/"),
    # ‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° RSS feed URLs ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
    ("EnergyNewsCenter RSS2", "direct", "https://www.energynewscenter.com/rss/"),
    ("EnergyNewsCenter RSS3", "direct", "https://www.energynewscenter.com/feed/rss/"),
]

# =============================================================================
# UTILITIES
# =============================================================================
def now_tz() -> datetime:
    return datetime.now(TZ)

def normalize_url(url: str) -> str:
    url = (url or "").strip()
    if not url:
        return url
    try:
        u = urlparse(url)
        return u._replace(fragment="").geturl()
    except Exception:
        return url

def extract_domain(url: str) -> str:
    """Extract domain name from URL"""
    url = normalize_url(url)
    if not url:
        return ""
    try:
        domain = urlparse(url).netloc.lower()
        # Remove www. prefix
        if domain.startswith("www."):
            domain = domain[4:]
        return domain
    except Exception:
        return ""

def is_allowed_source(url: str) -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    if not ALLOWED_NEWS_SOURCES_LIST:  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î allowed sources = ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        return True
    
    domain = extract_domain(url)
    if not domain:
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ domain ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
    for allowed_source in ALLOWED_NEWS_SOURCES_LIST:
        if allowed_source in domain:  # ‡πÉ‡∏ä‡πâ partial match ‡πÄ‡∏ä‡πà‡∏ô "reuters" ‡∏à‡∏∞ match "reuters.com"
            return True
    
    return False

def shorten_google_news_url(url: str) -> str:
    """Extract actual URL from Google News redirect"""
    url = normalize_url(url)
    if not url:
        return url
    try:
        u = urlparse(url)
        if "news.google.com" in u.netloc:
            qs = parse_qs(u.query)
            if "url" in qs and qs["url"]:
                return normalize_url(unquote(qs["url"][0]))
    except Exception:
        pass
    return url

def read_sent_links() -> set:
    sent = set()
    for fn in os.listdir(SENT_DIR):
        if not fn.endswith(".txt"):
            continue
        fp = os.path.join(SENT_DIR, fn)
        try:
            with open(fp, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        sent.add(line)
        except Exception:
            continue
    return sent

def append_sent_link(url: str):
    url = normalize_url(url)
    if not url:
        return
    fn = os.path.join(SENT_DIR, now_tz().strftime("%Y-%m-%d") + ".txt")
    with open(fn, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def in_time_window(published_dt: datetime, hours: int) -> bool:
    if not published_dt:
        return False
    return published_dt >= (now_tz() - timedelta(hours=hours))

def cut(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else s[: n - 1].rstrip() + "‚Ä¶"

def create_simple_summary(text: str, max_length: int = 150) -> str:
    """Create a simple summary from text if LLM is not available"""
    text = (text or "").strip()
    if not text:
        return ""
    
    # Remove extra whitespace and newlines
    text = ' '.join(text.split())
    
    # Find first sentence or truncate
    sentences = re.split(r'[.!?]', text)
    if sentences and len(sentences[0]) > 10:
        summary = sentences[0].strip()
        if len(summary) > max_length:
            summary = summary[:max_length-1] + "‚Ä¶"
        return summary + "."
    
    # Fallback: simple truncation
    if len(text) > max_length:
        return text[:max_length-1] + "‚Ä¶"
    return text

# =============================================================================
# RSS PARSING
# =============================================================================
def fetch_feed(name: str, section: str, url: str):
    """‡∏î‡∏∂‡∏á RSS feed ‡∏à‡∏≤‡∏Å URL"""
    print(f"[FEED] ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {name} ({url})...")
    try:
        d = feedparser.parse(url)
        entries = d.entries or []
        print(f"[FEED] {name}: ‡∏û‡∏ö {len(entries)} entries")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á entries (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug)
        if entries and len(entries) > 0:
            print(f"[FEED] ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏£‡∏Å: {entries[0].title[:50]}...")
        
        return entries
    except Exception as e:
        print(f"[FEED] {name}: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î - {str(e)}")
        return []

def parse_entry(e, feed_name: str, section: str):
    title = (getattr(e, "title", "") or "").strip()
    link = (getattr(e, "link", "") or "").strip()
    summary = (getattr(e, "summary", "") or "").strip()
    published = getattr(e, "published", None) or getattr(e, "updated", None)

    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ published_parsed
    if not published and hasattr(e, 'published_parsed'):
        try:
            import time as time_module
            published = time_module.strftime('%Y-%m-%dT%H:%M:%SZ', e.published_parsed)
        except:
            pass

    try:
        published_dt = dateutil_parser.parse(published) if published else None
        if published_dt and published_dt.tzinfo is None:
            published_dt = TZ.localize(published_dt)
        if published_dt:
            published_dt = published_dt.astimezone(TZ)
    except Exception:
        published_dt = None

    canon = shorten_google_news_url(link)

    return {
        "title": title,
        "url": normalize_url(link),
        "canon_url": normalize_url(canon),
        "summary": summary,
        "published_dt": published_dt,
        "feed": feed_name,
        "section": section,
    }

# =============================================================================
# LLM ANALYZER (‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô)
# =============================================================================
class LLMAnalyzer:
    def __init__(self, api_key: str, model: str, endpoint: str):
        self.api_key = api_key
        self.model = model
        self.endpoint = endpoint
    
    def analyze_news(self, title: str, summary: str) -> dict:
        """Analyze news using LLM"""
        if not self.api_key:
            return self._get_default_analysis(title, summary)
        
        system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
        ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:
        {
            "relevant": true/false,
            "country": "‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á",
            "summary_th": "‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ",
            "topics": ["‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠1", "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠2"]
        }
        
        ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö:"""
        
        user_prompt = f"""‡∏Ç‡πà‡∏≤‡∏ß: {title}
        
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {summary[:500]}
        
        ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ:"""
        
        try:
            response = requests.post(
                self.endpoint,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 300
                },
                timeout=30
            )
            
            if response.status_code != 200:
                print(f"[LLM] HTTP Error {response.status_code}")
                return self._get_default_analysis(title, summary)
            
            data = response.json()
            content = data["choices"][0]["message"]["content"].strip()
            
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
                
                # Validate and clean up
                return {
                    "relevant": bool(analysis.get("relevant", True)),
                    "country": str(analysis.get("country", "")).strip(),
                    "summary_th": str(analysis.get("summary_th", "")).strip()[:150],
                    "topics": [str(t).strip() for t in analysis.get("topics", []) if t]
                }
                
        except json.JSONDecodeError:
            print("[LLM] Failed to parse JSON response")
        except Exception as e:
            print(f"[LLM] Error: {str(e)}")
        
        return self._get_default_analysis(title, summary)
    
    def _get_default_analysis(self, title: str, summary: str):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        combined = f"{title} {summary}"
        simple_summary = create_simple_summary(combined, 100)
        
        return {
            "relevant": True,
            "country": "",
            "summary_th": simple_summary if simple_summary else "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "topics": []
        }

# =============================================================================
# NEWS PROCESSOR
# =============================================================================
class NewsProcessor:
    def __init__(self):
        self.sent_links = read_sent_links()
        self.llm_analyzer = LLMAnalyzer(GROQ_API_KEY, GROQ_MODEL, GROQ_ENDPOINT) if GROQ_API_KEY else None
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß
        self.news_sources = {
            'reuters.com': 'Reuters',
            'bloomberg.com': 'Bloomberg',
            'bangkokpost.com': 'Bangkok Post',
            'thansettakij.com': '‡∏ê‡∏≤‡∏ô‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',
            'posttoday.com': 'Post Today',
            'prachachat.net': '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à',
            'mgronline.com': '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå',
            'komchadluek.net': '‡∏Ñ‡∏°‡∏ä‡∏±‡∏î‡∏•‡∏∂‡∏Å',
            'nationthailand.com': 'The Nation Thailand',
            'naewna.com': '‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡πâ‡∏≤',
            'dailynews.co.th': '‡πÄ‡∏î‡∏•‡∏¥‡∏ô‡∏¥‡∏ß‡∏™‡πå',
            'thairath.co.th': '‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê',
            'khaosod.co.th': '‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏î',
            'matichon.co.th': '‡∏°‡∏ï‡∏¥‡∏ä‡∏ô',
            'sanook.com': '‡∏™‡∏ô‡∏∏‡∏Å‡∏î‡∏≠‡∏ó‡∏Ñ‡∏≠‡∏°',
            'kapook.com': '‡∏Å‡∏∞‡∏õ‡∏∏‡∏Å',
            'manager.co.th': '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£',
            # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
            'energynewscenter.com': 'Energy News Center',  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        }
    
    def get_source_name(self, url: str) -> str:
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å URL"""
        domain = extract_domain(url)
        if not domain:
            return domain
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ domain ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        for source_domain, source_name in self.news_sources.items():
            if source_domain in domain:
                return source_name
        
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ domain ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠
        return domain
    
    def fetch_and_filter_news(self):
        """Fetch and filter news from all feeds"""
        all_news = []
        
        for feed_name, feed_type, feed_url in FEEDS:
            print(f"\n[Fetching] {feed_name} ({feed_type})...")
            
            try:
                entries = fetch_feed(feed_name, feed_type, feed_url)
                
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏á MAX_PER_FEED ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                limit = 20 if feed_type == "direct" else MAX_PER_FEED
                
                for entry in entries[:limit]:
                    news_item = self._process_entry(entry, feed_name, feed_type)
                    if news_item:
                        all_news.append(news_item)
                        print(f"  ‚úì {news_item['title'][:50]}...")
                        
            except Exception as e:
                print(f"  ‚úó Error: {str(e)}")
        
        # Sort by date (‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
        all_news.sort(key=lambda x: -((x.get('published_dt') or datetime.min).timestamp()))
        
        return all_news
    
    def _process_entry(self, entry, feed_name: str, feed_type: str):
        """Process individual news entry"""
        item = parse_entry(entry, feed_name, feed_type)
        
        # Basic validation
        if not item["title"] or not item["url"]:
            return None
        
        # Check if already sent
        if item["canon_url"] in self.sent_links or item["url"] in self.sent_links:
            return None
        
        # Check time window
        if item["published_dt"] and not in_time_window(item["published_dt"], WINDOW_HOURS):
            return None
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á (direct) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ALLOWED_NEWS_SOURCES
        # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á‡∏ó‡∏∏‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
        if feed_type != "direct":
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            display_url = item["canon_url"] or item["url"]
            if not is_allowed_source(display_url):
                # print(f"  ‚úó ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {extract_domain(display_url)} (‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï)")
                return None
        
        # Combine text for analysis
        full_text = f"{item['title']} {item['summary']}"
        
        # Step 1: Keyword filtering
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        if not KeywordFilter.is_energy_related(full_text):
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
            if feed_type != "direct":
                return None
        
        # Step 2: Detect country (‡πÅ‡∏ï‡πà‡πÄ‡∏ß‡πá‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏™‡∏°‡∏≠)
        country = KeywordFilter.detect_country(full_text)
        if not country:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Thailand ‡πÄ‡∏õ‡πá‡∏ô default
            if feed_type == "direct":
                country = "Thailand"
            else:
                return None
        
        # Step 3: LLM analysis (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
        llm_summary = ""
        if USE_LLM_SUMMARY and self.llm_analyzer:
            llm_analysis = self.llm_analyzer.analyze_news(item['title'], item['summary'])
            
            # ‡πÉ‡∏ä‡πâ LLM country ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
            if llm_analysis['country'] and llm_analysis['country'] in PROJECTS_BY_COUNTRY:
                country = llm_analysis['country']
            
            # ‡πÉ‡∏ä‡πâ summary ‡∏à‡∏≤‡∏Å LLM
            if llm_analysis.get('summary_th'):
                llm_summary = llm_analysis['summary_th']
        
        # Get project hints for this country
        project_hints = PROJECTS_BY_COUNTRY.get(country, [])[:2]
        
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß
        display_url = item["canon_url"] or item["url"]
        source_name = self.get_source_name(display_url)
        
        # Build final news item
        return {
            'title': item['title'][:100],
            'url': item['url'],
            'canon_url': item['canon_url'],
            'source_name': source_name,
            'domain': extract_domain(display_url),
            'summary': item['summary'][:200],
            'published_dt': item['published_dt'],
            'country': country,
            'project_hints': project_hints,
            'llm_summary': llm_summary,
            'feed': feed_name,
            'feed_type': feed_type,
            'simple_summary': create_simple_summary(full_text, 100)
        }

# =============================================================================
# LINE MESSAGE BUILDER
# =============================================================================
class LineMessageBuilder:
    @staticmethod
    def create_flex_bubble(news_item):
        """Create a LINE Flex Bubble for a news item"""
        title = cut(news_item.get('title', ''), 80)
        
        # Format timestamp
        pub_dt = news_item.get('published_dt')
        time_str = pub_dt.strftime("%d/%m/%Y %H:%M") if pub_dt else ""
        
        # Build bubble contents
        contents = [
            {
                "type": "text",
                "text": title,
                "weight": "bold",
                "size": "md",
                "wrap": True,
                "margin": "md"
            }
        ]
        
        # Add metadata - ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        metadata_parts = []
        if time_str:
            metadata_parts.append(time_str)
        if news_item.get('feed'):
            metadata_parts.append(news_item['feed'])
        
        if metadata_parts:
            contents.append({
                "type": "text",
                "text": " | ".join(metadata_parts),
                "size": "xs",
                "color": "#888888",
                "margin": "sm"
            })
        
        # ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà**
        if news_item.get('source_name'):
            # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å dictionary ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
            contents.append({
                "type": "text",
                "text": f"üì∞ {news_item['source_name']}",
                "size": "xs",
                "color": "#666666",
                "margin": "sm"
            })
        elif news_item.get('domain'):
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ domain
            contents.append({
                "type": "text",
                "text": f"üåê {cut(news_item['domain'], 30)}",
                "size": "xs",
                "color": "#666666",
                "margin": "sm"
            })
        
        # Add country
        contents.append({
            "type": "text",
            "text": f"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®: {news_item.get('country', 'N/A')}",
            "size": "sm",
            "margin": "xs",
            "color": "#666666"
        })
        
        # Add project hints
        if news_item.get('project_hints'):
            hints_text = ", ".join(news_item['project_hints'][:2])
            contents.append({
                "type": "text",
                "text": f"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {hints_text}",
                "size": "sm",
                "color": "#2E7D32",
                "wrap": True,
                "margin": "xs"
            })
        
        # ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢**
        summary_text = ""
        
        # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å LLM ‡∏Å‡πà‡∏≠‡∏ô
        if news_item.get('llm_summary'):
            summary_text = news_item['llm_summary']
        # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏≤‡∏Å LLM ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ simple summary
        elif news_item.get('simple_summary'):
            summary_text = news_item['simple_summary']
        # 3. Fallback ‡πÉ‡∏ä‡πâ summary ‡∏à‡∏≤‡∏Å RSS
        elif news_item.get('summary'):
            summary_text = create_simple_summary(news_item['summary'], 120)
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏£‡∏∏‡∏õ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å title
        if not summary_text or len(summary_text.strip()) < 10:
            summary_text = f"{news_item.get('title', '‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô')[:60]}..."
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏™‡∏£‡∏∏‡∏õ (‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢)
        if summary_text:
            contents.append({
                "type": "text",
                "text": cut(summary_text, 120),
                "size": "sm",
                "wrap": True,
                "margin": "md",
                "color": "#424242"
            })
        
        # Create bubble
        bubble = {
            "type": "bubble",
            "size": "kilo",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": contents,
                "paddingAll": "12px"
            }
        }
        
        # Add button if URL exists
        url = news_item.get('canon_url') or news_item.get('url')
        if url and len(url) < 1000:
            bubble["footer"] = {
                "type": "box",
                "layout": "vertical",
                "spacing": "sm",
                "contents": [
                    {
                        "type": "button",
                        "style": "primary",
                        "height": "sm",
                        "action": {
                            "type": "uri",
                            "label": "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ï‡πá‡∏°",
                            "uri": url
                        }
                    }
                ]
            }
        
        return bubble
    
    @staticmethod
    def create_carousel_message(news_items):
        """Create LINE carousel message from news items"""
        bubbles = []
        
        for item in news_items[:BUBBLES_PER_CAROUSEL]:
            bubble = LineMessageBuilder.create_flex_bubble(item)
            if bubble:
                bubbles.append(bubble)
        
        if not bubbles:
            return None
        
        return {
            "type": "flex",
            "altText": f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô {datetime.now(TZ).strftime('%d/%m/%Y')} ({len(bubbles)} ‡∏Ç‡πà‡∏≤‡∏ß)",
            "contents": {
                "type": "carousel",
                "contents": bubbles
            }
        }

# =============================================================================
# LINE SENDER
# =============================================================================
class LineSender:
    def __init__(self, access_token):
        self.access_token = access_token
        self.headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
    
    def send_message(self, message_obj):
        """Send message to LINE"""
        if DRY_RUN:
            print("\n" + "="*60)
            print("DRY RUN - Would send the following news:")
            print("="*60)
            
            contents = message_obj.get('contents', {}).get('contents', [])
            for i, bubble in enumerate(contents):
                body_contents = bubble.get('body', {}).get('contents', [])
                title = ""
                source = ""
                
                for content in body_contents:
                    if content.get('type') == 'text':
                        text = content.get('text', '')
                        if len(text) > 10 and not title:
                            title = text[:60]
                        elif 'üì∞' in text or 'üåê' in text:
                            source = text
                            break
                
                print(f"{i+1}. {title}")
                if source:
                    print(f"   Source: {source}")
            
            print(f"\nTotal: {len(contents)} news items")
            return True
        
        url = "https://api.line.me/v2/bot/message/broadcast"
        
        try:
            response = requests.post(
                url,
                headers=self.headers,
                json={"messages": [message_obj]},
                timeout=30
            )
            
            if response.status_code == 200:
                print("[LINE] Message sent successfully!")
                return True
            else:
                print(f"[LINE] Error {response.status_code}: {response.text[:200]}")
                return False
                
        except Exception as e:
            print(f"[LINE] Exception: {str(e)}")
            return False

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    print("="*60)
    print("‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô")
    print("="*60)
    
    # Configuration check
    if not LINE_CHANNEL_ACCESS_TOKEN:
        print("[ERROR] LINE_CHANNEL_ACCESS_TOKEN is required")
        return
    
    if USE_LLM_SUMMARY and not GROQ_API_KEY:
        print("[WARNING] LLM summary enabled but no GROQ_API_KEY provided")
        print("[INFO] Will use simple summary for all news")
    
    print(f"\n[CONFIG] Use LLM: {'Yes' if USE_LLM_SUMMARY and GROQ_API_KEY else 'No (simple summary)'}")
    print(f"[CONFIG] Time window: {WINDOW_HOURS} hours")
    print(f"[CONFIG] Dry run: {'Yes' if DRY_RUN else 'No'}")
    print(f"[CONFIG] Allowed news sources: {ALLOWED_NEWS_SOURCES_LIST if ALLOWED_NEWS_SOURCES_LIST else 'All sources'}")
    print(f"[CONFIG] ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feed ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(FEEDS)}")
    print(f"[CONFIG] Feed ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£: {[f[0] for f in FEEDS]}")
    
    # Initialize components
    processor = NewsProcessor()
    line_sender = LineSender(LINE_CHANNEL_ACCESS_TOKEN)
    
    # Step 1: Fetch and filter news
    print("\n[1] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß...")
    news_items = processor.fetch_and_filter_news()
    
    if not news_items:
        print("\n[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
        return
    
    print(f"\n[2] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_items)} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # Count statistics
    llm_summary_count = sum(1 for item in news_items if item.get('llm_summary'))
    direct_count = sum(1 for item in news_items if item.get('feed_type') == 'direct')
    
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß
    source_counts = {}
    for item in news_items:
        source = item.get('source_name') or item.get('domain', 'Unknown')
        source_counts[source] = source_counts.get(source, 0) + 1
    
    print(f"   - ‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢ AI: {llm_summary_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á: {direct_count} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"   - ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏û‡∏ö:")
    for source, count in sorted(source_counts.items()):
        print(f"     ‚Ä¢ {source}: {count} ‡∏Ç‡πà‡∏≤‡∏ß")
    
    # Step 2: Create LINE message
    print("\n[3] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LINE...")
    line_message = LineMessageBuilder.create_carousel_message(news_items)
    
    if not line_message:
        print("[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ")
        return
    
    # Step 3: Send message
    print("\n[4] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
    success = line_sender.send_message(line_message)
    
    # Step 4: Mark as sent if successful
    if success and not DRY_RUN:
        for item in news_items:
            append_sent_link(item.get('canon_url') or item.get('url'))
        print("\n[SUCCESS] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    
    print("\n" + "="*60)
    print("‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    print("="*60)

if __name__ == "__main__":
    main()
